<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CKA Study Guide 2026 | Certified Kubernetes Administrator Exam Prep & Cheat Sheet</title>
  <meta name="description"
    content="Free Ultimate CKA study guide for 2026: cluster architecture, networking, storage, RBAC, troubleshooting. Commands, YAML examples &amp; practice mocks for Certified Kubernetes Administrator exam.">
  <meta name="keywords"
    content="CKA Study Guide 2026, Certified Kubernetes Administrator, CKA Exam Prep, Kubernetes Certification, K8s Troubleshooting, Kubectl Cheat Sheet, CKA Practice Exam, Kubernetes Admin Guide">
  <meta name="author" content="CKA Study Guide">
  <meta name="robots" content="index, follow">
  <!-- Replace with your real page URL when you publish the site -->
  <link rel="canonical" href="https://skcloudops.github.io/k8s/">
  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator Preparation">
  <meta property="og:description"
    content="Free CKA study guide with commands, YAML examples and diagrams. Covers cluster architecture, scheduling, networking, storage, security and troubleshooting for the CKA exam.">
  <meta property="og:url" content="https://skcloudops.github.io/k8s/">
  <meta property="og:locale" content="en_US">
  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator">
  <meta name="twitter:description"
    content="Free CKA study guide: commands, YAML snippets and diagrams for Certified Kubernetes Administrator (CKA) exam preparation.">
  <meta name="theme-color" content="#1a7f37">

  <!-- SEO & Social Media -->
  <meta property="og:site_name" content="CKA Study Guide">
  <meta property="og:image" content="https://skcloudops.github.io/k8s/assets/cka-og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta name="twitter:image" content="https://skcloudops.github.io/k8s/assets/cka-og-image.png">
  <meta name="twitter:creator" content="@CKAStudyGuide">

  <!-- Structured Data: Course & FAQ for Rich Snippets -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Course",
    "name": "Certified Kubernetes Administrator (CKA) Study Guide",
    "description": "Comprehensive reference guide for the CKA exam. Covers architecture, networking, storage, and troubleshooting with interactive labs and cheat sheets.",
    "provider": {
      "@type": "Organization",
      "name": "SK CloudOps",
      "sameAs": "https://skcloudops.github.io/k8s/"
    },
    "courseCode": "CKA",
    "educationalLevel": "Intermediate",
    "offers": [{
      "@type": "Offer",
      "category": "Free",
      "price": "0",
      "priceCurrency": "USD"
    }]
  }
  </script>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "FAQPage",
    "mainEntity": [{
      "@type": "Question",
      "name": "What are the core topics for the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "The CKA exam covers Cluster Architecture (25%), Workloads & Scheduling (15%), Services & Networking (20%), Storage (10%), and Troubleshooting (30%)."
      }
    }, {
      "@type": "Question",
      "name": "How long is the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "The Certified Kubernetes Administrator (CKA) exam is a 2-hour performance-based test."
      }
    }, {
      "@type": "Question",
      "name": "Can I use documentation during the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Yes, candidates can access official Kubernetes documentation during the exam via a single browser tab."
      }
    }]
  }
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Outfit:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="style.css">
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "LearningResource",
    "name": "CKA Exam Study Guide | Certified Kubernetes Administrator Preparation",
    "description": "Free study guide for the Certified Kubernetes Administrator (CKA) exam. Covers cluster architecture, scheduling, workloads, networking, storage, security, and troubleshooting with commands, YAML examples and diagrams.",
    "learningResourceType": "Study Guide",
    "educationalLevel": "Professional",
    "teaches": ["Kubernetes", "Container orchestration", "kubectl", "CKA certification", "Cluster administration"],
    "about": [
      { "@type": "Thing", "name": "Certified Kubernetes Administrator" },
      { "@type": "Thing", "name": "Kubernetes" }
    ]
  }
  </script>
</head>

<body>
  <header class="site-header" style="z-index: 2000;">
    <div style="display: flex; align-items: center; gap: 0.5rem; flex: 1; min-width: 0;">
      <button type="button" class="icon-btn" id="toggle-left" title="Toggle Navigation">
        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="9" y1="3" x2="9" y2="21"></line>
        </svg>
      </button>
      <h1 style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis;">CKA Study Guide</h1>
      <span class="badge">CKA</span>
    </div>

    <div class="header-controls">
      <!-- Challenge Mode Toggle -->
      <div class="challenge-toggle-wrapper header-hide-mobile" title="Hide answers to test yourself">
        <span class="challenge-label" style="display: none;">Challenge</span>
        <label class="toggle-switch">
          <input type="checkbox" id="challenge-toggle">
          <span class="slider"></span>
        </label>
      </div>

      <!-- Font Size Controls -->
      <div class="font-size-controls header-hide-mobile" title="Font size">
        <button id="font-decrease" title="Decrease font size">A-</button>
        <button id="font-increase" title="Increase font size">A+</button>
      </div>

      <!-- Search Button -->
      <button type="button" class="icon-btn" id="search-btn" title="Search (Ctrl+K)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"></circle>
          <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
      </button>

      <!-- Notes Button -->
      <button type="button" class="icon-btn header-hide-mobile" id="notes-btn" title="Section Notes (N)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M14 2H6a2 2 0 0 0-2 2v16c0 1.1.9 2 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
      </button>

      <!-- Dark Mode -->
      <button type="button" class="icon-btn" id="dark-mode-btn" title="Toggle Dark Mode (D)">
        <svg id="dm-icon-moon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
          stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
        </svg>
        <svg id="dm-icon-sun" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
          stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display:none">
          <circle cx="12" cy="12" r="5"></circle>
          <line x1="12" y1="1" x2="12" y2="3"></line>
          <line x1="12" y1="21" x2="12" y2="23"></line>
          <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
          <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
          <line x1="1" y1="12" x2="3" y2="12"></line>
          <line x1="21" y1="12" x2="23" y2="12"></line>
          <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
          <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
        </svg>
      </button>

      <!-- Keyboard Shortcuts -->
      <button type="button" class="icon-btn header-hide-mobile" id="shortcuts-btn" title="Keyboard Shortcuts (?)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <rect x="2" y="6" width="20" height="12" rx="2"></rect>
          <path d="M6 10h.01M10 10h.01M14 10h.01M18 10h.01M8 14h8"></path>
        </svg>
      </button>

      <!-- Pomodoro Timer -->
      <div class="pomo-wrap header-hide-mobile">
        <button type="button" class="icon-btn" id="pomo-header-btn" title="Pomodoro Timer (T)">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
            stroke-linecap="round" stroke-linejoin="round">
            <circle cx="12" cy="13" r="8"></circle>
            <path d="M12 9v4l2 2"></path>
            <path d="M5 3L2 6"></path>
            <path d="M22 6l-3-3"></path>
          </svg>
          <span class="pomo-tick" id="pomo-header-tick">25:00</span>
        </button>
        <div id="pomodoro-widget">
          <div id="pomo-label">ðŸŽ¯ Focus Session</div>
          <svg class="pomo-ring" viewBox="0 0 80 80">
            <circle cx="40" cy="40" r="34" fill="none" stroke="var(--surface2)" stroke-width="6" />
            <circle id="pomo-progress-circle" cx="40" cy="40" r="34" fill="none" stroke="var(--accent2)"
              stroke-width="6" stroke-linecap="round" stroke-dasharray="213.6" stroke-dashoffset="0"
              transform="rotate(-90 40 40)" />
          </svg>
          <div id="pomo-time">25:00</div>
          <div class="pomo-btns">
            <button class="pomo-btn primary" id="pomo-start">Start</button>
            <button class="pomo-btn" id="pomo-reset">Reset</button>
          </div>
          <div id="pomo-sessions">Sessions today: <strong id="pomo-session-count">0</strong></div>
        </div>
      </div>


      <!-- Zen Mode -->
      <button type="button" class="icon-btn header-hide-mobile" id="zen-btn" title="Zen / Focus Mode (Z)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle>
          <path d="M8 12a4 4 0 0 1 8 0"></path>
          <line x1="12" y1="8" x2="12" y2="8.01"></line>
        </svg>
      </button>

      <!-- Quiz / Flashcards -->
      <button type="button" class="icon-btn header-hide-mobile" id="quiz-btn" title="CKA Quiz Mode (Q)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 11l3 3L22 4"></path>
          <path d="M21 12v7a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h11"></path>
        </svg>
      </button>


      <button type="button" class="nav-toggle" aria-label="Open menu" id="nav-toggle">â˜°</button>
    </div>
  </header>



  <div class="layout">
    <nav id="nav" aria-label="Section navigation">
      <div class="nav-header">
        <span class="nav-section">CKA Preparation</span>
        <button type="button" class="nav-close" aria-label="Close menu" id="nav-close">Ã—</button>
      </div>

      <a href="#section-01">01 Introduction & Foundations</a>
      <a href="#section-02">02 Core Concepts</a>
      <a href="#section-03">03 Scheduling</a>
      <a href="#section-04">04 Logging & Monitoring</a>
      <a href="#section-05">05 Application Lifecycle</a>
      <a href="#section-06">06 Cluster Maintenance</a>
      <a href="#section-07">07 Security</a>
      <a href="#section-08">08 Storage</a>
      <a href="#section-09">09 Networking</a>
      <a href="#section-10">10 Cluster Design & Installation</a>
      <a href="#section-11">11 kubeadm Installation</a>
      <a href="#section-12">12 Helm</a>
      <a href="#section-13">13 Kustomize</a>
      <a href="#section-14">14 Troubleshooting</a>
      <a href="#section-15">15 Advanced Kubectl & JSON Path</a>
      <a href="#section-16">16 Practice & Exam Preparation</a>
      <a href="#section-17">17 Cheat Sheet</a>
      <a href="lab.html" style="margin-top: 0.5rem; padding-top: 0.5rem; border-top: 1px solid var(--border);">ðŸ“‹ CKA Lab</a>

    </nav>
    <main>
      <header id="intro">
        <h1 class="page-title">Certified Kubernetes Administrator (CKA)</h1>
        <p class="subtitle">Complete reference guide for the CKA exam. Use the navigation to jump between topics.</p>

        <figure class="diagram">
          <img src="assets/k8s-request-workflow.png" alt="Kubernetes request workflow diagram showing kubectl apply to pod running flow" width="1024" height="576" style="max-width:100%;height:auto;display:block;">
          <figcaption><strong>Kubernetes Request Flow</strong> â€” What happens when you run <code>kubectl apply -f deployment.yaml</code>. 1) kubectl â†’ API Server (auth, RBAC, admission). 2) API Server â†’ etcd (persist desired state). 3) Controller creates Pod. 4) Scheduler assigns node. 5) kubelet pulls image & runs container via CRI. 6) CNI assigns pod IP; kube-proxy programs Service routing. 7) Pod Running. Every CKA candidate must understand this flow.</figcaption>
        </figure>
      </header>

      <!-- 01 Introduction & Foundations -->
      <h2 id="section-01">01 â€” Introduction & Foundations</h2>

      <div class="cka-definition">
        <div class="cka-def-term">CKA â€” Certified Kubernetes Administrator</div>
        <div class="cka-def-body">A performance-based certification from the Linux Foundation / CNCF that validates your ability to perform Kubernetes administration tasks. The exam is 2 hours, uses live clusters, and allows access to official Kubernetes documentation. Passing score: 66%.</div>
      </div>

      <h3>Course Introduction</h3>
      <p>This study guide aligns with the full CKA curriculum. It covers cluster architecture, workloads, networking, storage, security, and troubleshooting with commands, YAML examples, and CKA-focused definitions.</p>

      <h3>Certification Overview</h3>
      <p>The CKA is part of the <strong>Kubernetes Trilogy</strong>: CKA (Administrator), CKAD (Application Developer), and CKS (Security Specialist). Use code <code>KUBERNETES15</code> for a 15% discount at registration.</p>

      <h3>Course focus</h3>
      <p>This material focuses on the <strong>administration</strong> part of Kubernetes, aligned with the CKA exam.</p>
      <h4>Pre-requisites</h4>
      <ul>
        <li>Docker</li>
        <li>Basics of Kubernetes (Pods, Deployments, Services)</li>
        <li>YAML</li>
        <li>Basic lab environment (e.g. VirtualBox)</li>
      </ul>
      <h4>Exam areas (weighted)</h4>
      <ul>
        <li><strong>Cluster Architecture, Installation & Validation</strong> (25%): Design, install, configure,
          validate, manage HA, etcd, upgrade.</li>
        <li><strong>Workloads & Scheduling</strong> (15%): Deployments, scaling, DaemonSets, resource limits, scheduling
          (manual, node selector, affinity, taints/tolerations), static pods.</li>
        <li><strong>Services & Networking</strong> (20%): Networking model, Services (ClusterIP, NodePort,
          LoadBalancer), DNS, CNI, Ingress.</li>
        <li><strong>Storage</strong> (10%): PV, PVC, StorageClass, volume mounts.</li>
        <li><strong>Troubleshooting</strong> (30%): Application, control plane, worker nodes, network.</li>
        <li>Plus: Security (RBAC, TLS, kubeconfig, NetworkPolicies), Logging/Monitoring, Application Lifecycle
          (rollouts, ConfigMaps, Secrets).</li>
        <li><strong>2025 curriculum additions:</strong> Helm and Kustomize for cluster components (sections 12, 13); Gateway API with Ingress; CRDs and operators; extension interfaces (CNI, CSI, CRI); CoreDNS; workload autoscaling (HPA); PodDisruptionBudgets.</li>
      </ul>
      <h3>Course Release Notes</h3>
      <p>This guide aligns with the CKA 2025â€“2026 curriculum. Updates include: Helm & Kustomize (sections 12â€“13), Gateway API, Admission Controllers, HPA/VPA, PodDisruptionBudgets, CoreDNS, and extension interfaces (CNI, CSI, CRI).</p>

      <h3>Certification</h3>
      <p>Certified Kubernetes Administrator (CKA): exam curriculum, candidate handbook, and exam tips are published by
        the Linux Foundation / CNCF. Use code <code>KUBERNETES15</code> when registering for CKA or CKAD for a 15%
        discount.</p>

      <h3>The Kubernetes Trilogy</h3>
      <p>CKA (Administrator) â†’ CKAD (Application Developer) â†’ CKS (Security Specialist). Each builds on the previous; CKA is the foundation for cluster operations.</p>

      <h3>Notes & Resources</h3>
      <p>Use the notes panel (N) in this guide to capture your own notes per section. Bookmark <a href="https://kubernetes.io/docs/" target="_blank" rel="noopener">kubernetes.io/docs</a> for the exam.</p>

      <h3>FAQ</h3>
      <ul>
        <li><strong>How long is the CKA exam?</strong> 2 hours, performance-based.</li>
        <li><strong>Can I use documentation?</strong> Yes, one tab to kubernetes.io/docs.</li>
        <li><strong>Passing score?</strong> 66%.</li>
        <li><strong>What's next after CKA?</strong> CKAD (Application Developer) or CKS (Security Specialist).</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 01 Introduction &amp; Foundations</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 01 Introduction & Foundations" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 02 Core Concepts -->
      <h2 id="section-02">02 â€” Core Concepts</h2>

      <div class="cka-definition">
        <div class="cka-def-term">Control Plane</div>
        <div class="cka-def-body">The brain of the cluster. Hosts kube-apiserver, etcd, kube-scheduler, kube-controller-manager, and optionally cloud-controller-manager. Manages cluster state, scheduling, and reconciliation. Can run as static pods or systemd services.</div>
      </div>

      <div class="cka-definition">
        <div class="cka-def-term">Worker Node</div>
        <div class="cka-def-body">A machine (VM or bare metal) that runs your workloads. Runs kubelet, kube-proxy, and a container runtime (containerd/CRI-O). Hosts Pods and communicates with the control plane via the API server.</div>
      </div>

      <h3>Cluster Architecture</h3>
      <p>A Kubernetes cluster consists of a <strong>Control Plane</strong> (the brain) and <strong>Worker Nodes</strong>
        (the muscle).</p>
      <ul>
        <li><strong>Control Plane:</strong> Manages the state of the cluster. It makes global decisions (scheduling),
          detects and responds to cluster events (starting new pods), and stores the cluster configuration.</li>
        <li><strong>Worker Node:</strong> A machine (VM or physical) that runs containerized applications. It hosts the
          Pods and communicates with the control plane via the kubelet.</li>
      </ul>

      <h3>Control Plane Components</h3>

      <h4>kube-apiserver</h4>
      <p>The <strong>front-end</strong> of the Kubernetes control plane. It exposes the Kubernetes API (REST). It is the
        <strong>only</strong> component that communicates directly with <strong>etcd</strong>.
        <br><em>Key function:</em> Authenticates, validates, and processes all REST requests (internal & external). It
        is designed to scale horizontally.
      </p>

      <h4>Kube API Server</h4>
      <p>Same as kube-apiserver â€” the front-end API for the cluster. All components communicate through it.</p>

      <h4>etcd / ETCD in Kubernetes</h4>
      <div class="cka-definition">
        <div class="cka-def-term">etcd</div>
        <div class="cka-def-body">Distributed key-value store used as Kubernetes' backing store. Uses RAFT consensus. Only kube-apiserver talks to etcd. CKA exam: know <code>etcdctl</code>/<code>etcdutl</code> for snapshot backup/restore, <code>--endpoints</code>, and <code>ETCDCTL_API=3</code>.</div>
      </div>
      <p>A consistent and highly-available <strong>key-value store</strong> used as Kubernetes' backing store for all
        cluster data.
        <br><em>Pro Tip:</em> It uses the <strong>RAFT algorithm</strong> for consensus. It is the "single source of
        truth" â€” if data is in etcd, it exists; otherwise, it does not.
      </p>
      <h4>ETCD Commands</h4>
      <p>Use <code>etcdctl</code> (etcd v3) or <code>etcdutl</code> for snapshots. Set <code>ETCDCTL_API=3</code>. See Cluster Maintenance for backup/restore.</p>
      <figure class="diagram">
        <img src="assets/k8s-etcd-raft.png" alt="etcd RAFT consensus: Leader replicates writes to Followers" width="560" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>etcd RAFT:</strong> Odd number of nodes (3 or 5). Leader handles writes and replicates log to followers. Majority must ack before commit.</figcaption>
      </figure>

      <h4>Kube Scheduler</h4>
      <p>Watches for newly created Pods with no assigned node and selects a node for them to run on.
        <br><em>Process:</em>
        <br>1. <strong>Filtering:</strong> Ruling out nodes that don't meet requirements (CPU/RAM, Taints, Affinity).
        <br>2. <strong>Scoring:</strong> Ranking the remaining nodes to find the best fit.
      </p>

      <h4>Kube Controller Manager</h4>
      <p>A daemon that runs controller processes. Logically, each controller is a separate process, but they are
        compiled into a single binary.
        <br><em>Responsibility:</em> <strong>Reconciliation Loop</strong>. It continuously watches the <em>current
          state</em> and compares it to the <em>desired state</em>, making changes to match them.
        <br><em>Examples:</em> Node Controller (notices down nodes), Replication Controller (maintains pod count),
        Endpoints Controller.
      </p>

      <h4>cloud-controller-manager</h4>
      <p>Embeds cloud-specific control logic. It lets you link your cluster into your cloud provider's API (e.g., AWS,
        Azure, GCP). It handles things like Node instances (cloud VMs), LoadBalancers, and Routes.</p>

      <h3>Node Components</h3>

      <h4>Kubelet</h4>
      <p>An agent that runs on each node. It registers the node with the API server and ensures that containers are
        running in a Pod.
        <br><em>Mechanism:</em> It takes a set of PodSpecs (from API server or static files) and ensures the containers
        described are running and healthy. It does <strong>not</strong> manage containers not created by Kubernetes.
      </p>

      <h4>Kube Proxy</h4>
      <p>A network proxy running on each node. It maintains network rules that allow network communication to your Pods
        from inside or outside the cluster.
        <br><em>Implementation:</em> Commonly uses <strong>iptables</strong> or <strong>IPVS</strong> to forward traffic
        to backend Pods (Service abstraction).
      </p>

      <h4>Container Runtime</h4>
      <p>The software responsible for running containers. Kubernetes supports any runtime that adheres to the
        <strong>Container Runtime Interface (CRI)</strong>.
      </p>

      <h3>Docker vs ContainerD (CRI)</h3>
      <h4>Docker Deprecation Note</h4>
      <p>Kubernetes v1.24+ no longer supports Docker Engine as a container runtime. Docker images still work â€” they use the OCI format. Use containerd or CRI-O instead.</p>
      <p><strong>Fact:</strong> Kubernetes removed the "dockershim" in v1.24. This means Kubernetes can no longer use
        the Docker Engine directly as a runtime. It now uses <strong>containerd</strong> or <strong>CRI-O</strong> via
        the CRI.</p>
      <ul>
        <li><strong>containerd:</strong> An industry-standard container runtime. It was part of Docker but is now
          independent.</li>
        <li><strong>crictl:</strong> A CLI tool for CRI-compatible container runtimes. Used mainly for debugging on the
          node.</li>
        <li><strong>nerdctl:</strong> A Docker-compatible CLI for containerd (supports <code>docker run</code> style
          commands).</li>
      </ul>
      <div class="snippet-label">Useful crictl commands</div>
      <pre><code>crictl ps -a                        # list all containers
crictl images                       # list images
crictl pods                         # list pods (sandbox containers)
crictl logs &lt;container-id&gt;          # view logs directly from runtime
crictl inspect &lt;container-id&gt;       # deep dive into container status</code></pre>

      <figure class="diagram">
        <img src="assets/k8s-enterprise-architecture.png" alt="Kubernetes Cluster Architecture" width="960" height="560"
          style="max-width:100%;height:auto;display:block;">
        <figcaption>
          <strong>Kubernetes Cluster Architecture</strong> â€” The <strong>kube-apiserver</strong> is the single gateway:
          every component communicates exclusively through it. <strong>etcd</strong> is the sole persistent store and is
          only accessed by the API server. Each worker node runs a <strong>kubelet</strong> (receives pod specs via
          watch), <strong>kube-proxy</strong> (programs iptables/IPVS for Service routing), and a <strong>container
            runtime</strong> (executes containers via CRI).
        </figcaption>
      </figure>

      <h3>Workloads</h3>

      <h4>Pods</h4>
      <div class="cka-definition">
        <div class="cka-def-term">Pod</div>
        <div class="cka-def-body">Smallest deployable unit. One or more containers sharing network and storage. Ephemeral â€” when a pod dies, it is not resurrected; a controller creates a new one. CKA: use <code>kubectl run ... --dry-run=client -o yaml</code> to generate YAML fast.</div>
      </div>
      <p>The smallest deployable object in Kubernetes. A Pod determines how to run a container.
        <br><em>Key Concept:</em> Containers in a pod share the <strong>same network namespace</strong> (IP address),
        storage volumes, and process namespace (sometimes).
        <br><em>Life:</em> Pods are <strong>ephemeral</strong>. They are born, they run, and they die. They are not
        resurrected; a new one is created to replace them.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-pod-lifecycle.png" alt="Pod lifecycle states: Pending, Running, Succeeded, Failed, Unknown" width="480" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption>Pod lifecycle: <strong>Pending</strong> (waiting for scheduling) &rarr; <strong>Running</strong> (containers running) &rarr; <strong>Succeeded</strong> / <strong>Failed</strong> / <strong>Unknown</strong>.</figcaption>
      </figure>

      <h4>Pods with YAML</h4>
      <p>Define pods in YAML with <code>apiVersion</code>, <code>kind: Pod</code>, <code>metadata</code>, and <code>spec.containers</code>. Use <code>kubectl apply -f pod.yaml</code> or generate with <code>kubectl run ... --dry-run=client -o yaml</code>.</p>
      <div class="snippet-label">Run a pod (imperative)</div>
      <pre><code>kubectl run nginx --image nginx
kubectl get pods -wide</code></pre>

      <h4>ReplicaSet</h4>
      <p>Ensures a specified number of pod replicas are running at any given time.
        <br><em>Selector:</em> Uses <code>selector</code> to match Pod labels. If a pod dies, RS starts a new one.
      </p>

      <h4>Deployments</h4>
      <div class="cka-definition">
        <div class="cka-def-term">Deployment</div>
        <div class="cka-def-body">Manages ReplicaSets for stateless apps. Declarative updates: rolling update, rollback. CKA: <code>kubectl rollout status/history/undo</code>, <code>kubectl set image</code>, <code>kubectl scale</code>. Deployment creates ReplicaSet which creates Pods.</div>
      </div>
      <p>The standard way to manage stateless applications. A Deployment manages ReplicaSets and provides declarative
        updates (Rolling Updates, Rollbacks).
        <br><em>Capabilities:</em> Scaling, pausing/resuming updates, rolling back to previous revisions.
      </p>

      <div class="snippet-label">Create Deployment</div>
      <pre><code>kubectl create deployment webapp --image=nginx --replicas=3
kubectl get deploy,rs,po</code></pre>

      <h4>Scaling Applications</h4>
      <p>Scale: <code>kubectl scale --replicas=5 deployment/webapp</code> or edit via
        <code>kubectl edit deploy webapp</code>. HPA for auto-scaling (see Application Lifecycle).
      </p>
      <figure class="diagram">
        <img src="assets/k8s-deployment-replicaset-pod.png" alt="Workloads hierarchy: Deployment â†’ ReplicaSet â†’ Pod" width="400" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Workloads hierarchy:</strong> Deployment manages ReplicaSets; ReplicaSet maintains pod count via selector; Pod runs containers.</figcaption>
      </figure>

      <h3>Kubectl & Configuration</h3>
      <h4>Imperative vs Declarative</h4>
      <p>Imperative: <code>kubectl run</code>, <code>kubectl create</code> â€” direct commands. Declarative: <code>kubectl apply -f file.yaml</code> â€” desired state in YAML. Exam tip: use imperative with <code>--dry-run=client -o yaml</code> to generate, then edit.</p>
      <h4>Kubectl Explain</h4>
      <p>Documentation in terminal: <code>kubectl explain pod</code>, <code>kubectl explain pod.spec.containers</code>. Use <code>--recursive</code> for full structure.</p>
      <h4>Kubectl Apply</h4>
      <p>Declarative updates: <code>kubectl apply -f file.yaml</code>. Creates or updates resources. Use <code>-f -</code> to read from stdin.</p>
      <h4>Imperative commands â€” exam speed reference</h4>
      <p>In the CKA exam, speed matters. Use imperative commands with <code>--dry-run=client -o yaml</code> to
        generate
        YAML quickly, then edit as needed.</p>
      <div class="snippet-label">Generate YAML templates fast</div>
      <pre><code># Pod
kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml

# Deployment
kubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml > deploy.yaml

# Service (ClusterIP)
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

# Service (NodePort)
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

# Job
kubectl create job my-job --image=busybox --dry-run=client -o yaml -- echo hello

# CronJob
kubectl create cronjob my-cron --image=busybox --schedule="*/5 * * * *" --dry-run=client -o yaml -- echo hi

# ConfigMap
kubectl create configmap app-config --from-literal=KEY=VALUE --dry-run=client -o yaml

# Secret
kubectl create secret generic app-secret --from-literal=PASSWORD=pass --dry-run=client -o yaml

# ServiceAccount
kubectl create serviceaccount my-sa --dry-run=client -o yaml

# Namespace
kubectl create namespace dev

# Role & RoleBinding
kubectl create role pod-reader --verb=get,list --resource=pods -n dev
kubectl create rolebinding dev-read --role=pod-reader --user=jane -n dev

# ClusterRole & ClusterRoleBinding
kubectl create clusterrole node-reader --verb=get,list --resource=nodes
kubectl create clusterrolebinding node-read --clusterrole=node-reader --user=jane

# Ingress
kubectl create ingress my-ingress --rule="host/path=svc:80" --dry-run=client -o yaml</code></pre>
      <div class="snippet-label">Quick edit and apply pattern</div>
      <pre><code># Generate â†’ Edit â†’ Apply
kubectl run mypod --image=nginx --dry-run=client -o yaml > mypod.yaml
vi mypod.yaml   # add labels, resources, volumes, etc.
kubectl apply -f mypod.yaml

# Edit running resource
kubectl edit deployment nginx

# Replace (force update)
kubectl replace --force -f pod.yaml</code></pre>

      <h3>Namespaces</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Namespace</div>
        <div class="cka-def-body">Virtual cluster within a physical cluster. Isolates resources; most objects are namespaced. System namespaces: <code>kube-system</code>, <code>kube-public</code>, <code>kube-node-lease</code>. CKA: <code>-n</code>/<code>--namespace</code>, <code>--all-namespaces</code>, <code>kubectl config set-context --current --namespace=dev</code>.</div>
      </div>
      <p>Objects live in a namespace (default is <code>default</code>). List pods in a namespace:
        <code>kubectl get pods --namespace=kube-system</code> or <code>-n kube-system</code>. Create in a namespace:
        <code>kubectl create -f pod-definition.yaml --namespace=dev</code>. Add <code>namespace: dev</code> under
        <code>metadata</code> in the YAML to fix the namespace.
      </p>
      <div class="snippet-label">Create namespace</div>
      <pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev</code></pre>
      <pre><code>kubectl create -f namespace-dev.yaml
# or
kubectl create namespace dev</code></pre>
      <p>Switch default namespace:
        <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>. All namespaces:
        <code>kubectl get pods --all-namespaces</code>.
      </p>
      <div class="snippet-label">ResourceQuota (limit resources in a namespace)</div>
      <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi</code></pre>
      <pre><code>kubectl create -f compute-quota.yaml</code></pre>

      <h3>Services</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Service</div>
        <div class="cka-def-body">Abstracts a set of Pods behind a stable network endpoint. Types: ClusterIP (internal VIP, default), NodePort (node IP + port 30000â€“32767), LoadBalancer (cloud LB). Selector matches Pod labels to form Endpoints.</div>
      </div>
      <h4>Services & Networking Basics</h4>
      <p>Three types: <strong>NodePort</strong> (expose on node port), <strong>ClusterIP</strong> (virtual IP inside
        cluster), <strong>LoadBalancer</strong> (cloud load balancer).</p>
      <h4>ClusterIP</h4>
      <p>Default type. Internal VIP; only reachable from within the cluster. Use for backend services.</p>
      <h4>LoadBalancer</h4>
      <p>Cloud-provider LB; creates external IP. Requires cloud integration (e.g. AWS ELB, GCP LB).</p>
      <div class="snippet-label">NodePort service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  selector:
    app: myapp
    type: front-end</code></pre>
      <div class="snippet-label">ClusterIP service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: back-end</code></pre>
      <pre><code>kubectl create -f service-definition.yaml
kubectl get services
curl http://&lt;node-ip&gt;:30008   # for NodePort</code></pre>
      <figure class="diagram">
        <img src="assets/k8s-service-types.png" alt="Kubernetes Service Types: NodePort, ClusterIP, LoadBalancer" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption>Service types: NodePort (expose on node port), ClusterIP (internal VIP), LoadBalancer (cloud LB).
        </figcaption>
      </figure>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 02 Core Concepts</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 02 Core Concepts" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 03 Scheduling -->
      <h2 id="section-03">03 &mdash; Scheduling</h2>
      <h3>How Scheduling Works</h3>
      <p>The <strong>kube-scheduler</strong> is a control-plane component that continuously watches the API server for
        newly created Pods that have no <code>spec.nodeName</code> set. When it finds one, it runs through a
        multi-phase pipeline to decide which node is the best fit:</p>
      <ul>
        <li><strong>1. Filtering (Predicates):</strong> Eliminates nodes that <em>cannot</em> run the pod. Common
          reasons: insufficient CPU/memory, unmatched taints, anti-affinity conflicts, unresolvable volume topology,
          node is cordoned. After filtering you get a shortlist of <em>feasible</em> nodes.</li>
        <li><strong>2. Scoring (Priorities):</strong> Ranks each feasible node 0&ndash;100 using weighted plugins:
          <code>LeastAllocated</code> (prefer emptier nodes), <code>ImageLocality</code> (prefer nodes that already
          have the image), <code>NodeAffinity</code>, <code>InterPodAffinity</code>,
          <code>PodTopologySpread</code>. The node with the highest total score wins.
        </li>
        <li><strong>3. Binding:</strong> The scheduler creates a <strong>Binding</strong> object that writes
          <code>spec.nodeName</code> on the pod. The kubelet on that node then pulls the images and starts the
          containers.
        </li>
      </ul>
      <p><strong>Key detail:</strong> If <em>no</em> feasible nodes remain after filtering, the pod stays in
        <code>Pending</code> state with a <code>FailedScheduling</code> event. Use
        <code>kubectl describe pod &lt;name&gt;</code> to check events.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-scheduling-flow.png" alt="Pod scheduling flow: Pod created â†’ Scheduler â†’ Binding â†’ Kubelet" width="560" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>Scheduling flow: Filter + Score &rarr; Binding object &rarr; kubelet starts the pod.</figcaption>
      </figure>
      <div class="snippet-label">Debug scheduling decisions</div>
      <pre><code># Why is my pod pending? Check events
kubectl describe pod my-pending-pod | grep -A 10 Events

# See scheduler logs for detailed scoring
kubectl logs -n kube-system kube-scheduler-controlplane

# Check which scheduler placed a pod
kubectl get events --field-selector reason=Scheduled,involvedObject.name=my-pod</code></pre>

      <h3>Manual Scheduling (nodeName)</h3>
      <p>Pin a pod to a specific node by setting <code>spec.nodeName</code>. This <strong>completely bypasses the
          scheduler</strong> &mdash; no filtering, no scoring, no admission of scheduling plugins.
        <br><strong>Important rules:</strong>
      </p>
      <ul>
        <li><code>nodeName</code> can only be set at <strong>creation time</strong>. You cannot change it on a running
          pod &mdash; you must delete and recreate.</li>
        <li>The node must exist; if it doesn&rsquo;t, the pod is rejected immediately.</li>
        <li>Taints on the target node are <strong>still respected</strong> &mdash; the kubelet will reject the pod if
          tolerations don&rsquo;t match.</li>
        <li>In the CKA exam, the &ldquo;force reschedule&rdquo; pattern is:
          <code>kubectl get pod X -o yaml &gt; pod.yaml</code> &rarr; edit &rarr; delete old &rarr; recreate.
        </li>
      </ul>
      <div class="snippet-label">Pod with hardcoded nodeName</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  nodeName: node02        # bypasses scheduler</code></pre>
      <div class="snippet-label">Binding object (alternative &mdash; POST to API)</div>
      <pre><code>apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02</code></pre>
      <div class="snippet-label">Force-move a running pod to a different node</div>
      <pre><code># Step 1 - Export the running pod YAML
kubectl get pod nginx -o yaml &gt; nginx-moved.yaml

# Step 2 - Edit: change spec.nodeName to the target node, remove status/uid/resourceVersion
vi nginx-moved.yaml

# Step 3 - Delete the original and create the new one
kubectl delete pod nginx
kubectl apply -f nginx-moved.yaml

# Step 4 - Verify placement
kubectl get pod nginx -o wide</code></pre>

      <h3>Labels and Selectors</h3>
      <p>Labels are <strong>key-value pairs</strong> attached to Kubernetes objects. They are the primary mechanism for
        organizing and selecting groups of objects. Selectors filter objects by label and are critical for how
        ReplicaSets, Services, Deployments, and NetworkPolicies discover their target objects.</p>
      <p><strong>Two types of selectors:</strong></p>
      <ul>
        <li><strong>Equality-based:</strong> <code>=</code>, <code>==</code>, <code>!=</code> &mdash; used in
          <code>nodeSelector</code> and <code>kubectl --selector</code>.
        </li>
        <li><strong>Set-based:</strong> <code>In</code>, <code>NotIn</code>, <code>Exists</code>,
          <code>DoesNotExist</code> &mdash; used in <code>matchExpressions</code> (Deployments, Node Affinity).
        </li>
      </ul>
      <div class="snippet-label">Pod with labels</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    env: production
    tier: frontend
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080</code></pre>
      <div class="snippet-label">Deployment using matchLabels selector</div>
      <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
spec:
  replicas: 3
  selector:
    matchLabels:            # these labels MUST match template.metadata.labels
      app: App1
      tier: frontend
  template:
    metadata:
      labels:
        app: App1
        tier: frontend
    spec:
      containers:
      - name: webapp
        image: simple-webapp</code></pre>
      <pre><code># Filter by single label
kubectl get pods --selector app=App1

# Filter by multiple labels (AND logic)
kubectl get pods --selector app=App1,env=production

# All resource types matching a label
kubectl get all --selector env=production

# Count pods matching a selector
kubectl get pods --selector env=production --no-headers | wc -l

# Add/change a label on a running pod
kubectl label pod nginx tier=frontend
kubectl label pod nginx tier=backend --overwrite

# Remove a label (trailing minus)
kubectl label pod nginx tier-

# Show labels as columns
kubectl get pods --show-labels
kubectl get pods -L app,env    # show specific labels as columns</code></pre>
      <p><strong>Annotations</strong> (under <code>metadata.annotations</code>) store non-identifying metadata like
        build version, last-applied-config, contact info &mdash; they are <strong>NOT</strong> used for selection.
        Common annotations include <code>kubernetes.io/change-cause</code> (rollout history) and
        <code>kubectl.kubernetes.io/last-applied-configuration</code>.
      </p>

      <h3>Taints and Tolerations</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Taints & Tolerations</div>
        <div class="cka-def-body"><strong>Taints</strong> on nodes repel pods; <strong>tolerations</strong> on pods allow them onto tainted nodes. Effects: NoSchedule (block new), PreferNoSchedule (soft avoid), NoExecute (block + evict existing). CKA: <code>kubectl taint nodes node01 key=value:NoSchedule</code>, remove with <code>key=value:NoSchedule-</code>.</div>
      </div>
      <p>Taints on <strong>nodes</strong> repel pods. Tolerations on <strong>pods</strong> allow them onto tainted
        nodes. Think of taints as a &ldquo;keep out&rdquo; sign on the node and tolerations as a &ldquo;VIP
        pass&rdquo; on the pod.</p>
      <p><em>Critical: Tolerations do NOT guarantee the pod will land on that node &mdash; they only
          &ldquo;allow&rdquo; it. Use Node Affinity to <strong>attract</strong> pods to specific nodes.</em></p>
      <figure class="diagram">
        <img src="assets/k8s-taints-tolerations.png" alt="Taints repel pods from nodes; Tolerations allow pods onto tainted nodes" width="500" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Taints and Tolerations:</strong> Node taints &ldquo;keep out&rdquo; pods; pod tolerations act as a &ldquo;VIP pass&rdquo; to land on tainted nodes.</figcaption>
      </figure>
      <ul>
        <li><strong>NoSchedule:</strong> New pods without a matching toleration are <em>never</em> scheduled here.
          Existing pods stay untouched.</li>
        <li><strong>PreferNoSchedule:</strong> Scheduler <em>tries</em> to avoid this node, but if no other nodes are
          available the pod can still land here. Soft constraint.</li>
        <li><strong>NoExecute:</strong> New pods blocked <strong>AND</strong> existing pods without the toleration are
          <strong>evicted immediately</strong>. You can add <code>tolerationSeconds</code> to allow a grace period
          before eviction.
        </li>
      </ul>
      <pre><code># Apply a taint to a node
kubectl taint nodes node01 app=blue:NoSchedule

# Apply multiple taints
kubectl taint nodes node01 env=staging:NoSchedule
kubectl taint nodes node01 dedicated=gpu:NoExecute

# Check taints on a node
kubectl describe node node01 | grep -i taint

# Remove a specific taint (trailing minus removes it)
kubectl taint nodes node01 app=blue:NoSchedule-

# Remove ALL taints with a specific key (regardless of effect)
kubectl taint nodes node01 app-</code></pre>
      <div class="snippet-label">Pod with toleration (Equal operator)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "app"
    operator: "Equal"     # matches only when key=value
    value: "blue"
    effect: "NoSchedule"</code></pre>
      <div class="snippet-label">Pod with toleration (Exists operator &mdash; matches any value)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: tolerant-pod
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  # Tolerate any taint with key "dedicated", regardless of value
  - key: "dedicated"
    operator: "Exists"
    effect: "NoSchedule"
  # Tolerate NoExecute with a grace period (stay 300s then evict)
  - key: "node.kubernetes.io/not-ready"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 300</code></pre>
      <p><strong>Control-plane taint:</strong> Master/control plane nodes automatically receive
        <code>node-role.kubernetes.io/control-plane:NoSchedule</code> so user workloads never land there.
        To allow scheduling on control plane (single-node cluster):
      </p>
      <pre><code># Remove the control-plane taint (single-node clusters)
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-</code></pre>

      <h3>Node Selectors</h3>
      <p>The simplest form of node selection constraint. Label a node, then reference those labels with
        <code>nodeSelector</code> in the pod spec. The scheduler only considers nodes that match
        <strong>all</strong> the specified labels.
      </p>
      <p><strong>Limitation:</strong> Supports only equality matching &mdash; you cannot say &ldquo;Large OR
        Medium&rdquo; or &ldquo;NOT Small&rdquo;. For complex expressions, use Node Affinity.</p>
      <pre><code># Label a node
kubectl label nodes node01 size=Large

# Label multiple nodes at once
kubectl label nodes node01 node02 disktype=ssd

# Verify the label
kubectl get nodes --show-labels | grep size

# Check labels on a specific node
kubectl describe node node01 | grep -A 5 Labels

# Remove a label
kubectl label nodes node01 size-</code></pre>
      <div class="snippet-label">Pod with nodeSelector</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: data-processor
spec:
  containers:
  - name: data-processor
    image: data-processor
  nodeSelector:
    size: Large       # only scheduled on nodes with this label</code></pre>
      <div class="snippet-label">Deployment with nodeSelector (common exam pattern)</div>
      <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gpu-app
  template:
    metadata:
      labels:
        app: gpu-app
    spec:
      containers:
      - name: cuda
        image: nvidia/cuda:latest
      nodeSelector:
        accelerator: nvidia-gpu   # all replicas land on GPU nodes</code></pre>

      <h3>Node Affinity</h3>
      <p>More expressive than <code>nodeSelector</code>. Supports <code>In</code>, <code>NotIn</code>,
        <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code>, <code>Lt</code> operators and both
        <strong>hard</strong> and <strong>soft</strong> rules.
      </p>
      <ul>
        <li><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong> <em>Hard rule.</em> Pod stays
          <code>Pending</code> forever if no node matches. Running pods are NOT evicted if node labels change
          later ("IgnoredDuringExecution").
        </li>
        <li><strong>preferredDuringSchedulingIgnoredDuringExecution:</strong> <em>Soft rule.</em> Scheduler tries to
          match but places pod elsewhere if no nodes qualify. Use <code>weight</code> (1&ndash;100) to
          rank preferences.</li>
      </ul>
      <div class="snippet-label">Node Affinity &mdash; In operator (node must have size=Large OR Medium)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: data-processor
spec:
  containers:
  - name: data-processor
    image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50               # 1-100; higher = more preferred
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd</code></pre>
      <div class="snippet-label">Node Affinity &mdash; NotIn operator (avoid Small nodes)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: no-small-nodes
spec:
  containers:
  - name: app
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: NotIn               # NOT Small
            values:
            - Small</code></pre>
      <div class="snippet-label">Node Affinity &mdash; Exists operator (node has the key, any value)</div>
      <pre><code>  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu                      # any node that has a "gpu" label
            operator: Exists              # value doesn't matter</code></pre>
      <p><strong>Combining Taints + Node Affinity (Dedicated Nodes Pattern):</strong></p>
      <ul class="step-list">
        <li><strong>Step 1 &mdash; Taint the node:</strong>
          <code>kubectl taint nodes node01 dedicated=team-a:NoSchedule</code>
        </li>
        <li><strong>Step 2 &mdash; Label the node:</strong>
          <code>kubectl label nodes node01 dedicated=team-a</code>
        </li>
        <li><strong>Step 3 &mdash; Pod spec:</strong> Add both a toleration for the taint AND a node affinity rule
          for the label. This ensures <em>only</em> your pods can run there, and they always prefer that node.</li>
      </ul>
      <h4>Taints vs Affinity</h4>
      <p>Taints <em>repel</em> pods (unless tolerated); Affinity <em>attracts</em> pods to nodes. Use both for dedicated nodes: taint to block others, affinity to pull your workload.</p>
      <figure class="diagram">
        <img src="assets/k8s-taints-vs-affinity.png" alt="Taints repel pods; Node Affinity attracts pods" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Taints vs Affinity:</strong> Taints = keep out (unless tolerated). Node Affinity = prefer this node.</figcaption>
      </figure>

      <h3>Resource Limits</h3>
      <h4>Editing Pods & Deployments</h4>
      <p>Use <code>kubectl edit deployment &lt;name&gt;</code> or <code>kubectl edit pod &lt;name&gt;</code>. For immutable pod fields, delete and recreate. Deployments support in-place edits (triggers rollout).</p>
      <p><strong>Requests</strong> &mdash; the <em>guaranteed</em> allocation. The scheduler uses requests to decide
        which node has enough capacity.<br>
        <strong>Limits</strong> &mdash; the <em>maximum</em> a container can consume at runtime.
      </p>
      <p><strong>CPU vs Memory behaviour:</strong></p>
      <ul>
        <li>CPU exceeds limit &rarr; <strong>throttled</strong> (never killed for CPU). Container gets fewer cycles.
        </li>
        <li>Memory exceeds limit &rarr; container is <strong>OOMKilled</strong> (exit code 137) and restarted per
          <code>restartPolicy</code>.
        </li>
        <li>No requests set &rarr; scheduler treats it as 0 (pod placed anywhere &mdash; risky for stability).</li>
      </ul>
      <p><strong>CPU units:</strong> <code>1</code> = 1 vCPU/core. <code>500m</code> = 0.5 CPU = 500 millicores.
        Minimum is <code>1m</code>.<br>
        <strong>Memory units:</strong> <code>Mi</code> (mebibytes), <code>Gi</code> (gibibytes). Use power-of-2
        units, not <code>M</code>/<code>G</code> (decimal).
      </p>
      <div class="snippet-label">Pod with resource requests and limits</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "64Mi"        # scheduler looks for node with &ge;64Mi free
        cpu: "250m"           # 0.25 vCPU (millicores)
      limits:
        memory: "128Mi"       # OOMKilled if exceeded
        cpu: "500m"           # throttled if exceeded</code></pre>
      <p><strong>QoS Classes</strong> (determined automatically by Kubernetes):</p>
      <ul>
        <li><strong>Guaranteed:</strong> Every container has requests == limits for both CPU and memory. Highest
          priority &mdash; last to be evicted.</li>
        <li><strong>Burstable:</strong> At least one container has requests &lt; limits. Middle priority.</li>
        <li><strong>BestEffort:</strong> No requests or limits set at all. First to be evicted under pressure.</li>
      </ul>
      <pre><code># Check QoS class of a pod
kubectl get pod resource-demo -o jsonpath='{.status.qosClass}'

# Check resource usage on nodes
kubectl top nodes
kubectl top pods --sort-by=memory

# Describe node to see resource allocation
kubectl describe node node01 | grep -A 10 "Allocated resources"</code></pre>

      <h3>LimitRange</h3>
      <p>A <strong>namespace-scoped</strong> policy that automatically injects default resource requests/limits into
        containers that don&rsquo;t specify them, and enforces min/max boundaries.</p>
      <p><strong>Key behaviour:</strong></p>
      <ul>
        <li>Applied at <strong>admission time</strong> (when pod is created) &mdash; existing pods are NOT retroactively
          changed.</li>
        <li>If a container specifies only <code>limits</code> but not <code>requests</code>, requests default to the
          limit value (not the <code>defaultRequest</code>).</li>
        <li>If a container specifies resources that exceed <code>max</code> or fall below <code>min</code>, the pod
          creation is <strong>rejected</strong>.</li>
        <li>Can also be applied to Pods (total resources) and PersistentVolumeClaims (storage size).</li>
      </ul>
      <div class="snippet-label">LimitRange for containers</div>
      <pre><code>apiVersion: v1
kind: LimitRange
metadata:
  name: mem-cpu-limits
  namespace: dev
spec:
  limits:
  - type: Container
    default:              # used when container omits limits
      cpu: "500m"
      memory: "128Mi"
    defaultRequest:       # used when container omits requests
      cpu: "250m"
      memory: "64Mi"
    max:                  # container cannot request/limit above this
      cpu: "2"
      memory: "1Gi"
    min:                  # container must request/limit at least this
      cpu: "100m"
      memory: "16Mi"</code></pre>
      <pre><code>kubectl apply -f limitrange.yaml
kubectl describe limitrange mem-cpu-limits -n dev

# Test: create a pod without resources and check what defaults are applied
kubectl run test-lr --image=nginx -n dev
kubectl get pod test-lr -n dev -o jsonpath='{.spec.containers[0].resources}'</code></pre>

      <h3>ResourceQuota</h3>
      <p>A <strong>namespace-scoped</strong> policy that limits the <em>total</em> amount of resources (CPU, memory,
        storage, object count) that can be consumed in a namespace. Unlike LimitRange (per-container), ResourceQuota
        constrains the entire namespace.</p>
      <div class="snippet-label">ResourceQuota YAML</div>
      <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "4"          # total CPU requests across all pods
    requests.memory: "4Gi"     # total memory requests
    limits.cpu: "8"            # total CPU limits
    limits.memory: "8Gi"       # total memory limits
    pods: "10"                 # max 10 pods in this namespace
    services: "5"
    persistentvolumeclaims: "4"</code></pre>
      <pre><code># Apply and check quota usage
kubectl apply -f resourcequota.yaml
kubectl get resourcequota compute-quota -n dev
kubectl describe resourcequota compute-quota -n dev

# When quota is active, every pod MUST specify requests/limits
# or use a LimitRange to set defaults â€” otherwise creation fails</code></pre>


      <h3>DaemonSets</h3>
      <div class="cka-definition">
        <div class="cka-def-term">DaemonSet</div>
        <div class="cka-def-body">Ensures one pod per node (or subset via nodeSelector). Used for log collectors, node monitoring, CNI, CSI node plugin. Auto-deploys to new nodes. CKA: <code>kubectl get ds</code>, tolerations for control-plane nodes, <code>nodeSelector</code> for GPU-only nodes.</div>
      </div>
      <p>Ensures <strong>exactly one copy of a Pod</strong> runs on every node (or a subset of nodes selected via
        <code>nodeSelector</code> or node affinity). When a new node is added to the cluster, the DaemonSet
        controller automatically creates a pod on it. When a node is removed, the pod is garbage collected.
      </p>
      <p><strong>Common use cases:</strong></p>
      <ul>
        <li><strong>Log collectors:</strong> Fluentd, Filebeat, Fluent Bit</li>
        <li><strong>Node monitoring:</strong> Prometheus node-exporter, Datadog agent</li>
        <li><strong>Networking:</strong> kube-proxy, CNI plugins (Calico, Flannel, Weave)</li>
        <li><strong>Storage:</strong> CSI node drivers (like EBS CSI, Ceph CSI)</li>
      </ul>
      <p><strong>How it works internally:</strong> The DaemonSet controller uses the default scheduler (since K8s
        1.12+).
        It sets <code>nodeAffinity</code> on each pod to target a specific node, so the scheduler places it correctly.
        This is why DaemonSet pods show the <code>default-scheduler</code> in events.</p>
      <div class="snippet-label">DaemonSet YAML</div>
      <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: monitoring-daemon       # must match template labels
  template:
    metadata:
      labels:
        app: monitoring-daemon     # must match selector
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule         # also run on master nodes
      containers:
      - name: monitoring-agent
        image: prom/node-exporter
        ports:
        - containerPort: 9100
        resources:
          limits:
            memory: "200Mi"
            cpu: "100m"</code></pre>
      <div class="snippet-label">DaemonSet on specific nodes only (nodeSelector)</div>
      <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-monitor
spec:
  selector:
    matchLabels:
      app: gpu-monitor
  template:
    metadata:
      labels:
        app: gpu-monitor
    spec:
      nodeSelector:
        accelerator: nvidia-gpu   # only nodes with GPU label
      containers:
      - name: dcgm-exporter
        image: nvcr.io/nvidia/k8s/dcgm-exporter:latest</code></pre>
      <pre><code># Create and manage DaemonSets
kubectl apply -f daemonset.yaml
kubectl get daemonsets -n kube-system
kubectl get ds -A                           # shorthand, all namespaces
kubectl describe daemonset monitoring-daemon -n kube-system

# Check which nodes have the DaemonSet pod
kubectl get pods -l app=monitoring-daemon -o wide

# Rollback a DaemonSet update
kubectl rollout undo daemonset monitoring-daemon -n kube-system</code></pre>

      <h3>Static Pods</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Static Pod</div>
        <div class="cka-def-body">Managed by kubelet from a manifest directory (<code>/etc/kubernetes/manifests/</code>). Not created by API server; appear as mirror pods. Control-plane components run as static pods via kubeadm. To delete: remove manifest file. CKA: check <code>staticPodPath</code> in kubelet config.</div>
      </div>
      <p>Pods managed <strong>directly by the kubelet</strong> on a specific node, without going through the API
        server or any controller. The kubelet watches a local manifest directory and automatically creates/deletes
        pods based on the YAML files it finds there.</p>
      <p><strong>Key characteristics:</strong></p>
      <ul>
        <li>Default path: <code>/etc/kubernetes/manifests/</code></li>
        <li>The API server shows them as read-only <strong>mirror pods</strong> (you can see them with
          <code>kubectl get pods</code> but cannot control them through the API).
        </li>
        <li>Control-plane components (etcd, kube-apiserver, controller-manager, scheduler) are all deployed as
          static pods on the control plane node via kubeadm.</li>
        <li>To delete: <strong>remove the manifest file</strong> from the directory &mdash;
          <code>kubectl delete pod</code> will NOT work because the kubelet recreates the mirror pod immediately.
        </li>
        <li><strong>Naming convention:</strong> Static pods have the node name appended, e.g.,
          <code>kube-apiserver-controlplane</code>, <code>etcd-controlplane</code>.
        </li>
      </ul>
      <div class="snippet-label">Configure staticPodPath in kubelet</div>
      <pre><code># /var/lib/kubelet/config.yaml
staticPodPath: /etc/kubernetes/manifests

# OR as a kubelet start flag:
--pod-manifest-path=/etc/kubernetes/manifests

# Apply changes
systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code></pre>
      <div class="snippet-label">Example static pod manifest (drop into manifests dir)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: static-nginx
  namespace: kube-system
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80</code></pre>
      <div class="snippet-label">Exam techniques for static pods</div>
      <pre><code># Static pods have the node name appended: kube-apiserver-controlplane
kubectl get pods -n kube-system

# Confirm it's static (ownerReferences.kind == Node)
kubectl get pod kube-apiserver-controlplane -n kube-system \
  -o jsonpath='{.metadata.ownerReferences[*].kind}'

# Locate the staticPodPath on a node
ps aux | grep kubelet | grep config
# then read the config file
cat /var/lib/kubelet/config.yaml | grep staticPodPath

# Create a static pod on a worker node via SSH
ssh node01
cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["sleep", "3600"]
EOF

# Delete a static pod: remove the file
rm /etc/kubernetes/manifests/static-busybox.yaml</code></pre>
      <p><strong>Static Pods vs DaemonSets:</strong> Both run one pod per node, but static pods are managed by the
        kubelet (no controller) while DaemonSets are managed by the DaemonSet controller via the API server.
        Static pods cannot be managed with <code>kubectl</code>.</p>

      <h3>Node Maintenance &mdash; Cordon, Drain, Uncordon</h3>
      <p>Safely move workloads off a node before maintenance (OS upgrade, kernel patch, reboot, hardware replacement).
        This is a <strong>very common CKA exam scenario</strong>.</p>
      <ul>
        <li><strong>cordon:</strong> Marks node <code>SchedulingDisabled</code>. No new pods placed; existing pods
          <strong>keep running</strong> undisturbed.
        </li>
        <li><strong>drain:</strong> Cordon + graceful eviction of all pods. Respects <code>PodDisruptionBudgets</code>.
          DaemonSet pods are ignored by default (<code>--ignore-daemonsets</code> flag needed to proceed).
          Pods with <code>emptyDir</code> volumes need <code>--delete-emptydir-data</code>.</li>
        <li><strong>uncordon:</strong> Re-enables scheduling on the node after maintenance. Note: previously evicted
          pods do <em>not</em> automatically come back &mdash; only new pods can be scheduled here.</li>
      </ul>
      <pre><code># Mark as unschedulable only (pods keep running)
kubectl cordon node01

# Evict pods and mark unschedulable
kubectl drain node01 --ignore-daemonsets --delete-emptydir-data

# Force drain when pods have no controller (standalone pods)
kubectl drain node01 --ignore-daemonsets --delete-emptydir-data --force

# Re-enable scheduling after work is done
kubectl uncordon node01

# Check node status and conditions
kubectl get nodes
kubectl describe node node01 | grep -E "Taints|Unschedulable"

# Typical maintenance workflow:
# 1. Drain the node (moves pods to other nodes)
# 2. Perform maintenance (upgrade OS, reboot, etc.)
# 3. Uncordon the node
# 4. Verify: kubectl get nodes</code></pre>
      <div class="snippet-label">PodDisruptionBudget (controls voluntary evictions)</div>
      <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 2            # OR use maxUnavailable: 1
  selector:
    matchLabels:
      app: webapp</code></pre>
      <pre><code># Check PDBs
kubectl get pdb
kubectl describe pdb webapp-pdb</code></pre>

      <h3>Pod Priority and Preemption</h3>
      <p>When the cluster is full and a new pod cannot be scheduled, the scheduler checks if the pod has a higher
        <strong>priority</strong> than any currently running pods. If so, it can <strong>preempt</strong> (evict)
        lower-priority pods to make room.
      </p>
      <p><strong>How preemption works:</strong></p>
      <ul>
        <li>The scheduler identifies the lowest-priority pods on feasible nodes.</li>
        <li>Those pods receive a graceful termination signal (respecting <code>terminationGracePeriodSeconds</code>).
        </li>
        <li>Once evicted, the high-priority pod is scheduled onto the freed node.</li>
        <li>If <code>preemptionPolicy: Never</code>, the pod waits in the queue without evicting anyone.</li>
      </ul>
      <p><strong>Built-in PriorityClasses:</strong> <code>system-cluster-critical</code> (2 billion) and
        <code>system-node-critical</code> (2 billion + 1000) are reserved for control-plane components. You should
        never use values that high for user workloads.
      </p>
      <div class="snippet-label">PriorityClass definition</div>
      <pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000              # higher = more important (system max ~2 billion)
globalDefault: false        # if true, applies to pods without a class
description: "Critical production workloads"
preemptionPolicy: PreemptLowerPriority   # or Never (no eviction)</code></pre>
      <div class="snippet-label">Low-priority class (non-preempting &mdash; waits instead of evicting)</div>
      <pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-low
value: 100
preemptionPolicy: Never     # will NOT evict other pods; just waits
description: "Non-critical batch jobs"</code></pre>
      <div class="snippet-label">Pod using PriorityClass</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: nginx</code></pre>
      <pre><code># List all priority classes (including built-in ones)
kubectl get priorityclasses

# Check which priority class a pod uses
kubectl get pod critical-app -o jsonpath='{.spec.priorityClassName}'

# See preemption events
kubectl get events --field-selector reason=Preempted</code></pre>

      <h3>Multiple Schedulers</h3>
      <p>Kubernetes supports running <strong>multiple schedulers simultaneously</strong>. You deploy a custom scheduler
        as a Pod or Deployment alongside the default <code>default-scheduler</code>. Each scheduler has a unique
        <code>schedulerName</code>. Pods opt into a specific scheduler using <code>spec.schedulerName</code>.
      </p>
      <p><strong>When to use:</strong> When you need different scheduling logic for different workloads (e.g., GPU-aware
        scheduling, custom bin-packing, rack-aware placement).</p>
      <div class="snippet-label">Custom scheduler configuration (ConfigMap)</div>
      <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-config
  namespace: kube-system
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    leaderElection:
      leaderElect: false          # set true if running multiple replicas
    profiles:
    - schedulerName: my-custom-scheduler</code></pre>
      <div class="snippet-label">Custom scheduler Pod manifest</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler        # needs RBAC to watch/bind pods
  containers:
  - name: kube-scheduler
    image: registry.k8s.io/kube-scheduler:v1.29.0
    command:
    - kube-scheduler
    - --config=/etc/kubernetes/my-scheduler-config.yaml
    volumeMounts:
    - name: config
      mountPath: /etc/kubernetes
  volumes:
  - name: config
    configMap:
      name: my-scheduler-config</code></pre>
      <div class="snippet-label">Pod assigned to custom scheduler</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: custom-scheduled-pod
spec:
  schedulerName: my-custom-scheduler    # must match name in scheduler config
  containers:
  - name: nginx
    image: nginx</code></pre>
      <pre><code># Verify which scheduler placed this pod
kubectl get events --field-selector reason=Scheduled
kubectl describe pod custom-scheduled-pod | grep "Node:"

# If pod stays Pending, check the custom scheduler is running
kubectl get pods -n kube-system | grep scheduler
kubectl logs my-custom-scheduler -n kube-system

# List all pods and their scheduler
kubectl get pods -o custom-columns="NAME:.metadata.name,SCHEDULER:.spec.schedulerName"</code></pre>

      <h3>Scheduler Profiles</h3>
      <p>From Kubernetes 1.18+ you can run <strong>multiple scheduling profiles in a single scheduler binary</strong>
        instead of deploying separate scheduler pods. Each profile has its own <code>schedulerName</code> and can
        customize which <strong>plugins</strong> are enabled/disabled at each scheduling phase.</p>
      <p><strong>Scheduling extension points (plugin phases):</strong></p>
      <ul>
        <li><strong>QueueSort:</strong> Determines order in the scheduling queue.</li>
        <li><strong>PreFilter / Filter:</strong> Eliminates ineligible nodes (e.g., NodeResources, TaintToleration,
          NodeAffinity).</li>
        <li><strong>PreScore / Score:</strong> Ranks feasible nodes (e.g., LeastAllocated, ImageLocality,
          PodTopologySpread).</li>
        <li><strong>Reserve / Permit / PreBind / Bind / PostBind:</strong> Handle the binding lifecycle.</li>
      </ul>
      <div class="snippet-label">KubeSchedulerConfiguration with two profiles</div>
      <pre><code>apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler
  plugins:
    score:
      disabled:
      - name: PodTopologySpread    # disable a score plugin for this profile
- schedulerName: high-throughput-scheduler
  plugins:
    filter:
      disabled:
      - name: TaintToleration      # relax taint check for this profile
    score:
      enabled:
      - name: ImageLocality        # prefer nodes with cached images
        weight: 2</code></pre>
      <div class="snippet-label">Pod using a specific profile</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: throughput-pod
spec:
  schedulerName: high-throughput-scheduler   # matches profile name
  containers:
  - name: app
    image: nginx</code></pre>

      <h3>Admission Controllers (2025)</h3>
      <h4>Validating Admission Controllers</h4>
      <p>Validate requests; accept or reject. Examples: PodSecurity, ResourceQuota, LimitRanger.</p>
      <h4>Mutating Admission Controllers</h4>
      <p>Modify objects before persistence. Examples: DefaultStorageClass, ServiceAccount injection.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Admission Controller</div>
        <div class="cka-def-body">Intercepts requests to the API server after authentication/authorization, before persistence. <strong>Validating:</strong> accept or reject (e.g., PodSecurity, ResourceQuota). <strong>Mutating:</strong> modify objects (e.g., DefaultStorageClass, ServiceAccount). CKA: know they run in-order; use <code>kube-apiserver --enable-admission-plugins</code>.</div>
      </div>
      <p>Admission controllers enforce cluster policies. <strong>Validating</strong> controllers only validate (e.g., <code>PodSecurity</code>, <code>LimitRanger</code>). <strong>Mutating</strong> controllers can modify objects (e.g., <code>DefaultStorageClass</code>, inject default values). Configure via <code>--enable-admission-plugins</code> and <code>--disable-admission-plugins</code> on kube-apiserver.      </p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 03 Scheduling</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 03 Scheduling" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 04 Logging & Monitoring -->
      <h2 id="section-04">04 â€” Logging and Monitoring</h2>
      <div class="cka-definition">
        <div class="cka-def-term">Logging & Monitoring (CKA)</div>
        <div class="cka-def-body">Application logs via <code>kubectl logs</code> (stdout/stderr). Use <code>--previous</code> for crashed containers. Cluster metrics: <strong>Metrics Server</strong> powers <code>kubectl top nodes/pods</code>. HPA requires Metrics Server for CPU-based scaling.</div>
      </div>
      <h3>Monitor Cluster Components</h3>
      <p>Check control-plane pods: <code>kubectl get pods -n kube-system</code>. For node metrics: <code>kubectl top nodes</code>. Verify scheduler, API server, etcd are running.</p>
      <h3>Managing Application Logs</h3>
      <p>Container stdout/stderr are captured by kubelet and exposed via the API. Use <code>kubectl logs</code>. For a
        <strong>multi-container</strong> pod you must specify the container name.
      </p>
      <pre><code>kubectl logs -f &lt;pod-name&gt;
kubectl logs -f &lt;pod-name&gt; &lt;container-name&gt;
kubectl logs &lt;pod-name&gt; --previous
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>
      <p><code>-f</code> streams logs; <code>--previous</code> shows logs from the <strong>previous</strong> container
        instance (e.g. after a crash/restart).</p>
      <h3>Cluster metrics</h3>
      <p><strong>Metrics Server</strong> provides resource metrics (CPU/memory) for <code>kubectl top nodes</code> and
        <code>kubectl top pods</code>. It is not for long-term storage; use a monitoring stack (Prometheus, etc.) for
        that. Legacy Heapster is deprecated.
      </p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 04 Logging &amp; Monitoring</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 04 Logging and Monitoring" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 05 Application Lifecycle -->
      <h2 id="section-05">05 â€” Application Lifecycle Management</h2>
      <h3>Deployment Strategies</h3>
      <p><strong>RollingUpdate</strong> (default): Gradually replace old pods. <strong>Recreate</strong>: Kill all, then create new. Configure via <code>spec.strategy</code>.</p>
      <h3>Rolling Updates</h3>
      <p>Gradually replace old ReplicaSet pods with new ones. Configure <code>maxSurge</code> and <code>maxUnavailable</code>.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-rolling-update.png" alt="Rolling update: gradually replace old ReplicaSet with new" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Rolling update:</strong> Deployment gradually replaces old ReplicaSet pods with new version pods, maintaining availability.</figcaption>
      </figure>
      <pre><code>kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
kubectl apply -f deployment-definition.yaml
kubectl rollout undo deployment/myapp-deployment</code></pre>
      <h3>Rollbacks</h3>
      <p><code>kubectl rollout undo deployment/&lt;name&gt;</code>. Check history: <code>kubectl rollout history</code>.</p>
      <h3>Scaling</h3>
      <p><code>kubectl scale deployment/&lt;name&gt; --replicas=N</code>. Or edit <code>spec.replicas</code> and apply.</p>

      <h3>Application Configuration</h3>
      <h4>ConfigMaps</h4>
      <div class="cka-definition">
        <div class="cka-def-term">ConfigMap</div>
        <div class="cka-def-body">Stores non-sensitive config as key-value pairs. Inject via <code>env</code>, <code>envFrom</code>, or volume mount. CKA: <code>kubectl create configmap x --from-literal=K=V</code>, <code>--from-file</code>, <code>configMapKeyRef</code>/<code>configMapRef</code> in pod spec.</div>
      </div>
      <p>Create: imperative
        <code>kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod</code> or
        from file <code>--from-file=app_config.properties</code>.
      </p>
      <div class="snippet-label">ConfigMap YAML</div>
      <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod</code></pre>
      <pre><code>kubectl create -f config-map.yaml
kubectl get configmaps
kubectl get cm
kubectl describe configmaps app-config</code></pre>
      <div class="snippet-label">Inject ConfigMap into pod (envFrom)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config</code></pre>

      <h3>Secrets</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Secret</div>
        <div class="cka-def-body">Stores sensitive data (base64-encoded in YAML). Types: generic, tls, docker-registry. Inject via <code>secretKeyRef</code>/<code>secretRef</code>. CKA: <code>kubectl create secret generic x --from-literal=K=V</code>, <code>secret tls</code> for Ingress. Enable encryption at rest for prod.</div>
      </div>
      <p>Store sensitive data (base64 in YAML). Create:
        <code>kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd</code>.
        Encode for YAML: <code>echo -n "paswrd" | base64</code>.
      </p>
      <pre><code>apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk</code></pre>
      <pre><code>kubectl create -f secret-data.yaml
kubectl get secrets
kubectl describe secret app-secret
kubectl get secret app-secret -o yaml</code></pre>
      <p>Decode: <code>echo -n "bX1zcWw=" | base64 --decode</code>. Inject into pod with
        <code>envFrom: - secretRef: name: app-secret</code> (same structure as configMapRef).
      </p>
      <h4>Encrypting Secrets at Rest</h4>
      <p>Enable encryption in <code>kube-apiserver</code> via <code>--encryption-provider-config</code>. Define which resources (e.g. secrets) are encrypted. CKA: know the concept; config is in a YAML file.</p>

      <h4>Commands & Arguments (Docker)</h4>
      <p>Docker: <code>ENTRYPOINT</code> + <code>CMD</code>. Kubernetes <code>command</code> overrides ENTRYPOINT, <code>args</code> overrides CMD.</p>
      <p>Override image <strong>CMD</strong> with <code>command</code> (args to entrypoint) and
        <strong>ENTRYPOINT</strong> with <code>args</code> in the container spec. In Kubernetes, <code>command</code>
        corresponds to Docker ENTRYPOINT and <code>args</code> to CMD. Example: <code>command: ["sleep"]</code>,
        <code>args: ["3600"]</code>.
      </p>
      <div class="snippet-label">Docker â†’ Kubernetes mapping</div>
      <pre><code># Dockerfile          â†’ Pod spec
# ENTRYPOINT ["python"] â†’ command: ["python"]
# CMD ["app.py"]       â†’ args: ["app.py"]

apiVersion: v1
kind: Pod
metadata:
  name: command-demo
spec:
  containers:
  - name: demo
    image: ubuntu
    command: ["sleep"]    # overrides ENTRYPOINT
    args: ["5000"]        # overrides CMD</code></pre>
      <h3>Environment Variables</h3>
      <p>Set env vars directly, from ConfigMaps, or from Secrets. Three injection methods:</p>
      <pre><code>containers:
- name: app
  image: myapp
  env:
  # 1. Plain key-value
  - name: APP_COLOR
    value: pink
  # 2. From ConfigMap key
  - name: APP_ENV
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: APP_ENV
  # 3. From Secret key
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password</code></pre>

      <h3>Pod Design</h3>
      <h3>Multi-Container Pods</h3>
      <h4>Multi-Container Design Patterns</h4>
      <p>CKA tests multi-container pods. Three main patterns:</p>
      <figure class="diagram">
        <img src="assets/k8s-multi-container-patterns.png" alt="Multi-container patterns: Sidecar, Ambassador, Adapter" width="540" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Multi-container patterns:</strong> Sidecar (auxiliary), Ambassador (proxy to external services), Adapter (transforms output).</figcaption>
      </figure>
      <ul>
        <li><strong>Sidecar:</strong> auxiliary container that enhances the main container (e.g. log shipper that
          reads
          shared volume, Istio proxy).</li>
        <li><strong>Ambassador:</strong> proxy container that handles connections to external services on behalf of
          the
          main container (e.g. localhost proxy to reach different DB environments).</li>
        <li><strong>Adapter:</strong> transforms data from the main container before exporting it (e.g. log format
          normaliser, metrics adapter).</li>
      </ul>
      <div class="snippet-label">Sidecar example (log streamer)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'while true; do echo "$(date) INFO app running" >> /var/log/app.log; sleep 5; done']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  - name: sidecar
    image: busybox
    command: ['sh', '-c', 'tail -f /var/log/app.log']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  volumes:
  - name: log-volume
    emptyDir: {}</code></pre>
      <h3>Init containers</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Init Container</div>
        <div class="cka-def-body">Runs to completion before main containers start. Runs sequentially; if one fails, pod restarts. Use for setup (clone repo, wait for DB). CKA: <code>initContainers</code> in pod spec, ordered execution, shared volumes with main containers.</div>
      </div>
      <p>Run to completion before main containers start. Use for one-time setup or waiting for dependencies. If an
        init
        container fails, the pod is restarted (all init containers re-run). Order: initContainers run sequentially;
        then
        main containers start.</p>
      <figure class="diagram">
        <img src="assets/k8s-init-containers.png" alt="Init containers run sequentially before main containers" width="500" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Init containers:</strong> Run sequentially to completion; only then do main containers start. Failure restarts the pod.</figcaption>
      </figure>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']</code></pre>

      <h3>Self-Healing</h3>
      <p>Liveness and readiness probes make deployments self-healing. Liveness restarts unhealthy containers; readiness removes from Service endpoints until ready.</p>
      <h4>Liveness and readiness probes</h4>
      <p>Probes make deployments <strong>self-healing</strong>. <strong>livenessProbe</strong>: if it fails, the
        container is restarted. <strong>readinessProbe</strong>: if it fails, the pod is removed from Service
        endpoints
        (no traffic until ready). Types: <code>httpGet</code> (HTTP path), <code>exec</code> (command),
        <code>tcpSocket</code> (port open).
      </p>
      <figure class="diagram">
        <img src="assets/k8s-liveness-readiness.png" alt="Liveness vs Readiness probes" width="500" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Probes:</strong> Liveness fails &rarr; restart container. Readiness fails &rarr; remove from Service endpoints (no traffic).</figcaption>
      </figure>
      <div class="snippet-label">Probes example</div>
      <pre><code>containers:
- name: app
  image: myapp:1.0
  livenessProbe:
    httpGet:
      path: /healthz
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 20
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10</code></pre>

      <h3>Autoscaling (2025)</h3>
      <h4>Introduction to Autoscaling</h4>
      <p>HPA scales replicas by CPU/custom metrics. VPA adjusts resource requests. Manual scaling via <code>kubectl scale</code>.</p>
      <h4>Manual Scaling</h4>
      <p><code>kubectl scale deployment/&lt;name&gt; --replicas=N</code>.</p>
      <h4>In-Place Resize</h4>
      <p>Kubernetes 1.27+: resize CPU/memory without restart. Update <code>resources</code> in pod spec; kubelet applies if supported.</p>
      <h4>Vertical Pod Autoscaler (VPA)</h4>
      <p>Recommends or auto-updates resource requests/limits based on usage. Install VPA controller; create VPA resource targeting deployment.</p>
      <figure class="diagram">
        <img src="assets/k8s-hpa-flow.png" alt="HPA flow: Metrics Server to HPA to Deployment scaling" width="560" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>HPA flow:</strong> Metrics Server collects CPU â†’ HPA compares to target â†’ scales Deployment replicas.</figcaption>
      </figure>
      <h3>HorizontalPodAutoscaler (HPA)</h3>
      <div class="cka-definition">
        <div class="cka-def-term">HPA (Horizontal Pod Autoscaler)</div>
        <div class="cka-def-body">Scales Deployment/ReplicaSet based on CPU or custom metrics. Requires Metrics Server. Set <code>resources.requests</code> on deployment. CKA: <code>kubectl autoscale deploy x --min=2 --max=10 --cpu-percent=80</code>, <code>kubectl get hpa</code>. VPA (Vertical Pod Autoscaler) adjusts CPU/memory requests.</div>
      </div>
      <p><strong>Workload autoscaling</strong>: HPA scales a Deployment/ReplicaSet based on CPU (or custom metrics).
        Requires <strong>Metrics Server</strong> in the cluster for <code>kubectl top</code> and CPU-based scaling.
        Set
        <code>resources.requests</code> on the deployment so HPA can compute utilization.
      </p>
      <div class="snippet-label">HPA YAML</div>
      <pre><code>apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80</code></pre>
      <pre><code>kubectl autoscale deployment myapp-deployment --min=2 --max=10 --cpu-percent=80
kubectl get hpa
kubectl describe hpa myapp-hpa</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 05 Application Lifecycle</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 05 Application Lifecycle" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 06 Cluster Maintenance -->
      <h2 id="section-06">06 â€” Cluster Maintenance</h2>
      <h3>OS Upgrades</h3>
      <p>Drain node before upgrade: <code>kubectl drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data</code>. Perform OS updates. Uncordon: <code>kubectl uncordon &lt;node&gt;</code>.</p>
      <h3>Kubernetes Releases</h3>
      <p>Upgrade one minor version at a time. Control plane first, then workers. Use <code>kubeadm upgrade</code>.</p>
      <h3>Cluster Upgrade Process</h3>
      <p>1. Upgrade kubeadm. 2. <code>kubeadm upgrade plan</code> and <code>kubeadm upgrade apply</code>. 3. Upgrade kubelet and kubectl. 4. Restart kubelet.</p>
      <h3>Backup & Restore Methods</h3>
      <p>Backup etcd before upgrades. Use <code>etcdctl snapshot save</code> or <code>etcdutl snapshot save</code>. Restore with <code>etcdutl snapshot restore</code>. See ETCDCTL & ETCDUTL below.</p>
      <h3>ETCDCTL & ETCDUTL</h3>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db --endpoints=...
etcdutl snapshot restore snapshot.db --data-dir=/var/lib/etcd-restore</code></pre>
      <h3>Certification Tips</h3>
      <p>Set aliases at exam start. Use imperative + dry-run for YAML. Verify answers. Don't get stuck â€” flag and move on.</p>
      <h3>OS upgrades and node maintenance</h3>
      <p><strong>Important:</strong> If a node is down for <strong>more than 5 minutes</strong>, the control plane may
        terminate pods that were on that node (they will be recreated elsewhere if managed by a controller).</p>
      <ul>
        <li><strong>kubectl drain &lt;node&gt;</strong>: Evicts workloads from the node (pods are rescheduled
          elsewhere)
          and marks the node <strong>unschedulable</strong> (cordon). Use before OS upgrade or maintenance.</li>
        <li><strong>kubectl uncordon &lt;node&gt;</strong>: Marks the node schedulable again after it is back.</li>
        <li><strong>kubectl cordon &lt;node&gt;</strong>: Only marks the node unschedulable; does <strong>not</strong>
          evict existing pods.</li>
      </ul>
      <figure class="diagram">
        <img src="assets/k8s-cordon-drain-uncordon.png" alt="Node maintenance: cordon, drain, uncordon" width="420" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption>cordon = no new pods; drain = cordon + evict; uncordon = allow scheduling again.</figcaption>
      </figure>

      <h3>Kubernetes versions</h3>
      <p>Kubernetes supports the last 3 minor versions. Upgrade one minor version at a time. kubeadm, kubelet, and
        control-plane components can be at different versions during upgrade.</p>

      <h3>Cluster upgrade (kubeadm)</h3>
      <p><strong>Master:</strong></p>
      <pre><code>kubeadm upgrade plan
apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply v1.12.0
apt-get upgrade kubelet=1.12.0-00
systemctl restart kubelet
kubectl get nodes</code></pre>
      <p><strong>Workers:</strong> drain, upgrade kubeadm and kubelet, update node config, restart kubelet, uncordon.
      </p>
      <pre><code>kubectl drain node-1
apt-get upgrade -y kubeadm=1.12.0-00 kubelet=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet
kubectl uncordon node-1</code></pre>

      <h3>Backup and restore</h3>
      <p><strong>Resource configs:</strong> store YAML in Git; or
        <code>kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml</code> (partial). Tools like
        Velero
        can backup/restore resources.
      </p>
      <p><strong>ETCD snapshot:</strong></p>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db</code></pre>
      <p>With kubeadm, etcd runs as a pod; use the correct endpoint and certs. Example (adjust paths/endpoints for
        your
        setup):</p>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
      <p>Restore: <code>etcdctl snapshot restore snapshot.db --data-dir=/var/lib/etcd-from-backup</code>, then point
        etcd to the new data-dir and restart control-plane components.</p>

      <h3>PodDisruptionBudget (PDB)</h3>
      <p>PDB limits voluntary disruptions (e.g. <code>kubectl drain</code>, node upgrades) so that a minimum number of
        pods stay available. <strong>minAvailable</strong>: at least N pods (number or percentage).
        <strong>maxUnavailable</strong>: at most N pods down (number or percentage). Drain respects PDBs and may block
        until pods can be rescheduled.
      </p>
      <div class="snippet-label">PDB example (minAvailable)</div>
      <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp</code></pre>
      <div class="snippet-label">PDB with maxUnavailable</div>
      <pre><code>spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: myapp</code></pre>
      <pre><code>kubectl get pdb
kubectl describe pdb myapp-pdb</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 06 Cluster Maintenance</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 06 Cluster Maintenance" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 07 Security -->
      <h2 id="section-07">07 â€” Security</h2>
      <div class="cka-definition">
        <div class="cka-def-term">RBAC</div>
        <div class="cka-def-body">Role-Based Access Control. Defines who can do what via Roles/ClusterRoles (permissions) and RoleBindings/ClusterRoleBindings (who gets them). CKA: create Role (namespace-scoped), ClusterRole (cluster-wide), bind to User/Group/ServiceAccount. <code>kubectl auth can-i --as=user get pods</code>.</div>
      </div>

      <h3>Security Fundamentals</h3>
      <h3>Kubernetes Security Primitives</h3>
      <p>Security starts with the <strong>API server</strong> â€” it is the gatekeeper. Two key questions: <strong>who
          can
          access?</strong> (authentication) and <strong>what can they do?</strong> (authorization). All access goes
        through the API server, making it the single chokepoint for security.</p>

      <h3>Authentication</h3>
      <h3>Authorization</h3>
      <p>Kubernetes does <strong>not</strong> manage user accounts natively (no User object). It relies on external
        mechanisms. Two types of accounts:</p>
      <ul>
        <li><strong>Users</strong> (humans): admins, developers â€” managed externally (certificates, LDAP, OIDC
          tokens).
        </li>
        <li><strong>Service Accounts</strong> (machines): used by pods and processes â€” managed by Kubernetes.</li>
      </ul>
      <p>Authentication methods: <strong>static password/token files</strong> (deprecated),
        <strong>certificates</strong> (most common with kubeadm), <strong>OIDC</strong>, <strong>webhook</strong>. All
        requests are authenticated by the API server before authorization.
      </p>

      <h3>Service Accounts</h3>
      <p>ServiceAccounts are namespace-scoped. Every namespace has a <code>default</code> SA. Pods automatically mount
        the SA token. Use custom SAs to give pods specific RBAC permissions.</p>
      <pre><code># Create
kubectl create serviceaccount dashboard-sa
kubectl get serviceaccount

# Use in pod
apiVersion: v1
kind: Pod
metadata:
  name: my-dashboard
spec:
  serviceAccountName: dashboard-sa
  automountServiceAccountToken: false   # opt-out of auto-mount
  containers:
  - name: dashboard
    image: my-dashboard:v1</code></pre>
      <p>Since Kubernetes 1.24, ServiceAccount tokens are no longer auto-created as Secrets. Use
        <code>kubectl create token &lt;sa-name&gt;</code> for short-lived tokens, or create a Secret with annotation
        <code>kubernetes.io/service-account.name</code>.
      </p>

      <h3>TLS & Certificates</h3>
      <h4>TLS Basics</h4>
      <p>TLS provides encryption and authentication. Certificates bind identity to public key. Kubernetes uses TLS for API server, etcd, and kubelet.</p>
      <figure class="diagram">
        <img src="assets/k8s-tls-cert-chain.png" alt="Kubernetes TLS certificate chain: CA signs component certs" width="400" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>TLS certificate chain:</strong> CA is root of trust. API server, etcd, kubelet certs are signed by cluster CA.</figcaption>
      </figure>
      <h4>TLS in Kubernetes</h4>
      <p>Cluster components use <strong>TLS</strong>: servers use <strong>server certificates</strong>, clients use
        <strong>client certificates</strong> for authentication. API server, etcd, kubelet, and other control-plane
        components each have certs (typically under <code>/etc/kubernetes/pki/</code> on kubeadm). Certificate API can
        issue signed certs for users; the controller manager handles signing.
      </p>
      <h4>Key certificate files (kubeadm)</h4>
      <ul>
        <li><code>ca.crt / ca.key</code> â€” Cluster CA (root of trust)</li>
        <li><code>apiserver.crt / apiserver.key</code> â€” API server TLS</li>
        <li><code>apiserver-kubelet-client.crt</code> â€” API â†’ kubelet client cert</li>
        <li><code>apiserver-etcd-client.crt</code> â€” API â†’ etcd client cert</li>
        <li><code>etcd/ca.crt, etcd/server.crt</code> â€” etcd TLS</li>
        <li><code>front-proxy-ca.crt</code> â€” Aggregation layer</li>
      </ul>

      <h4>Certificate Creation</h4>
      <p>Use OpenSSL or Certificates API. For user certs: generate key + CSR, create Kubernetes CSR object, approve, extract cert.</p>
      <h4>Certificate Inspection</h4>
      <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout</code></pre>
      <p>Check subject, issuer, validity (not after), SANs (Subject Alternative Names). Same for other certs in
        <code>/etc/kubernetes/pki/</code> (ca.crt, apiserver-kubelet-client.crt, etc.).
      </p>

      <h4>Certificates API</h4>
      <p>Kubernetes has a built-in <strong>Certificate Signing Request (CSR)</strong> API for issuing client
        certificates. The controller manager signs the certificates using the cluster CA.</p>
      <div class="snippet-label">Create and approve a user certificate</div>
      <pre><code># 1. User generates key and CSR
openssl genrsa -out jane.key 2048
openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr

# 2. Admin creates CSR object (base64-encode the CSR)
cat jane.csr | base64 | tr -d '\n'</code></pre>
      <pre><code># 3. CSR YAML
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: jane
spec:
  request: &lt;base64-encoded-csr&gt;
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth</code></pre>
      <pre><code># 4. Approve and extract certificate
kubectl create -f jane-csr.yaml
kubectl get csr
kubectl certificate approve jane
kubectl get csr jane -o jsonpath='{.status.certificate}' | base64 --decode > jane.crt

# 5. Deny a CSR
kubectl certificate deny bad-user</code></pre>
      <h3>kubeconfig</h3>
      <p>kubeconfig holds clusters (API server URL, CA), users (credentials: cert/key or token), and
        <strong>contexts</strong> (which cluster + which user + optional namespace). Default file:
        <code>~/.kube/config</code>.
      </p>
      <pre><code>kubectl config view
kubectl config view --kubeconfig=my-custom-config
kubectl config use-context &lt;context-name&gt;
kubectl config current-context
kubectl get pods --kubeconfig config</code></pre>
      <p>To set default namespace for current context:
        <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>.
      </p>
      <h3>API Groups</h3>
      <p>Kubernetes API is organized into groups: <code>core</code> (v1), <code>apps</code>, <code>networking.k8s.io</code>, etc. Resources like Pods are in core; Deployments in apps/v1.</p>
      <h3>KubeConfig</h3>
      <p>Stores clusters, users, contexts. File: <code>~/.kube/config</code>. <code>kubectl config use-context</code> switches context. Each exam question may use a different context.</p>
      <h3>Access Control</h3>
      <h3>Role-Based Access Control (RBAC)</h3>
      <h4>Roles</h4>
      <p>Role: apiGroups, resources, verbs. RoleBinding: links subject (user/group/serviceaccount) to Role. Both are
        namespace-scoped.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "update", "delete", "create"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f developer-role.yaml
kubectl create -f devuser-developer-binding.yaml
kubectl get roles
kubectl get rolebindings
kubectl describe role developer
kubectl describe rolebinding devuser-developer-binding
kubectl auth can-i create pods --as=dev-user -n default</code></pre>

      <h4>RoleBindings</h4>
      <p>RoleBinding links a Role to a subject (user, group, ServiceAccount). Scope: same namespace as RoleBinding.</p>
      <h4>ClusterRoles</h4>
      <p>ClusterRole can define access to cluster-scoped resources (e.g. nodes) or to namespaced resources across all
        namespaces.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f cluster-admin-role.yaml
kubectl create -f cluster-admin-role-binding.yaml
kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false</code></pre>

      <h3>Network Security</h3>
      <h4>Network Policies</h4>
      <div class="cka-definition">
        <div class="cka-def-term">NetworkPolicy</div>
        <div class="cka-def-body">Controls ingress/egress to pods by selector. Requires CNI with policy support (Calico, Cilium). Default: allow all. Once a policy selects a pod, traffic not explicitly allowed is denied. CKA: <code>podSelector</code>, <code>policyTypes</code> (Ingress, Egress), <code>from</code>/<code>to</code> (podSelector, namespaceSelector, ipBlock).</div>
      </div>
      <p>Controls ingress/egress to pods by selector. Default: no restrictions. Once a NetworkPolicy selects a pod,
        that
        pod denies traffic not allowed by any policy.</p>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: api-pod
    ports:
    - protocol: TCP
      port: 3306</code></pre>
      <pre><code>kubectl create -f policy-definition.yaml</code></pre>
      <h4>Developing Network Policies</h4>
      <p>Start with deny-all, then add allow rules. Test with <code>kubectl run</code> debug pods. Use <code>podSelector</code>, <code>namespaceSelector</code>, <code>ipBlock</code> for <code>from</code>/<code>to</code>.</p>
      <h3>Image Security</h3>
      <p>Default image registry is <strong>docker.io</strong>. Image <code>nginx</code> is really
        <code>docker.io/library/nginx</code>. For private registries, create a <code>docker-registry</code> secret and
        reference it in the pod spec.
      </p>
      <div class="snippet-label">Pull from private registry</div>
      <pre><code># Create registry secret
kubectl create secret docker-registry regcred \
  --docker-server=private-registry.io \
  --docker-username=registry-user \
  --docker-password=registry-password \
  --docker-email=registry-user@org.com</code></pre>
      <pre><code># Use in pod
apiVersion: v1
kind: Pod
metadata:
  name: private-app
spec:
  containers:
  - name: app
    image: private-registry.io/apps/internal-app
  imagePullSecrets:
  - name: regcred</code></pre>

      <h3>Security Contexts</h3>
      <p>Restrict pod/container behavior: run as non-root user, read-only root filesystem, drop capabilities. Set at
        <code>spec.securityContext</code> (pod) or <code>spec.containers[].securityContext</code> (container).
        Container-level settings override pod-level. Capabilities can only be set at <strong>container</strong> level.
      </p>
      <pre><code>spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  containers:
  - name: ubuntu
    image: ubuntu
    securityContext:
      runAsUser: 1000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
        drop: ["ALL"]</code></pre>
      <pre><code># Check who's running inside the container
kubectl exec my-pod -- whoami
kubectl exec my-pod -- id</code></pre>
      <figure class="diagram">
        <img src="assets/k8s-rbac-flow.png" alt="RBAC: Role â†’ RoleBinding â†’ User/Group/ServiceAccount" width="400" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>RBAC: Role defines permissions; RoleBinding binds Role to User/Group/ServiceAccount.</figcaption>
      </figure>

      <h3>Advanced (2025)</h3>
      <p>Custom Resource Definitions (CRD), Custom Controllers, Operator Framework. See section 15 (Advanced Kubectl) for CRDs.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 07 Security</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 07 Security" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 08 Storage -->
      <h2 id="section-08">08 â€” Storage</h2>
      <h3>Docker Storage</h3>
      <p>Docker uses layers (copy-on-write). Volumes persist data outside container lifecycle. <code>docker volume</code> for volume management.</p>
      <h4>Volume Drivers</h4>
      <p>Docker volume drivers (local, nfs, etc.) provide storage backends. Kubernetes uses its own volume system.</p>
      <div class="cka-definition">
        <div class="cka-def-term">PV & PVC</div>
        <div class="cka-def-body"><strong>PersistentVolume (PV):</strong> Cluster-wide storage resource. <strong>PersistentVolumeClaim (PVC):</strong> User request for storage. Binding matches PVC (size, accessMode, storageClass) to PV. CKA: static provisioning (create PV + PVC) or dynamic (StorageClass). Know <code>accessModes</code> (RWO, ROX, RWX) and <code>reclaimPolicy</code>.</div>
      </div>
      <h3>Kubernetes Storage</h3>
      <h4>Container Storage Interface (CSI)</h4>
      <p>Standard for storage drivers. CSI drivers provision volumes for PVs. Dynamic provisioning via StorageClass uses a CSI provisioner.</p>
      <h4>Volumes</h4>
      <p>Pod-level storage: <code>emptyDir</code>, <code>hostPath</code>, <code>persistentVolumeClaim</code>, <code>configMap</code>, <code>secret</code>. Mount via <code>volumeMounts</code>.</p>
      <h3>Persistent Volumes (PV) and Persistent Volume Claims (PVC)</h3>
      <p>PV is <strong>cluster-scoped</strong> storage (admin provisions); PVC is <strong>namespace-scoped</strong>
        (user requests storage). They bind when capacity and <code>accessModes</code> match. If no PV matches, PVC
        stays
        <strong>Pending</strong>. Access modes: ReadWriteOnce (RWO), ReadOnlyMany (ROX), ReadWriteMany (RWX).
      </p>
      <figure class="diagram">
        <img src="assets/k8s-storage-pv-pvc-pod.png" alt="Storage flow: PV â†’ PVC â†’ Pod" width="500" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>Storage flow: PV (pool) â†’ PVC (claim) â†’ Pod (volumeMounts). PVC and Pod must be in same namespace.
        </figcaption>
      </figure>
      <div class="snippet-label">PV definition</div>
      <pre><code>kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-vol1
spec:
  accessModes: ["ReadWriteOnce"]
  capacity:
    storage: 1Gi
  hostPath:
    path: /tmp/data</code></pre>
      <div class="snippet-label">PVC definition</div>
      <pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 1Gi</code></pre>
      <pre><code>kubectl create -f pv-definition.yaml
kubectl create -f pvc-definition.yaml
kubectl get pv
kubectl get pvc
kubectl delete pvc myclaim
kubectl delete pv pv-vol1</code></pre>

      <h4>Using PVC in Pods</h4>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: myfrontend
    image: nginx
    volumeMounts:
    - mountPath: "/var/www/html"
      name: web
  volumes:
  - name: web
    persistentVolumeClaim:
      claimName: myclaim</code></pre>
      <pre><code>kubectl create -f pod-definition.yaml
kubectl get pod,pvc,pv</code></pre>

      <h3>Storage Classes</h3>
      <p>With a StorageClass, PVC can trigger automatic PV creation (no manual PV).</p>
      <pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd</code></pre>
      <pre><code>kubectl create -f sc-definition.yaml
kubectl get sc</code></pre>
      <p>In PVC add <code>storageClassName: google-storage</code>. Then create PVC and use <code>claimName</code> in
        pod
        volume as above.</p>
      <h4>Application Configuration Topics</h4>
      <p>ConfigMaps and Secrets can be mounted as volumes. Use <code>configMapRef</code>/<code>secretRef</code> for env, or <code>volumes</code> + <code>volumeMounts</code> for file injection.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 08 Storage</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 08 Storage" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 09 Networking -->
      <h2 id="section-09">09 â€” Networking</h2>
      <div class="cka-definition">
        <div class="cka-def-term">CNI (Container Network Interface)</div>
        <div class="cka-def-body">Standard for pod networking. Plugins (Calico, Flannel, Weave) assign pod IPs and configure routing. kubelet calls CNI when pod is created/destroyed. CKA: verify CNI pods in kube-system, check DNS (CoreDNS), service endpoints.</div>
      </div>
      <h3>Networking Fundamentals</h3>
      <h4>Switching</h4>
      <p>Layer 2; switches forward by MAC. <code>ip link</code>, <code>ip addr</code> for interfaces.</p>
      <h4>Routing</h4>
      <p>Layer 3; routers forward by IP. <code>ip route</code>, <code>route -n</code> for routing table.</p>
      <h4>Gateways</h4>
      <p>Default gateway: <code>ip route add default via &lt;gateway-ip&gt;</code>. Nodes need IP forwarding: <code>net.ipv4.ip_forward=1</code>.</p>
      <h3>Linux networking for CKA</h3>
      <p>Understanding Linux networking is essential for the CKA. Key commands:</p>
      <div class="snippet-label">Switching, routing, gateways</div>
      <pre><code># List network interfaces
ip link
ip addr

# Assign IP address to interface
ip addr add 192.168.1.10/24 dev eth0

# View routing table
ip route show
route

# Add route
ip route add 192.168.2.0/24 via 192.168.1.1

# Default gateway
ip route add default via 192.168.1.1

# Enable IP forwarding (required for routing between networks)
cat /proc/sys/net/ipv4/ip_forward   # 0=disabled, 1=enabled
echo 1 > /proc/sys/net/ipv4/ip_forward
# Permanent: set net.ipv4.ip_forward=1 in /etc/sysctl.conf
sysctl --system</code></pre>
      <h4>DNS Basics</h4>
      <p>FQDN resolution. Kubernetes uses CoreDNS for cluster-internal DNS (<code>.svc.cluster.local</code>).</p>
      <h4>CoreDNS</h4>
      <p>Cluster DNS. Config in ConfigMap <code>coredns</code> (Corefile). Plugins: kubernetes, forward, etc.</p>
      <h4>Network Namespaces</h4>
      <pre><code># Create network namespace
ip netns add red
ip netns add blue

# List namespaces
ip netns

# Run command in namespace
ip netns exec red ip link
ip -n red link

# Connect namespaces with veth pair
ip link add veth-red type veth peer name veth-blue
ip link set veth-red netns red
ip link set veth-blue netns blue
ip -n red addr add 192.168.15.1/24 dev veth-red
ip -n blue addr add 192.168.15.2/24 dev veth-blue
ip -n red link set veth-red up
ip -n blue link set veth-blue up</code></pre>

      <h3>Cluster Networking</h3>
      <h4>Pod Networking</h4>
      <figure class="diagram">
        <img src="assets/k8s-pod-networking-cni.png" alt="Pod networking: CNI assigns IPs, pods communicate across nodes" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Pod networking:</strong> CNI assigns each pod a unique IP from Pod CIDR. Pods on different nodes communicate directly.</figcaption>
      </figure>
      <p>Kubernetes uses the <strong>Container Network Interface (CNI)</strong> standard to manage networking.
        <br><em>Core Responsibilities of CNI Plugin:</em>
        <br>1. <strong>Connectivity:</strong> Insert a network interface into the container's namespace.
        <br>2. <strong>IPAM (IP Address Management):</strong> Assign an IP address to the pod from the Pod CIDR.
      </p>

      <h4>CNI Fundamentals / CNI in Kubernetes</h4>
      <p><em>Golden Rules:</em>
        <br>- Every Pod gets a unique IP.
        <br>- Pods on the same node talk via <code>localhost</code>/bridge.
        <br>- Pods on different nodes talk directly without NAT (Network Address Translation).
      </p>

      <pre><code># Check CNI configuration
ls /etc/cni/net.d/
cat /etc/cni/net.d/10-flannel.conflist

# Check CNI binaries
ls /opt/cni/bin/

# Identify Pod CIDR (allocated to node)
kubectl get node -o jsonpath='{.spec.podCIDR}'</code></pre>

      <h3>Service Networking</h3>
      <p>A Service provides a stable <strong>ClusterIP</strong> (Virtual IP) that load balances traffic to dynamic Pod
        IPs.
        <br><strong>Service:</strong> The abstraction (stable VIP).
        <br><strong>Endpoints:</strong> The actual list of backend Pod IPs. Checked via
        <code>kubectl get endpoints</code>.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-service-to-pods.png" alt="Service flow: ClusterIP â†’ kube-proxy â†’ Endpoints â†’ Pods" width="560" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Service to Pods:</strong> Client &rarr; ClusterIP (VIP) &rarr; kube-proxy (iptables/IPVS) &rarr; Endpoints &rarr; backend Pods.</figcaption>
      </figure>
      <p><em>Mechanism:</em> <strong>kube-proxy</strong> runs on every node and watches the API server. It updates
        node
        network rules (usually <strong>iptables</strong> or <strong>IPVS</strong>) to trap traffic destined for the
        Service IP and redirect it to a random backing Pod IP.</p>

      <pre><code># View iptables rules for a service
iptables-save | grep &lt;service-name&gt;

# Check kube-proxy mode
kubectl logs -n kube-system -l k8s-app=kube-proxy</code></pre>

      <h3>DNS in Kubernetes / CoreDNS in Kubernetes</h3>
      <figure class="diagram">
        <img src="assets/k8s-dns-coredns.png" alt="DNS resolution: Pod â†’ CoreDNS â†’ Service IP" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>DNS resolution:</strong> Pod lookup (e.g. <code>my-svc.default.svc.cluster.local</code>) â†’ CoreDNS â†’ returns Service ClusterIP.</figcaption>
      </figure>
      <p><strong>CoreDNS</strong> is the cluster's internal DNS server. It watches the API server for new Services and
        Pods and creates DNS records.
        <br><em>Config:</em> The <strong>Corefile</strong> (inside a ConfigMap) defines behavior (plugins like
        <code>errors</code>, <code>health</code>, <code>kubernetes</code>, <code>forward</code>).
      </p>

      <p><strong>FQDN (Fully Qualified Domain Name) Structure:</strong>
        <br>Service: <code>my-svc.my-ns.svc.cluster.local</code>
        <br>Pod: <code>1-2-3-4.my-ns.pod.cluster.local</code> (IP with dashes)
      </p>

      <pre><code># Test DNS from a debug pod
kubectl run -it --rm debug --image=busybox:1.28 -- nslookup kubernetes
kubectl get configmap coredns -n kube-system -o yaml</code></pre>

      <h3>Ingress</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Ingress</div>
        <div class="cka-def-body">API object that manages external HTTP/HTTPS access to Services. Requires an Ingress controller (e.g., nginx-ingress). Rules map host/path to backend services. CKA: know <code>rules</code>, <code>pathType</code> (Prefix, Exact), TLS with <code>secretName</code>, and <code>rewrite-target</code> annotation.</div>
      </div>
      <div class="cka-definition">
        <div class="cka-def-term">Gateway API (2025)</div>
        <div class="cka-def-body">Successor to Ingress. Uses <code>Gateway</code> (infra) + <code>HTTPRoute</code> (routing rules). Role-oriented (GatewayClass, Gateway, Routes). More expressive than Ingress. CKA 2025: know it exists; Ingress still primary for exam.</div>
      </div>
      <p>Need an Ingress controller (e.g. nginx-ingress) and Ingress resources (rules). Ingress exposes HTTP/HTTPS
        routes to services.</p>
      <figure class="diagram">
        <img src="assets/k8s-ingress-flow.png" alt="Ingress flow: External â†’ Ingress Controller â†’ Service â†’ Pods" width="560" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Ingress flow:</strong> External traffic &rarr; Ingress Controller &rarr; Ingress rules (host/path) &rarr; Service &rarr; Pods.</figcaption>
      </figure>
      <h4>Ingress Annotations</h4>
      <p>Controller-specific: e.g. <code>nginx.ingress.kubernetes.io/rewrite-target</code>, <code>cert-manager.io/cluster-issuer</code>.</p>
      <h4>Rewrite Target</h4>
      <p>nginx-ingress: <code>nginx.ingress.kubernetes.io/rewrite-target: /</code> rewrites path before forwarding to backend.</p>
      <div class="snippet-label">Ingress with paths</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <div class="snippet-label">Ingress with host rules</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
  - host: watch.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <pre><code>kubectl create -f ingress.yaml
kubectl get ingress
kubectl describe ingress ingress-wear-watch</code></pre>

      <h3>Gateway API</h3>
      <p>The <strong>Gateway API</strong> is the newer, more expressive way to expose HTTP/gRPC/TCP routes (successor
        concept to Ingress). Key resources: <code>Gateway</code> (infrastructure that receives traffic),
        <code>HTTPRoute</code> (rules for routing to backends). Install a Gateway controller (e.g. Gateway API
        implementation) and create Gateway + HTTPRoute resources. The exam may reference it; know it exists as the
        evolution of Ingress.
      </p>

      <h4>Introduction to Gateway API</h4>
      <p>Gateway API is the evolution of Ingress. Resources: GatewayClass, Gateway, HTTPRoute, etc.</p>
      <h4>Practical Guide to Gateway API</h4>
      <p>Create Gateway + HTTPRoute. Install a Gateway controller (e.g. istio, envoy). Route traffic to Services.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 09 Networking</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 09 Networking" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 10 Design & Install -->
      <h2 id="section-10">10 â€” Cluster Design & Installation</h2>
      <h3>Design Kubernetes Cluster</h3>
      <p>Plan control-plane HA, node count, networking (Pod CIDR, Service CIDR), storage, and add-ons.</p>
      <h3>Choosing Infrastructure</h3>
      <p>VMs, bare metal, or managed (EKS, GKE, AKS). Consider networking, storage, and scaling needs.</p>
      <h3>Configure High Availability</h3>
      <p>Multiple API servers behind LB; etcd cluster (3 or 5 nodes); stacked or external etcd.</p>
      <h4>ETCD in HA</h4>
      <p>etcd uses RAFT. Odd number of members (3, 5). Stacked: etcd on same nodes as control plane. External: separate etcd cluster.</p>
      <h3>Kubernetes the Hard Way</h3>
      <p>Manual install from scratch (no kubeadm): set up VMs, install binaries, configure certs, etcd, API server, scheduler, controller-manager, kubelet, kube-proxy. Excellent for deep understanding.</p>
      <p><strong>Design:</strong> Plan control-plane count (HA = multiple masters), etcd (stacked with API server or
        external cluster), load balancer in front of API servers. Choose node sizing and networking (pod CIDR, service
        CIDR).</p>
      <p><strong>High availability:</strong> Multiple API server replicas behind a load balancer; etcd cluster (odd
        number, 3 or 5); scheduler and controller manager can run as active/standby (only one active).</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 10 Cluster Design</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 10 Cluster Design" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 11 kubeadm -->
      <h2 id="section-11">11 â€” kubeadm Installation</h2>
      <h3>Deployment with kubeadm</h3>
      <p><strong>kubeadm</strong> bootstraps the control plane and generates certs, manifests, and kubeconfig. You
        then
        join worker nodes with <code>kubeadm join</code>. The exam rarely asks for a full install; <strong>upgrading
          an
          existing cluster</strong> (kubeadm upgrade plan / apply, drain/uncordon workers) is frequently tested.</p>
      <figure class="diagram">
        <img src="assets/k8s-kubeadm-flow.png" alt="kubeadm init and join flow" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>kubeadm flow:</strong> Control plane: <code>kubeadm init</code>; workers: <code>kubeadm join</code> with token and CA hash.</figcaption>
      </figure>
      <h3>Provision VMs</h3>
      <p>Create VMs (or use cloud instances). Install container runtime, disable swap, configure networking. See Prerequisites below.</p>
      <h3>Deploy Control Plane</h3>
      <h3>Join Worker Nodes</h3>
      <p>Use <code>kubeadm join</code> with token and CA hash from <code>kubeadm init</code> output. Or: <code>kubeadm token create --print-join-command</code>.</p>
      <h4>kubeadm init steps (reference)</h4>
      <div class="snippet-label">Prerequisites on all nodes</div>
      <pre><code># Disable swap (required)
sudo swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# Enable kernel modules
cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

# Sysctl params
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system

# Install containerd, kubeadm, kubelet, kubectl
# (follow official docs for your OS)
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl</code></pre>
      <div class="snippet-label">Control plane init</div>
      <pre><code># Initialize control plane
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=&lt;master-ip&gt;

# Copy kubeconfig
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install CNI (e.g. Flannel)
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml</code></pre>
      <div class="snippet-label">Join workers</div>
      <pre><code># On worker nodes (use token from kubeadm init output):
sudo kubeadm join &lt;master-ip&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;

# If token expired, create a new one on master:
kubeadm token create --print-join-command</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 11 kubeadm</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 11 kubeadm" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 12 Helm -->
      <h2 id="section-12">12 â€” Helm</h2>
      <h3>Helm Installation</h3>
      <p><code>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</code> or use package manager.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Helm</div>
        <div class="cka-def-body">The package manager for Kubernetes. Packages charts (pre-configured K8s resources). Helm 3 removed Tiller; uses direct API calls. CKA: know <code>helm install</code>, <code>helm list</code>, <code>helm upgrade</code>, <code>helm uninstall</code>, and <code>values.yaml</code> for customization.</div>
      </div>
      <h3>Helm Introduction</h3>
      <p>Helm manages Kubernetes applications as <strong>charts</strong> â€” packages of YAML templates plus default values. Helm 3 is cluster-scoped and stores release state in Secrets.</p>
      <h3>Helm Charts</h3>
      <p>Charts are packages of templated K8s manifests. Structure: Chart.yaml, values.yaml, templates/.</p>
      <h3>Helm Components</h3>
      <ul>
        <li><strong>Chart:</strong> Package of templated K8s manifests</li>
        <li><strong>Release:</strong> Instance of a chart deployed to a cluster</li>
        <li><strong>Values:</strong> Configuration overrides (YAML)</li>
      </ul>
      <h3>Working with Helm</h3>
      <div class="snippet-label">Essential Helm commands (CKA)</div>
      <pre><code># Add a repo
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# Install a chart
helm install my-nginx bitnami/nginx
helm install my-nginx bitnami/nginx -f values.yaml
helm install my-nginx bitnami/nginx --set service.port=8080

# List releases
helm list
helm list -A

# Upgrade
helm upgrade my-nginx bitnami/nginx --set replicaCount=3

# Uninstall
helm uninstall my-nginx

# Template (render YAML without installing)
helm template my-nginx bitnami/nginx -f values.yaml

# Customizing values
helm show values bitnami/nginx
helm install my-nginx bitnami/nginx --set image.tag=1.25</code></pre>
      <h3>Helm Lifecycle Management</h3>
      <p><code>helm install</code> (create) â†’ <code>helm upgrade</code> (update) â†’ <code>helm rollback</code> (revert) â†’ <code>helm uninstall</code> (delete).</p>
      <h3>Helm2 vs Helm3</h3>
      <p>Helm 3 removed Tiller (no server-side component). Release data stored in Kubernetes Secrets in the release namespace. RBAC applies to Helm as a regular kubectl user.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 12 Helm</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 12 Helm" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 13 Kustomize -->
      <h2 id="section-13">13 â€” Kustomize</h2>
      <h3>Installation</h3>
      <p>Built into kubectl: <code>kubectl kustomize</code>. Standalone: <code>curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash</code>.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Kustomize</div>
        <div class="cka-def-body">Declarative customization of Kubernetes manifests without templating. Uses <code>kustomization.yaml</code> to patch, overlay, and compose resources. Built into kubectl: <code>kubectl apply -k .</code>. CKA: know overlays, patches, image transformers, and <code>kubectl kustomize</code>.</div>
      </div>
      <h3>Kustomize Ideology</h3>
      <p>Kustomize keeps base manifests unchanged and applies <strong>overlays</strong> (dev, staging, prod) for environment-specific config. No templating â€” pure YAML patches.</p>
      <h3>Kustomize vs Helm</h3>
      <p>Kustomize: native kubectl, no extra binary, patch-based. Helm: templating, chart ecosystem, versioned releases. Both are in the CKA 2025 curriculum.</p>
      <div class="snippet-label">kustomization.yaml and overlays</div>
      <pre><code># Base: kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml

# Overlay: overlays/prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: replace
        path: /spec/replicas
        value: 5
images:
  - name: myapp
    newTag: v2.0

# Apply
kubectl apply -k overlays/prod
kubectl kustomize overlays/prod   # preview</code></pre>
      <h4>Image Transformers</h4>
      <p>In kustomization.yaml: <code>images: - name: myapp; newTag: v2</code>. Replace image by name.</p>
      <h4>Components</h4>
      <p>Reusable kustomize fragments. Reference with <code>resources: - ../components/my-component</code>.</p>
      <h3>Transformers & Patches</h3>
      <ul>
        <li><strong>namePrefix/nameSuffix:</strong> Add prefix/suffix to resource names</li>
        <li><strong>images:</strong> Replace container images by name</li>
        <li><strong>patches:</strong> JSON Patch or strategic merge</li>
        <li><strong>configMapGenerator/secretGenerator:</strong> Generate ConfigMaps/Secrets from files</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 13 Kustomize</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 13 Kustomize" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 14 Troubleshooting -->
      <h2 id="section-14">14 â€” Troubleshooting</h2>
      <figure class="diagram">
        <img src="assets/k8s-troubleshooting-flowchart.png" alt="Troubleshooting decision flowchart for CKA" width="560" height="280" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Troubleshooting flow:</strong> Pending â†’ scheduler/taints/resources. Running but no traffic â†’ Service/Endpoints. CrashLoopBackOff â†’ logs --previous. No connectivity â†’ DNS/CNI/kube-proxy.</figcaption>
      </figure>
      <div class="cka-definition">
        <div class="cka-def-term">Troubleshooting (30% of CKA)</div>
        <div class="cka-def-body">Highest-weight domain. Application: check pod status, describe, logs (<code>--previous</code>), endpoints, selector mismatch. Control plane: kube-apiserver, etcd, scheduler. Worker: kubelet, container runtime. Network: CNI, CoreDNS, kube-proxy, firewall. Use <code>kubectl describe</code>, <code>journalctl -u kubelet</code>, <code>crictl</code>.</div>
      </div>
      <h3>Application Failure</h3>
      <h4>Debug Pod Failures</h4>
      <ul class="step-list">
        <li><strong>Is the service reachable?</strong> <code>curl http://&lt;node-ip&gt;:&lt;nodePort&gt;</code> or
          from
          inside cluster <code>curl http://&lt;svc-name&gt;.&lt;ns&gt;.svc.cluster.local:port</code>.</li>
        <li><strong>Does the service have endpoints?</strong> <code>kubectl get endpoints &lt;service-name&gt;</code>.
          If empty, the service selector does not match any pod (wrong labels or pods not ready).</li>
        <li><strong>Compare selector with pod labels:</strong> <code>kubectl describe service &lt;name&gt;</code> (see
          Selector) and <code>kubectl get pods --show-labels</code>. Fix selector or pod labels.</li>
        <li><strong>Pod status:</strong> <code>kubectl get pods</code>,
          <code>kubectl describe pod &lt;pod-name&gt;</code>. Check events (image pull, CrashLoopBackOff, etc.).
        </li>
        <li><strong>Container logs:</strong> <code>kubectl logs &lt;pod-name&gt;</code>,
          <code>kubectl logs &lt;pod-name&gt; -c &lt;container&gt;</code>,
          <code>kubectl logs &lt;pod-name&gt; --previous</code> for crashed container.
        </li>
      </ul>
      <pre><code>kubectl get pods -o wide
kubectl describe pod &lt;pod-name&gt;
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>

      <h3>Control Plane Failure</h3>
      <h4>Debug API Server</h4>
      <p>Check pod status, logs: <code>kubectl logs kube-apiserver-&lt;node&gt; -n kube-system</code>. Verify certs, endpoints.</p>
      <h4>Debug ETCD</h4>
      <p>Check etcd pod. Use <code>etcdctl endpoint health</code>. Verify data-dir, certs.</p>
      <p>Verify nodes are Ready, then check control-plane components. With kubeadm they are pods in
        <code>kube-system</code>; with manual install they are often systemd services.
      </p>
      <pre><code>kubectl get nodes
kubectl get pods -n kube-system
service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
service kubelet status
kubectl logs kube-apiserver-master -n kube-system
sudo journalctl -u kube-apiserver</code></pre>

      <h3>Worker Node Failure</h3>
      <h4>Debug kubelet</h4>
      <p><code>systemctl status kubelet</code>, <code>journalctl -u kubelet</code>. Check config, certs.</p>
      <h4>Node NotReady</h4>
      <p>kubelet not reporting. Check kubelet, container runtime, network. <code>kubectl describe node</code> for conditions.</p>
      <p>If a node is <strong>NotReady</strong>, check <code>kubectl describe node &lt;node&gt;</code> (conditions,
        LastHeartbeatTime). On the node: kubelet status, logs, and cert validity (kubelet may have expired cert).</p>
      <pre><code>kubectl get nodes
kubectl describe node worker-1
service kubelet status
sudo journalctl -u kubelet
openssl x509 -in /var/lib/kubelet/worker-1.crt -text</code></pre>
      <h4>Common worker node issues checklist</h4>
      <ul>
        <li><code>kubelet</code> not running â†’ <code>systemctl start kubelet</code>, check
          <code>journalctl -u kubelet</code>
        </li>
        <li>Config error â†’ check <code>/var/lib/kubelet/config.yaml</code> and
          <code>/etc/kubernetes/kubelet.conf</code>
        </li>
        <li>Expired certificates â†’ check cert dates with <code>openssl x509 -in &lt;cert&gt; -text -noout</code></li>
        <li>Node conditions: <code>MemoryPressure</code>, <code>DiskPressure</code>, <code>PIDPressure</code> â†’ check
          resources</li>
        <li>Container runtime down â†’ <code>systemctl status containerd</code></li>
      </ul>

      <h3>Network Troubleshooting</h3>
      <h4>Debug Service Issues</h4>
      <p>Check <code>kubectl get endpoints</code>. If empty, selector mismatch. Verify pod labels, service selector.</p>
      <h4>Debug DNS</h4>
      <p>CoreDNS pods running? <code>kubectl run test --rm -it --image=busybox:1.28 -- nslookup kubernetes</code>.</p>
      <h4>Debug CNI</h4>
      <p>When pods cannot communicate across nodes or DNS fails:</p>
      <ul class="step-list">
        <li><strong>Check CNI plugin:</strong> <code>kubectl get pods -n kube-system</code> â€” are Calico/Flannel/Weave
          pods running? Check logs.</li>
        <li><strong>CoreDNS running?</strong> <code>kubectl get pods -n kube-system -l k8s-app=kube-dns</code>. If
          CrashLooping, check <code>kubectl logs &lt;coredns-pod&gt; -n kube-system</code>.</li>
        <li><strong>DNS resolution:</strong>
          <code>kubectl run test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes</code>
        </li>
        <li><strong>kube-proxy running?</strong> <code>kubectl get ds kube-proxy -n kube-system</code>. Check iptables
          rules: <code>iptables -L -t nat | grep &lt;svc&gt;</code></li>
        <li><strong>Node networking:</strong> <code>ip link</code>, <code>ip addr</code>, <code>ip route</code> on
          affected node. Check IP forwarding: <code>cat /proc/sys/net/ipv4/ip_forward</code></li>
        <li><strong>Firewall/port issues:</strong> Ensure required ports are open (6443, 2379-2380, 10250-10252,
          30000-32767)</li>
      </ul>
      <pre><code># Debug networking from inside a pod
kubectl run netshoot --image=nicolaka/netshoot -it --rm --restart=Never -- bash
# Inside: ping, nslookup, curl, traceroute, tcpdump, iperf, etc.

# Test service from inside cluster
kubectl run test --image=busybox:1.28 --rm -it --restart=Never -- wget -qO- http://&lt;svc&gt;.&lt;ns&gt;.svc.cluster.local:&lt;port&gt;</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 14 Troubleshooting</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 14 Troubleshooting" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 13 Other Topics -->
      <h2 id="section-15">15 â€” Advanced Kubectl & JSON Path</h2>
      <div class="cka-definition">
        <div class="cka-def-term">JSONPath</div>
        <div class="cka-def-body">Query language for extracting data from JSON. Used with <code>kubectl get -o jsonpath='{...}'</code>. CKA: know <code>{.items[*].metadata.name}</code>, <code>{range .items[*]}...{end}</code>, <code>{.status.addresses[?(@.type=="InternalIP")].address}</code>, and <code>custom-columns</code>.</div>
      </div>
      <h3>JSON Path in Kubernetes</h3>
      <pre><code>kubectl get nodes -o json
kubectl get pods -o json
kubectl get pods -o=jsonpath='{.items[0].spec.containers[0].image}'
kubectl get pods -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu
kubectl get nodes --sort-by=.metadata.name
kubectl get nodes --sort-by=.status.capacity.cpu</code></pre>

      <h3>Advanced Kubectl Commands</h3>
      <p><code>kubectl get</code> with <code>-o wide</code>, <code>--sort-by</code>, <code>--selector</code>. <code>kubectl describe</code>, <code>kubectl logs -f</code>, <code>kubectl exec -it</code>.</p>
      <h3>CRDs and operators (2025)</h3>
      <p><strong>Custom Resource Definitions (CRDs)</strong> extend the API with custom resources. After defining a
        CRD,
        you get new resource types (e.g. <code>kubectl get myresource</code>). <strong>Operators</strong> are
        controllers that manage custom resources and their lifecycle (install, upgrade, backup). They use CRDs + a
        controller process. List CRDs: <code>kubectl get crd</code>.</p>

      <h3>Extension interfaces: CNI, CSI, CRI</h3>
      <p>Kubernetes uses pluggable interfaces. <strong>CNI</strong> (Container Network Interface): pod networking;
        plugins (Calico, Flannel, Weave) implement the pod network. <strong>CSI</strong> (Container Storage
        Interface):
        storage drivers for PVs; dynamic provisioning via StorageClass uses a CSI driver. <strong>CRI</strong>
        (Container Runtime Interface): container runtimes (containerd, CRI-O); kubelet talks to the runtime via CRI.
        Configure/validate these when installing or troubleshooting the cluster.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 15 Advanced Kubectl</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 15 Advanced Kubectl" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 16 Practice & Exam Preparation -->
      <h2 id="section-16">16 â€” Practice &amp; Exam Preparation</h2>
      <h3>Lightning Labs</h3>
      <h4>Playground &amp; Lightning Labs</h4>
      <p>Use the links below to practice in a real cluster, then try the scenarios on your own before revealing the
        solution.</p>

      <h3>Online playgrounds</h3>
      <p><a href="lab.html" class="lab-link-btn" style="display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.6rem 1rem; background: var(--accent); color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">ðŸ“‹ Open CKA Lab</a> â€” 60+ copyable kubectl commands + practice scenarios. Run in Killercoda or your cluster.</p>
      <ul class="playground-links">
        <li>
          <a href="https://killercoda.com/kubernetes" target="_blank" rel="noopener noreferrer">Killercoda â€”
            Kubernetes</a>
          <div class="link-desc">Free browser-based Kubernetes labs; no local install. Multiple CKA-focused scenarios.
          </div>
        </li>
        <li>
          <a href="https://www.katacoda.com/courses/kubernetes" target="_blank" rel="noopener noreferrer">Katacoda
            (legacy)</a>
          <div class="link-desc">Kubernetes playgrounds (some scenarios may be deprecated; Killercoda is the
            successor).
          </div>
        </li>
        <li>
          <a href="https://labs.play-with-k8s.com/" target="_blank" rel="noopener noreferrer">Play with Kubernetes</a>
          <div class="link-desc">Spin up a temporary cluster in the browser; good for quick tests.</div>
        </li>
        <li>
          <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/" target="_blank"
            rel="noopener noreferrer">Kubernetes Basics (official)</a>
          <div class="link-desc">Official interactive tutorial with an in-browser terminal.</div>
        </li>
      </ul>

      <h3>Practice scenarios (try yourself, then reveal solution)</h3>

      <div class="playground-scenario">
        <h4>Scenario 1: Deployment and Service</h4>
        <p class="playground-context">Namespace <code>app</code> exists. Create a deployment and expose it.</p>
        <p class="playground-task"><strong>Task:</strong> Create a deployment named <code>nginx-deploy</code> with
          image
          <code>nginx:1.21</code>, 3 replicas. Expose it as a ClusterIP service <code>nginx-svc</code> on port 80. Use
          namespace <code>app</code>.
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create deployment nginx-deploy --image=nginx:1.21 --replicas=3 -n app
kubectl expose deployment nginx-deploy --name=nginx-svc --port=80 -n app
kubectl get deploy,svc -n app</code></pre>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 2: ConfigMap and Pod</h4>
        <p class="playground-context">You need a pod that reads a config value from a ConfigMap.</p>
        <p class="playground-task"><strong>Task:</strong> Create a ConfigMap <code>app-cm</code> with
          <code>APP_ENV=production</code>. Create a pod <code>busybox-cm</code> (image <code>busybox:1.28</code>) that
          runs <code>sleep 3600</code> and has environment variable <code>APP_ENV</code> from the ConfigMap.
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create configmap app-cm --from-literal=APP_ENV=production
kubectl run busybox-cm --image=busybox:1.28 --restart=Never -- sleep 3600 --overrides='
{"spec":{"containers":[{"name":"busybox-cm","image":"busybox:1.28","command":["sleep","3600"],"env":[{"name":"APP_ENV","valueFrom":{"configMapKeyRef":{"name":"app-cm","key":"APP_ENV"}}}]}]}}'</code></pre>
          <p style="margin-top:0.75rem;font-size:0.9rem;">Or create a YAML file with <code>env</code> and
            <code>valueFrom.configMapKeyRef</code> and <code>kubectl apply -f</code>.
          </p>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 3: Node selector and drain</h4>
        <p class="playground-context">Node <code>worker-1</code> has label <code>disk=ssd</code>. You need to run a
          pod
          there and later drain the node.</p>
        <p class="playground-task"><strong>Task:</strong> Create a pod <code>ssd-pod</code> (image
          <code>nginx:alpine</code>) that is scheduled only on nodes with <code>disk=ssd</code>. Then cordon and drain
          <code>worker-1</code> (ignore DaemonSet pods).
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl run ssd-pod --image=nginx:alpine --restart=Never --overrides='
{"spec":{"nodeSelector":{"disk":"ssd"}}}'
kubectl cordon worker-1
kubectl drain worker-1 --ignore-daemonsets --delete-emptydir-data</code></pre>
        </div>
      </div>

      <!-- 15 Mock Exams -->
      <h3>Mock Exam 1, 2, 3</h3>
      <p>Time yourself (e.g. 10â€“15 minutes per question). Complete the task in your cluster or playground, then check
        the solution.</p>

      <div class="mock-question">
        <h4>Mock 1: RBAC â€” read-only pods in a namespace</h4>
        <p class="mock-context">Namespace <code>dev</code> exists. A user/service account should only list and get
          pods
          in <code>dev</code>, no create/delete.</p>
        <p class="mock-task"><strong>Task:</strong> Create a Role that allows <code>get</code>, <code>list</code> on
          <code>pods</code> in namespace <code>dev</code>. Create a RoleBinding binding that Role to a user named
          <code>jane</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl create role pod-reader --verb=get,list --resource=pods -n dev
kubectl create rolebinding jane-pod-reader --role=pod-reader --user=jane -n dev</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Or apply YAML with <code>kind: Role</code> (rules with
            apiGroups <code>[""]</code>, resources <code>["pods"]</code>, verbs <code>["get","list"]</code>) and
            <code>kind: RoleBinding</code> (roleRef, subjects with name <code>jane</code>).
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 2: Multi-container pod and logs</h4>
        <p class="mock-context">A pod has two containers: <code>main</code> and <code>sidecar</code>. It is failing
          and
          you need to inspect the previous instanceâ€™s logs.</p>
        <p class="mock-task"><strong>Task:</strong> Write the exact <code>kubectl logs</code> command to stream logs
          from the <strong>previous</strong> instance of the <code>sidecar</code> container in pod
          <code>myapp-pod</code> (namespace <code>default</code>).
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl logs myapp-pod -c sidecar --previous -f</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;"><code>-c sidecar</code> selects the container,
            <code>--previous</code> shows the crashed/previous instance, <code>-f</code> streams.
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 3: Static pod</h4>
        <p class="mock-context">The control plane uses static pods. You need to run a static pod that runs
          <code>busybox</code> with <code>sleep 3600</code>.
        </p>
        <p class="mock-task"><strong>Task:</strong> Create a static pod manifest (YAML) named
          <code>static-busybox</code> on the control plane node. Assume the static pod path is
          <code>/etc/kubernetes/manifests</code>. Provide the manifest path and key fields.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># File: /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command: ["sleep", "3600"]</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Copy this file to the control plane node at
            <code>/etc/kubernetes/manifests/</code>. Kubelet will create the pod. No <code>kubectl apply</code>
            needed.
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 4: Service and endpoints</h4>
        <p class="mock-context">A service <code>web-svc</code> in namespace <code>prod</code> is not getting traffic.
          Pods are running with label <code>app=web</code>.</p>
        <p class="mock-task"><strong>Task:</strong> (1) Check if the service has endpoints. (2) If empty, fix the
          service so it targets pods with label <code>app=web</code>. Assume the service exists but has the wrong
          selector.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl get endpoints web-svc -n prod
kubectl get svc web-svc -n prod -o yaml   # inspect selector
# Fix: patch or edit so selector is app=web
kubectl patch svc web-svc -n prod -p '{"spec":{"selector":{"app":"web"}}}'
# Or kubectl edit svc web-svc -n prod and set spec.selector.app: web
kubectl get endpoints web-svc -n prod     # should list pod IPs now</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 5: etcd backup</h4>
        <p class="mock-context">You need to take a snapshot of etcd for disaster recovery. The cluster was set up with
          kubeadm; etcd runs as a pod and listens on <code>https://127.0.0.1:2379</code> with certs under
          <code>/etc/kubernetes/pki/etcd/</code>.
        </p>
        <p class="mock-task"><strong>Task:</strong> Run the <code>etcdctl snapshot save</code> command with the
          correct
          API version, endpoint, and cert flags. Save to <code>/tmp/etcd-snapshot.db</code>.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Run from the control plane node (or from inside the etcd
            pod).
            Verify with <code>ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-snapshot.db</code>.</p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 6: PV and PVC binding</h4>
        <p class="mock-context">You need to create a PersistentVolume and a PersistentVolumeClaim, then mount it in a
          pod.</p>
        <p class="mock-task"><strong>Task:</strong> Create a PV named <code>pv-log</code> with 100Mi capacity,
          accessMode <code>ReadWriteMany</code>, hostPath <code>/pv/log</code>. Create a PVC named
          <code>claim-log-1</code> requesting 50Mi with <code>ReadWriteMany</code>. Create a pod <code>logger</code>
          (image <code>nginx</code>) mounting the PVC at <code>/var/log/nginx</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  hostPath:
    path: /pv/log
---
# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
---
# pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: logger
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - mountPath: /var/log/nginx
      name: log-vol
  volumes:
  - name: log-vol
    persistentVolumeClaim:
      claimName: claim-log-1</code></pre>
          <pre><code>kubectl apply -f pv.yaml -f pvc.yaml -f pod.yaml
kubectl get pv,pvc,pod</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 7: NetworkPolicy â€” deny all ingress</h4>
        <p class="mock-context">Namespace <code>secure</code> exists with pods labeled <code>app=db</code>. You need
          to
          deny all ingress traffic except from pods labeled <code>app=api</code> on port 5432.</p>
        <p class="mock-task"><strong>Task:</strong> Create a NetworkPolicy in namespace <code>secure</code> that
          selects
          pods with label <code>app=db</code>, denies all ingress, and only allows ingress from pods with
          <code>app=api</code> on TCP port 5432.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
  namespace: secure
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 5432</code></pre>
          <pre><code>kubectl apply -f netpol.yaml
kubectl get networkpolicy -n secure
kubectl describe netpol db-policy -n secure</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 8: Cluster upgrade (kubeadm)</h4>
        <p class="mock-context">Your cluster runs v1.30.0. You need to upgrade the control plane to v1.31.0.</p>
        <p class="mock-task"><strong>Task:</strong> Upgrade kubeadm, then the control plane, then kubelet and kubectl
          on
          the control plane node. Show the commands in order.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># 1. Upgrade kubeadm
sudo apt-mark unhold kubeadm
sudo apt-get update && sudo apt-get install -y kubeadm=1.31.0-*
sudo apt-mark hold kubeadm

# 2. Plan and apply
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.31.0

# 3. Drain the node
kubectl drain &lt;control-plane-node&gt; --ignore-daemonsets

# 4. Upgrade kubelet and kubectl
sudo apt-mark unhold kubelet kubectl
sudo apt-get install -y kubelet=1.31.0-* kubectl=1.31.0-*
sudo apt-mark hold kubelet kubectl
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 5. Uncordon
kubectl uncordon &lt;control-plane-node&gt;
kubectl get nodes</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 9: Ingress with TLS</h4>
        <p class="mock-context">Service <code>webapp-svc</code> exists on port 80 in namespace <code>apps</code>. You
          have a TLS cert and key.</p>
        <p class="mock-task"><strong>Task:</strong> Create a TLS secret <code>webapp-tls</code> and an Ingress
          <code>webapp-ingress</code> that terminates TLS on host <code>webapp.example.com</code> and routes to
          <code>webapp-svc</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># Create TLS secret
kubectl create secret tls webapp-tls --cert=tls.crt --key=tls.key -n apps

# Ingress YAML
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp-ingress
  namespace: apps
spec:
  tls:
  - hosts:
    - webapp.example.com
    secretName: webapp-tls
  rules:
  - host: webapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webapp-svc
            port:
              number: 80</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 10: JSONPath and sorting</h4>
        <p class="mock-context">You need to extract specific information from the cluster using JSONPath.</p>
        <p class="mock-task"><strong>Task:</strong> (1) List all node names sorted by CPU capacity. (2) Output a
          custom-columns table showing pod name and container image for all pods in all namespaces. (3) Get the
          InternalIP of all nodes.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># 1. Nodes sorted by CPU
kubectl get nodes --sort-by=.status.capacity.cpu

# 2. Custom columns: pod name + image
kubectl get pods -A -o=custom-columns='NAME:.metadata.name,IMAGE:.spec.containers[*].image'

# 3. InternalIP of all nodes
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'

# Alternative using range for formatted output:
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.addresses[?(@.type=="InternalIP")].address}{"\n"}{end}'</code></pre>
        </div>
      </div>

      <h3>Ultimate Mocks</h3>
      <p>Extended scenario-based mocks: multi-step troubleshooting, storage (PV/PVC), networking (NetworkPolicy,
        Ingress), and cluster upgrades. Use the same format: try the scenario in a playground, then compare with the
        solutions in the course docs or your notes.</p>
      <p><strong>Ultimate mock strategy:</strong> Set a 2-hour timer, attempt all questions sequentially. Flag hard
        questions and return to them. Prioritize high-weight questions (troubleshooting = 30%). Practice with
        Killer.sh
        (free retake included with CKA exam purchase).</p>

      <h3>Certification Tips</h3>
      <p>Set aliases, use imperative + dry-run, verify answers. Prioritize high-weight questions. Bookmark kubernetes.io/docs.</p>
      <h3>Tips and Tricks</h3>
      <h3>Exam environment setup (do this first!)</h3>
      <p>The CKA exam uses a browser-based terminal. Set up shortcuts immediately:</p>
      <div class="snippet-label">Shell aliases and settings (~/.bashrc)</div>
      <pre><code># Essential aliases â€” set these at the START of the exam
alias k=kubectl
alias kn='kubectl config set-context --current --namespace'
alias kgp='kubectl get pods'
alias kgs='kubectl get svc'
alias kgn='kubectl get nodes'
alias kga='kubectl get all'
alias kaf='kubectl apply -f'
alias kdp='kubectl describe pod'
alias kl='kubectl logs'
alias ke='kubectl exec -it'

# Enable kubectl autocompletion
source <(kubectl completion bash)
complete -o default -F __start_kubectl k

# Set default editor
export KUBE_EDITOR=vi
# or
export EDITOR=vi</code></pre>
      <div class="snippet-label">Vim settings (~/.vimrc)</div>
      <pre><code># Paste this into ~/.vimrc for YAML editing
set tabstop=2
set shiftwidth=2
set expandtab
set number
set autoindent</code></pre>

      <h3>Time management</h3>
      <ul>
        <li><strong>2 hours, ~17 questions.</strong> Average ~7 minutes per question.</li>
        <li><strong>Don't get stuck:</strong> Flag difficult questions, move on, come back.</li>
        <li><strong>High-weight first:</strong> Prioritize questions worth more marks.</li>
        <li><strong>Use imperative commands:</strong> Faster than writing YAML from scratch.</li>
        <li><strong>Bookmark the docs:</strong> Kubernetes.io docs are allowed. Pre-bookmark key pages.</li>
      </ul>

      <h3>Essential bookmarks for the exam</h3>
      <ul>
        <li><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/" target="_blank" rel="noopener">kubectl
            Cheat Sheet</a></li>
        <li><a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">Concepts</a> â†’ Workloads,
          Services, Storage, Config</li>
        <li><a href="https://kubernetes.io/docs/tasks/" target="_blank" rel="noopener">Tasks</a> â†’ Administer a
          Cluster,
          Manage TLS, Configure Pods</li>
        <li><a href="https://kubernetes.io/docs/reference/" target="_blank" rel="noopener">API Reference</a></li>
      </ul>

      <h3>Test pods for networking</h3>
      <p>Run a temporary pod for DNS/connectivity tests:
        <code>kubectl run dnsutils --image=busybox:1.28 --restart=Never --rm -it -- nslookup kubernetes.default.svc.cluster.local</code>.
        For a long-lived debug pod:
        <code>kubectl run debug --image=nicolaka/netshoot -it --rm --restart=Never -- bash</code>.
      </p>

      <h3>Useful one-liners</h3>
      <pre><code># All pods with node info
kubectl get pods -A -o wide

# Events sorted by time
kubectl get events -A --sort-by='.lastTimestamp'

# Node resource usage
kubectl describe node | grep -A5 "Allocated resources"

# Set default namespace
kubectl config set-context --current --namespace=&lt;ns&gt;

# Find which node a pod runs on
kubectl get pod &lt;name&gt; -o jsonpath='{.spec.nodeName}'

# Watch pods (live updates)
kubectl get pods -w

# Delete a pod stuck in Terminating
kubectl delete pod &lt;name&gt; --force --grace-period=0

# Get all images running in the cluster
kubectl get pods -A -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort | uniq

# Check component statuses
kubectl get componentstatuses   # may be deprecated
kubectl get --raw='/readyz?verbose'

# Quick pod for testing
kubectl run tmp --image=busybox:1.28 --rm -it --restart=Never -- sh

# Decode a secret
kubectl get secret &lt;name&gt; -o jsonpath='{.data.password}' | base64 --decode

# Find static pod manifests path
ps aux | grep kubelet | grep config
grep staticPodPath /var/lib/kubelet/config.yaml</code></pre>

      <h3>What's Next</h3>
      <p>After CKA: CKAD (application development), CKS (security). Practice on Killer.sh, KillerCoda, or your own cluster.</p>
      <h3>Common exam mistakes to avoid</h3>
      <ul>
        <li>âŒ Forgetting to switch context (<code>kubectl config use-context &lt;context&gt;</code>) â€” each question
          may
          use a different cluster</li>
        <li>âŒ Wrong namespace â€” always check the question for namespace and use <code>-n &lt;ns&gt;</code></li>
        <li>âŒ YAML indentation errors â€” use <code>kubectl apply -f file.yaml</code> and read the error</li>
        <li>âŒ Not verifying the answer â€” always run <code>kubectl get</code> / <code>kubectl describe</code> to
          confirm
        </li>
        <li>âŒ Spending too long on one question â€” flag it and move on</li>
        <li>âŒ Forgetting <code>--dry-run=client -o yaml</code> â€” the fastest way to generate templates</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 16 Practice &amp; Exam Preparation</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 16 Practice & Exam" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!--18 Cheat Sheet -->
      <h2 id="section-17">17 â€” CKA Exam Cheat Sheet</h2>
      <p>Quick-reference cards for every exam domain. Print this section or keep it open during practice sessions.
        Each
        card contains the most commonly tested commands and YAML patterns from exam dumps.</p>

      <div class="cheat-grid">

        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>kubectl Quick Reference</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># Create resources fast
k run pod1 --image=nginx
k create deploy d1 --image=nginx --replicas=3
k expose deploy d1 --port=80 --name=d1-svc
k create ns dev
k create sa my-sa
k create cm cfg --from-literal=k=v
k create secret generic s1 --from-literal=p=s
k create job j1 --image=busybox -- echo hi
k create cronjob cj --image=b --schedule="* * * * *" -- sh

# Generate YAML
k run x --image=nginx --dry-run=client -o yaml > x.yaml
k create deploy x --image=nginx --dry-run=client -o yaml

# Inspect
k get pods -A -o wide
k describe pod &lt;name&gt;
k logs &lt;pod&gt; -c &lt;container&gt; --previous
k exec -it &lt;pod&gt; -- sh
k get events --sort-by='.lastTimestamp'

# Edit / Patch / Replace
k edit deploy &lt;name&gt;
k patch svc s1 -p '{"spec":{"type":"NodePort"}}'
k replace --force -f pod.yaml
k delete pod x --force --grace-period=0

# Context & Namespace
k config use-context &lt;ctx&gt;
k config set-context --current --namespace=dev</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Pod Skeleton</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
  namespace: default
  labels:
    app: myapp
spec:
  serviceAccountName: my-sa     # optional
  nodeSelector:                  # optional
    disk: ssd
  tolerations:                   # optional
  - key: "node-role"
    operator: "Exists"
    effect: "NoSchedule"
  initContainers:                # optional
  - name: init
    image: busybox
    command: ['sh','-c','sleep 5']
  containers:
  - name: app
    image: nginx:1.21
    ports:
    - containerPort: 80
    env:
    - name: KEY
      value: val
    envFrom:
    - configMapRef:
        name: my-cm
    - secretRef:
        name: my-secret
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 256Mi
    livenessProbe:
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 10
    readinessProbe:
      httpGet:
        path: /ready
        port: 80
    volumeMounts:
    - name: data
      mountPath: /data
    securityContext:
      runAsUser: 1000
      runAsNonRoot: true
      capabilities:
        drop: ["ALL"]
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: my-pvc
  # OR emptyDir:
  # - name: tmp
  #   emptyDir: {}</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Deployment + Service</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
spec:
  type: NodePort        # or ClusterIP
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080     # only for NodePort</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>RBAC Pattern</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># Imperative (fastest in exam)
k create role dev-role \
  --verb=get,list,create,delete \
  --resource=pods -n dev

k create rolebinding dev-bind \
  --role=dev-role --user=jane -n dev

k create clusterrole node-viewer \
  --verb=get,list --resource=nodes

k create clusterrolebinding node-bind \
  --clusterrole=node-viewer --user=jane

# Check permissions
k auth can-i create pods --as=jane -n dev
k auth can-i '*' '*' --as=system:serviceaccount:default:my-sa

# ServiceAccount in pod
spec:
  serviceAccountName: my-sa</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>ETCD Backup & Restore</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># BACKUP
ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Verify
ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd.db

# RESTORE
ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd.db \
  --data-dir=/var/lib/etcd-restored

# Then update etcd manifest:
vi /etc/kubernetes/manifests/etcd.yaml
# Change: hostPath.path â†’ /var/lib/etcd-restored
# Wait for etcd pod to restart

# Find etcd certs (from pod spec)
k describe pod etcd-master -n kube-system | grep -A5 cert</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Cluster Upgrade Sequence</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># === CONTROL PLANE ===
apt-mark unhold kubeadm
apt-get update && apt-get install -y kubeadm=1.XX.0-*
apt-mark hold kubeadm
kubeadm upgrade plan
kubeadm upgrade apply v1.XX.0
kubectl drain &lt;cp-node&gt; --ignore-daemonsets
apt-mark unhold kubelet kubectl
apt-get install -y kubelet=1.XX.0-* kubectl=1.XX.0-*
apt-mark hold kubelet kubectl
systemctl daemon-reload && systemctl restart kubelet
kubectl uncordon &lt;cp-node&gt;

# === WORKER NODES ===
kubectl drain &lt;worker&gt; --ignore-daemonsets
# SSH to worker:
apt-mark unhold kubeadm kubelet
apt-get install -y kubeadm=1.XX.0-* kubelet=1.XX.0-*
kubeadm upgrade node
systemctl daemon-reload && systemctl restart kubelet
apt-mark hold kubeadm kubelet
# Back on control plane:
kubectl uncordon &lt;worker&gt;</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Storage: PV + PVC + SC</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># PersistentVolume
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity:
    storage: 1Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/data
---
# PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc1
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 500Mi
  storageClassName: ""   # empty = static binding
---
# StorageClass (dynamic)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
reclaimPolicy: Delete
allowVolumeExpansion: true

# Mount in pod:
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc1
  containers:
  - volumeMounts:
    - mountPath: /data
      name: data</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Networking Cheat Sheet</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># NetworkPolicy (allow specific ingress)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes: [Ingress, Egress]
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    - namespaceSelector:
        matchLabels:
          env: prod
    ports:
    - port: 3306
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: cache
    ports:
    - port: 6379

# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-svc
            port:
              number: 80</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>Troubleshooting Checklist</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># 1. Check nodes
k get nodes
k describe node &lt;node&gt;  # conditions, taints

# 2. Check control plane pods
k get pods -n kube-system
k logs kube-apiserver-master -n kube-system
k logs kube-scheduler-master -n kube-system

# 3. Check kubelet (on node)
systemctl status kubelet
journalctl -u kubelet -f
cat /var/lib/kubelet/config.yaml

# 4. Check service &amp; endpoints
k get svc,endpoints &lt;svc-name&gt;
# If endpoints empty â†’ wrong selector

# 5. Pod debugging
k get pods -o wide
k describe pod &lt;name&gt;   # check Events
k logs &lt;pod&gt; --previous
k exec -it &lt;pod&gt; -- sh

# 6. DNS test
k run dns-test --image=busybox:1.28 --rm -it \
  --restart=Never -- nslookup kubernetes

# 7. Network test
k run net-test --image=nicolaka/netshoot \
  --rm -it --restart=Never -- bash

# 8. Certificate check
openssl x509 -in /etc/kubernetes/pki/apiserver.crt \
  -text -noout | grep -A2 Validity</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>DaemonSet / Job / CronJob</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-agent
spec:
  selector:
    matchLabels:
      app: log-agent
  template:
    metadata:
      labels:
        app: log-agent
    spec:
      containers:
      - name: agent
        image: fluentd
---
# Job (run to completion)
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-job
spec:
  completions: 3
  parallelism: 2
  backoffLimit: 4
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl","-Mbignum=bpi",
                  "-wle","print bpi(2000)"]
      restartPolicy: Never
---
# CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup
spec:
  schedule: "0 2 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: busybox
            command: ["/bin/sh","-c","date"]
          restartPolicy: OnFailure</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>JSONPath & Output</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># Single value
k get pod x -o jsonpath='{.spec.nodeName}'

# All pod names
k get pods -o jsonpath='{.items[*].metadata.name}'

# Range (formatted)
k get nodes -o jsonpath='
{range .items[*]}
  {.metadata.name}{"\t"}
  {.status.capacity.cpu}{"\n"}
{end}'

# Filter with condition
k get nodes -o jsonpath='
{.items[*].status.addresses
[?(@.type=="InternalIP")].address}'

# Custom columns
k get pods -A -o custom-columns=\
NAME:.metadata.name,\
NODE:.spec.nodeName,\
IMAGE:.spec.containers[0].image

# Sort
k get pods --sort-by=.metadata.creationTimestamp
k get pv --sort-by=.spec.capacity.storage

# Output to file (exam pattern)
k get nodes -o json > /opt/output.json
k get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/os.txt

# Count
k get pods -A --no-headers | wc -l</code></pre>
          </div>
        </div>


        <div class="cheat-card">
          <div class="cheat-card-header">
            <h4>CSR / Certificates</h4>
          </div>
          <div class="cheat-card-body">
            <pre><code># Generate key + CSR
openssl genrsa -out user.key 2048
openssl req -new -key user.key \
  -subj "/CN=user/O=group" -out user.csr

# Create K8s CSR object
cat user.csr | base64 | tr -d '\n'

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: user
spec:
  request: &lt;base64-csr&gt;
  signerName: kubernetes.io/kube-apiserver-client
  usages: [client auth]

# Approve &amp; extract
k certificate approve user
k get csr user -o jsonpath='{.status.certificate}' \
  | base64 -d > user.crt

# Add to kubeconfig
k config set-credentials user \
  --client-certificate=user.crt \
  --client-key=user.key
k config set-context user-ctx \
  --cluster=kubernetes --user=user
k config use-context user-ctx</code></pre>
          </div>

        </div><!-- end cheat-grid -->

    <!-- Comments â€” 17 Cheat Sheet -->
    <section class="comments-section" id="comments" aria-label="Comments">
      <h3 class="comments-title">Comments â€” 17 Cheat Sheet</h3>
      <script src="https://utteranc.es/client.js"
              repo="SKCloudOps/k8s"
              issue-term="CKA Guide - 17 Cheat Sheet"
              theme="github-light"
              crossorigin="anonymous"
              async>
      </script>
    </section>

    </main>

  </div>
  <footer class="site-footer">
    CKA Study Guide â€” Reference for Certified Kubernetes Administrator exam preparation.
    &nbsp;Â·&nbsp; <span id="footer-stats"></span>
  </footer>

  <!-- Reading Progress Bar -->
  <div id="reading-progress"></div>

  <!-- Toast Container -->
  <div id="toast-container"></div>

  <!-- Search Overlay -->
  <div id="search-overlay" role="dialog" aria-modal="true" aria-label="Search">
    <div id="search-box">
      <div id="search-input-wrap">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"></circle>
          <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
        <input type="text" id="search-input" placeholder="Search topics, commands, conceptsâ€¦" autocomplete="off"
          spellcheck="false">
        <span id="search-shortcut-hint">ESC to close</span>
      </div>
      <div id="search-results">
        <div id="search-empty">Type to search across all sectionsâ€¦</div>
      </div>
      <div id="search-footer">
        <span><kbd>â†‘â†“</kbd> navigate</span>
        <span><kbd>â†µ</kbd> open</span>
        <span><kbd>ESC</kbd> close</span>
      </div>
    </div>
  </div>

  <!-- Keyboard Shortcuts Overlay -->
  <div id="shortcuts-overlay" role="dialog" aria-modal="true">
    <div id="shortcuts-box">
      <h2>âŒ¨ï¸ Keyboard Shortcuts</h2>
      <div class="shortcut-grid">
        <div class="shortcut-row"><span class="sc-desc">Search</span><span class="sc-keys"><kbd
              class="sc-key">Ctrl</kbd><kbd class="sc-key">K</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Dark Mode</span><span class="sc-keys"><kbd
              class="sc-key">D</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Next Section</span><span class="sc-keys"><kbd
              class="sc-key">â†’</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Prev Section</span><span class="sc-keys"><kbd
              class="sc-key">â†</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Notes Panel</span><span class="sc-keys"><kbd
              class="sc-key">N</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">This panel</span><span class="sc-keys"><kbd
              class="sc-key">?</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Mark Complete</span><span class="sc-keys"><kbd
              class="sc-key">C</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Bookmark Section</span><span class="sc-keys"><kbd
              class="sc-key">B</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Font Bigger</span><span class="sc-keys"><kbd
              class="sc-key">+</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Font Smaller</span><span class="sc-keys"><kbd
              class="sc-key">-</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Pomodoro Timer</span><span class="sc-keys"><kbd
              class="sc-key">T</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Close / Back</span><span class="sc-keys"><kbd
              class="sc-key">ESC</kbd></span></div>
      </div>
      <div id="shortcuts-close-hint">Press <kbd class="sc-key">ESC</kbd> or click outside to close</div>
    </div>
  </div>

  <!-- Notes Slide-in Panel -->
  <div id="notes-panel" role="complementary" aria-label="Section Notes">
    <div id="notes-header">
      <h3>ðŸ“ Section Notes</h3>
      <button id="notes-close" title="Close notes">Ã—</button>
    </div>
    <div id="notes-section-name">No section selected</div>
    <textarea id="notes-textarea"
      placeholder="Write your notes for this sectionâ€¦ They're saved automatically."></textarea>
    <div id="notes-footer">
      <span id="notes-char-count">0 characters</span>
      <button id="notes-export-btn" title="Export all notes as Markdown">â¬‡ Export</button>
      <button id="notes-save-btn">Save âœ“</button>
    </div>
  </div>



  <!-- Zen Mode Hint -->
  <div id="zen-hint">Press <kbd>Z</kbd> or move mouse to top to show header &nbsp;Â·&nbsp; Press <kbd>Z</kbd> again to
    exit Zen Mode</div>

  <!-- Text Highlight Toolbar -->
  <div id="highlight-toolbar">
    <div class="hl-label">Highlight</div>
    <div class="hl-row">
      <div class="hl-swatch" data-color="hl-yellow" style="background:#fde047" title="Yellow"></div>
      <div class="hl-swatch" data-color="hl-green" style="background:#86efac" title="Green"></div>
      <div class="hl-swatch" data-color="hl-pink" style="background:#f9a8d4" title="Pink"></div>
      <div class="hl-swatch" data-color="hl-blue" style="background:#93c5fd" title="Blue"></div>
      <button class="hl-clear" id="hl-clear-btn" title="Remove highlight">âœ•</button>
    </div>
  </div>



  <!-- CKA Quiz Overlay -->
  <div id="quiz-overlay" role="dialog" aria-modal="true" aria-label="CKA Quiz">
    <div id="quiz-box">
      <div id="quiz-header">
        <h2>âš¡ CKA Quick Quiz</h2>
        <button id="quiz-close" title="Close quiz">Ã—</button>
      </div>
      <div id="quiz-progress-bar">
        <div id="quiz-progress-fill" style="width:0%"></div>
      </div>
      <div id="quiz-question"></div>
      <div id="quiz-options"></div>
      <div id="quiz-explanation"></div>
      <div id="quiz-nav">
        <span id="quiz-score">Question 1 of 15</span>
        <button id="quiz-next">Next â†’</button>
      </div>
      <div id="quiz-results">
        <div class="result-score" id="quiz-final-score"></div>
        <p id="quiz-result-msg"></p>
        <button id="quiz-restart">ðŸ”„ Try Again</button>
      </div>
    </div>
  </div>

  <!-- Diagram zoom lightbox -->
  <div id="diagram-zoom-overlay" class="diagram-zoom-overlay" aria-hidden="true">
    <div class="zoom-container">
      <img id="diagram-zoom-img" src="" alt="">
    </div>
    <div class="diagram-zoom-controls">
      <button type="button" id="diagram-zoom-in" title="Zoom in">+</button>
      <button type="button" id="diagram-zoom-out" title="Zoom out">âˆ’</button>
      <button type="button" id="diagram-zoom-reset" title="Reset zoom">âŸ²</button>
      <button type="button" id="diagram-zoom-close" class="zoom-close" title="Close">Close</button>
    </div>
  </div>

  <script src="script.js"></script>
</body>

</html>
