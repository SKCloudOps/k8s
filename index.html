<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CKA Study Guide 2026 | Certified Kubernetes Administrator Exam Prep & Cheat Sheet</title>
  <meta name="description"
    content="Free Ultimate CKA study guide for 2026: cluster architecture, networking, storage, RBAC, troubleshooting. Commands, YAML examples &amp; practice mocks for Certified Kubernetes Administrator exam.">
  <meta name="keywords"
    content="CKA Study Guide 2026, Certified Kubernetes Administrator, CKA Exam Prep, Kubernetes Certification, K8s Troubleshooting, Kubectl Cheat Sheet, CKA Practice Exam, Kubernetes Admin Guide">
  <meta name="author" content="CKA Study Guide">
  <meta name="robots" content="index, follow">
  <!-- Replace with your real page URL when you publish the site -->
  <link rel="canonical" href="https://skcloudops.github.io/k8s/">
  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator Preparation">
  <meta property="og:description"
    content="Free CKA study guide with commands, YAML examples and diagrams. Covers cluster architecture, scheduling, networking, storage, security and troubleshooting for the CKA exam.">
  <meta property="og:url" content="https://skcloudops.github.io/k8s/">
  <meta property="og:locale" content="en_US">
  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator">
  <meta name="twitter:description"
    content="Free CKA study guide: commands, YAML snippets and diagrams for Certified Kubernetes Administrator (CKA) exam preparation.">
  <meta name="theme-color" content="#1a7f37">

  <!-- SEO & Social Media -->
  <meta property="og:site_name" content="CKA Study Guide">
  <meta property="og:image" content="https://skcloudops.github.io/k8s/assets/cka-og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta name="twitter:image" content="https://skcloudops.github.io/k8s/assets/cka-og-image.png">
  <meta name="twitter:creator" content="@CKAStudyGuide">

  <!-- Structured Data: Course & FAQ for Rich Snippets -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Course",
    "name": "Certified Kubernetes Administrator (CKA) Study Guide",
    "description": "Comprehensive reference guide for the CKA exam. Covers architecture, networking, storage, and troubleshooting with interactive labs and cheat sheets.",
    "provider": {
      "@type": "Organization",
      "name": "SK CloudOps",
      "sameAs": "https://skcloudops.github.io/k8s/"
    },
    "courseCode": "CKA",
    "educationalLevel": "Intermediate",
    "offers": [{
      "@type": "Offer",
      "category": "Free",
      "price": "0",
      "priceCurrency": "USD"
    }]
  }
  </script>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "FAQPage",
    "mainEntity": [{
      "@type": "Question",
      "name": "What are the core topics for the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "The CKA exam covers Cluster Architecture (25%), Workloads & Scheduling (15%), Services & Networking (20%), Storage (10%), and Troubleshooting (30%)."
      }
    }, {
      "@type": "Question",
      "name": "How long is the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "The Certified Kubernetes Administrator (CKA) exam is a 2-hour performance-based test."
      }
    }, {
      "@type": "Question",
      "name": "Can I use documentation during the CKA exam?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Yes, candidates can access official Kubernetes documentation during the exam via a single browser tab."
      }
    }]
  }
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Outfit:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="style.css">
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "LearningResource",
    "name": "CKA Exam Study Guide | Certified Kubernetes Administrator Preparation",
    "description": "Free study guide for the Certified Kubernetes Administrator (CKA) exam. Covers cluster architecture, scheduling, workloads, networking, storage, security, and troubleshooting with commands, YAML examples and diagrams.",
    "learningResourceType": "Study Guide",
    "educationalLevel": "Professional",
    "teaches": ["Kubernetes", "Container orchestration", "kubectl", "CKA certification", "Cluster administration"],
    "about": [
      { "@type": "Thing", "name": "Certified Kubernetes Administrator" },
      { "@type": "Thing", "name": "Kubernetes" }
    ]
  }
  </script>
</head>

<body>
  <header class="site-header" style="z-index: 2000;">
    <div style="display: flex; align-items: center; gap: 0.5rem; flex: 1; min-width: 0;">
      <button type="button" class="icon-btn" id="toggle-left" title="Toggle Navigation">
        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="9" y1="3" x2="9" y2="21"></line>
        </svg>
      </button>
      <h1 style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis;">CKA Study Guide</h1>
      <span class="badge">CKA</span>
    </div>

    <div class="header-controls">
      <!-- Challenge Mode Toggle -->
      <div class="challenge-toggle-wrapper header-hide-mobile" title="Hide answers to test yourself">
        <span class="challenge-label" style="display: none;">Challenge</span>
        <label class="toggle-switch">
          <input type="checkbox" id="challenge-toggle">
          <span class="slider"></span>
        </label>
      </div>

      <!-- Font Size Controls -->
      <div class="font-size-controls header-hide-mobile" title="Font size">
        <button id="font-decrease" title="Decrease font size">A-</button>
        <button id="font-increase" title="Increase font size">A+</button>
      </div>

      <!-- Search Button -->
      <button type="button" class="icon-btn" id="search-btn" title="Search (Ctrl+K)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"></circle>
          <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
      </button>

      <!-- Notes Button -->
      <button type="button" class="icon-btn header-hide-mobile" id="notes-btn" title="Section Notes (N)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M14 2H6a2 2 0 0 0-2 2v16c0 1.1.9 2 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
      </button>

      <!-- Dark Mode -->
      <button type="button" class="icon-btn" id="dark-mode-btn" title="Toggle Dark Mode (D)">
        <svg id="dm-icon-moon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
          stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
        </svg>
        <svg id="dm-icon-sun" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
          stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display:none">
          <circle cx="12" cy="12" r="5"></circle>
          <line x1="12" y1="1" x2="12" y2="3"></line>
          <line x1="12" y1="21" x2="12" y2="23"></line>
          <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
          <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
          <line x1="1" y1="12" x2="3" y2="12"></line>
          <line x1="21" y1="12" x2="23" y2="12"></line>
          <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
          <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
        </svg>
      </button>

      <!-- Keyboard Shortcuts -->
      <button type="button" class="icon-btn header-hide-mobile" id="shortcuts-btn" title="Keyboard Shortcuts (?)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <rect x="2" y="6" width="20" height="12" rx="2"></rect>
          <path d="M6 10h.01M10 10h.01M14 10h.01M18 10h.01M8 14h8"></path>
        </svg>
      </button>

      <!-- Pomodoro Timer -->
      <div class="pomo-wrap header-hide-mobile">
        <button type="button" class="icon-btn" id="pomo-header-btn" title="Pomodoro Timer (T)">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
            stroke-linecap="round" stroke-linejoin="round">
            <circle cx="12" cy="13" r="8"></circle>
            <path d="M12 9v4l2 2"></path>
            <path d="M5 3L2 6"></path>
            <path d="M22 6l-3-3"></path>
          </svg>
          <span class="pomo-tick" id="pomo-header-tick">25:00</span>
        </button>
        <div id="pomodoro-widget">
          <div id="pomo-label">ðŸŽ¯ Focus Session</div>
          <svg class="pomo-ring" viewBox="0 0 80 80">
            <circle cx="40" cy="40" r="34" fill="none" stroke="var(--surface2)" stroke-width="6" />
            <circle id="pomo-progress-circle" cx="40" cy="40" r="34" fill="none" stroke="var(--accent2)"
              stroke-width="6" stroke-linecap="round" stroke-dasharray="213.6" stroke-dashoffset="0"
              transform="rotate(-90 40 40)" />
          </svg>
          <div id="pomo-time">25:00</div>
          <div class="pomo-btns">
            <button class="pomo-btn primary" id="pomo-start">Start</button>
            <button class="pomo-btn" id="pomo-reset">Reset</button>
          </div>
          <div id="pomo-sessions">Sessions today: <strong id="pomo-session-count">0</strong></div>
        </div>
      </div>


      <!-- Zen Mode -->
      <button type="button" class="icon-btn header-hide-mobile" id="zen-btn" title="Zen / Focus Mode (Z)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle>
          <path d="M8 12a4 4 0 0 1 8 0"></path>
          <line x1="12" y1="8" x2="12" y2="8.01"></line>
        </svg>
      </button>

      <!-- Quiz / Flashcards -->
      <button type="button" class="icon-btn header-hide-mobile" id="quiz-btn" title="CKA Quiz Mode (Q)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 11l3 3L22 4"></path>
          <path d="M21 12v7a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h11"></path>
        </svg>
      </button>


      <button type="button" class="nav-toggle" aria-label="Open menu" id="nav-toggle">â˜°</button>
    </div>
  </header>



  <div class="layout">
    <nav id="nav" aria-label="Section navigation">
      <div class="nav-header">
        <span class="nav-section">CKA Preparation</span>
        <button type="button" class="nav-close" aria-label="Close menu" id="nav-close">Ã—</button>
      </div>

      <a href="#section-01">01 Introduction & Foundations</a>
      <a href="#section-02">02 Core Concepts</a>
      <a href="#section-03">03 Scheduling</a>
      <a href="#section-04">04 Logging & Monitoring</a>
      <a href="#section-05">05 Application Lifecycle</a>
      <a href="#section-06">06 Cluster Maintenance</a>
      <a href="#section-07">07 Security</a>
      <a href="#section-08">08 Storage</a>
      <a href="#section-09">09 Networking</a>
      <a href="#section-10">10 Cluster Design & Installation</a>
      <a href="#section-11">11 kubeadm Installation</a>
      <a href="#section-12">12 Helm</a>
      <a href="#section-13">13 Kustomize</a>
      <a href="#section-14">14 Troubleshooting</a>
      <a href="#section-15">15 Advanced Kubectl & JSON Path</a>
      <a href="#section-16">16 Practice & Exam Preparation</a>
      <a href="cheat-sheet.html">17 Cheat Sheet</a>
      <a href="lab.html" style="margin-top: 0.5rem; padding-top: 0.5rem; border-top: 1px solid var(--border);">ðŸ“‹ CKA Lab</a>

    </nav>
    <main>
      <header id="intro">
        <h1 class="page-title">Certified Kubernetes Administrator (CKA)</h1>
        <p class="subtitle">Complete reference guide for the CKA exam. Use the navigation to jump between topics.</p>

        <figure class="diagram">
          <img src="assets/k8s-request-workflow.png" alt="Kubernetes request workflow diagram showing kubectl apply to pod running flow" width="1024" height="576" style="max-width:100%;height:auto;display:block;">
          <figcaption><strong>Kubernetes Request Flow</strong> â€” What happens when you run <code>kubectl apply -f deployment.yaml</code>. 1) kubectl â†’ API Server (auth, RBAC, admission). 2) API Server â†’ etcd (persist desired state). 3) Controller creates Pod. 4) Scheduler assigns node. 5) kubelet pulls image & runs container via CRI. 6) CNI assigns pod IP; kube-proxy programs Service routing. 7) Pod Running. Every CKA candidate must understand this flow.</figcaption>
        </figure>
      </header>

      <!-- 01 Introduction & Foundations -->
      <h2 id="section-01">01 â€” Introduction & Foundations</h2>

      <div class="cka-definition">
        <div class="cka-def-term">CKA â€” Certified Kubernetes Administrator</div>
        <div class="cka-def-body">A performance-based certification from the Linux Foundation / CNCF that validates your ability to perform Kubernetes administration tasks. The exam is 2 hours, uses live clusters, and allows access to official Kubernetes documentation. Passing score: 66%.</div>
      </div>

      <h3>Course Introduction</h3>
      <p>This study guide aligns with the full CKA curriculum. It covers cluster architecture, workloads, networking, storage, security, and troubleshooting with commands, YAML examples, and CKA-focused definitions.</p>

      <h3>Certification Overview</h3>
      <p>The CKA is part of the <strong>Kubernetes Trilogy</strong>: CKA (Administrator), CKAD (Application Developer), and CKS (Security Specialist). Use code <code>KUBERNETES15</code> for a 15% discount at registration.</p>

      <h3>Course focus</h3>
      <p>This material focuses on the <strong>administration</strong> part of Kubernetes, aligned with the CKA exam.</p>
      <h4>Pre-requisites</h4>
      <ul>
        <li>Docker</li>
        <li>Basics of Kubernetes (Pods, Deployments, Services)</li>
        <li>YAML</li>
        <li>Basic lab environment (e.g. VirtualBox)</li>
      </ul>
      <h4>Exam areas (weighted)</h4>
      <ul>
        <li><strong>Cluster Architecture, Installation & Validation</strong> (25%): Design, install, configure,
          validate, manage HA, etcd, upgrade.</li>
        <li><strong>Workloads & Scheduling</strong> (15%): Deployments, scaling, DaemonSets, resource limits, scheduling
          (manual, node selector, affinity, taints/tolerations), static pods.</li>
        <li><strong>Services & Networking</strong> (20%): Networking model, Services (ClusterIP, NodePort,
          LoadBalancer), DNS, CNI, Ingress.</li>
        <li><strong>Storage</strong> (10%): PV, PVC, StorageClass, volume mounts.</li>
        <li><strong>Troubleshooting</strong> (30%): Application, control plane, worker nodes, network.</li>
        <li>Plus: Security (RBAC, TLS, kubeconfig, NetworkPolicies), Logging/Monitoring, Application Lifecycle
          (rollouts, ConfigMaps, Secrets).</li>
        <li><strong>2025 curriculum additions:</strong> Helm and Kustomize for cluster components (sections 12, 13); Gateway API with Ingress; CRDs and operators; extension interfaces (CNI, CSI, CRI); CoreDNS; workload autoscaling (HPA); PodDisruptionBudgets.</li>
      </ul>
      <h3>Course Release Notes</h3>
      <p>This guide aligns with the CKA 2025â€“2026 curriculum. Updates include: Helm & Kustomize (sections 12â€“13), Gateway API, Admission Controllers, HPA/VPA, PodDisruptionBudgets, CoreDNS, and extension interfaces (CNI, CSI, CRI).</p>

      <h3>Certification</h3>
      <p>Certified Kubernetes Administrator (CKA): exam curriculum, candidate handbook, and exam tips are published by
        the Linux Foundation / CNCF. Use code <code>KUBERNETES15</code> when registering for CKA or CKAD for a 15%
        discount.</p>

      <h3>The Kubernetes Trilogy</h3>
      <p>CKA (Administrator) â†’ CKAD (Application Developer) â†’ CKS (Security Specialist). Each builds on the previous; CKA is the foundation for cluster operations.</p>

      <h3>Notes & Resources</h3>
      <p>Use the notes panel (N) in this guide to capture your own notes per section. Bookmark <a href="https://kubernetes.io/docs/" target="_blank" rel="noopener">kubernetes.io/docs</a> for the exam.</p>

      <h3>FAQ</h3>
      <ul>
        <li><strong>How long is the CKA exam?</strong> 2 hours, performance-based.</li>
        <li><strong>Can I use documentation?</strong> Yes, one tab to kubernetes.io/docs.</li>
        <li><strong>Passing score?</strong> 66%.</li>
        <li><strong>What's next after CKA?</strong> CKAD (Application Developer) or CKS (Security Specialist).</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 01 Introduction &amp; Foundations</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 01 Introduction & Foundations" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 02 Core Concepts -->
      <h2 id="section-02">02 â€” Core Concepts</h2>

      <div class="cka-definition">
        <div class="cka-def-term">Control Plane</div>
        <div class="cka-def-body">The brain of the cluster. Hosts kube-apiserver, etcd, kube-scheduler, kube-controller-manager, and optionally cloud-controller-manager. Manages cluster state, scheduling, and reconciliation. Can run as static pods or systemd services.</div>
      </div>

      <div class="cka-definition">
        <div class="cka-def-term">Worker Node</div>
        <div class="cka-def-body">A machine (VM or bare metal) that runs your workloads. Runs kubelet, kube-proxy, and a container runtime (containerd/CRI-O). Hosts Pods and communicates with the control plane via the API server.</div>
      </div>

      <h3>Cluster Architecture</h3>
      <p>A Kubernetes cluster consists of a <strong>Control Plane</strong> (the brain) and <strong>Worker Nodes</strong>
        (the muscle).</p>
      <ul>
        <li><strong>Control Plane:</strong> Manages the state of the cluster. It makes global decisions (scheduling),
          detects and responds to cluster events (starting new pods), and stores the cluster configuration.</li>
        <li><strong>Worker Node:</strong> A machine (VM or physical) that runs containerized applications. It hosts the
          Pods and communicates with the control plane via the kubelet.</li>
      </ul>

      <h3>Control Plane Components</h3>

      <h4>kube-apiserver</h4>
      <p>The <strong>kube-apiserver</strong> is the front-end of the Kubernetes control plane. It exposes the Kubernetes REST API and is the <strong>only</strong> component that communicates directly with <strong>etcd</strong>. Every request â€” from kubectl, from the scheduler, from the kubelet, from controllers â€” goes through the API server. It authenticates and authorizes requests, validates and defaults resource specs, and persists state to etcd. It is designed to scale horizontally (run multiple replicas behind a load balancer).</p>

      <h4>etcd / ETCD in Kubernetes</h4>
      <div class="cka-definition">
        <div class="cka-def-term">etcd</div>
        <div class="cka-def-body">Distributed key-value store used as Kubernetes' backing store. Uses RAFT consensus. Only kube-apiserver talks to etcd. CKA exam: know <code>etcdctl</code>/<code>etcdutl</code> for snapshot backup/restore, <code>--endpoints</code>, and <code>ETCDCTL_API=3</code>.</div>
      </div>
      <p>A consistent and highly-available <strong>key-value store</strong> used as Kubernetes' backing store for all
        cluster data.
        <br><em>Pro Tip:</em> It uses the <strong>RAFT algorithm</strong> for consensus. It is the "single source of
        truth" â€” if data is in etcd, it exists; otherwise, it does not.
      </p>
      <h4>ETCD Commands</h4>
      <p>Use <code>etcdctl</code> (etcd v3) or <code>etcdutl</code> for snapshots. Set <code>ETCDCTL_API=3</code>. See Cluster Maintenance for backup/restore.</p>
      <figure class="diagram">
        <img src="assets/k8s-etcd-raft.png" alt="etcd RAFT consensus: Leader replicates writes to Followers" width="560" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>etcd RAFT:</strong> Odd number of nodes (3 or 5). Leader handles writes and replicates log to followers. Majority must ack before commit.</figcaption>
      </figure>

      <h4>Kube Scheduler</h4>
      <p>Watches for newly created Pods with no assigned node and selects a node for them to run on.
        <br><em>Process:</em>
        <br>1. <strong>Filtering:</strong> Ruling out nodes that don't meet requirements (CPU/RAM, Taints, Affinity).
        <br>2. <strong>Scoring:</strong> Ranking the remaining nodes to find the best fit.
      </p>

      <h4>Kube Controller Manager</h4>
      <p>A daemon that runs controller processes. Logically, each controller is a separate process, but they are
        compiled into a single binary.
        <br><em>Responsibility:</em> <strong>Reconciliation Loop</strong>. It continuously watches the <em>current
          state</em> and compares it to the <em>desired state</em>, making changes to match them.
        <br><em>Examples:</em> Node Controller (notices down nodes), Replication Controller (maintains pod count),
        Endpoints Controller.
      </p>

      <h4>cloud-controller-manager</h4>
      <p>Embeds cloud-specific control logic. It lets you link your cluster into your cloud provider's API (e.g., AWS,
        Azure, GCP). It handles things like Node instances (cloud VMs), LoadBalancers, and Routes.</p>

      <h3>Node Components</h3>

      <h4>Kubelet</h4>
      <p>An agent that runs on each node. It registers the node with the API server and ensures that containers are
        running in a Pod.
        <br><em>Mechanism:</em> It takes a set of PodSpecs (from API server or static files) and ensures the containers
        described are running and healthy. It does <strong>not</strong> manage containers not created by Kubernetes.
      </p>

      <h4>Kube Proxy</h4>
      <p>A network proxy running on each node. It maintains network rules that allow network communication to your Pods
        from inside or outside the cluster.
        <br><em>Implementation:</em> Commonly uses <strong>iptables</strong> or <strong>IPVS</strong> to forward traffic
        to backend Pods (Service abstraction).
      </p>

      <h4>Container Runtime</h4>
      <p>The software responsible for running containers. Kubernetes supports any runtime that adheres to the
        <strong>Container Runtime Interface (CRI)</strong>.
      </p>

      <h3>Docker vs ContainerD (CRI)</h3>
      <h4>Docker Deprecation Note</h4>
      <p>Kubernetes v1.24+ no longer supports Docker Engine as a container runtime. Docker images still work â€” they use the OCI format. Use containerd or CRI-O instead.</p>
      <p><strong>Fact:</strong> Kubernetes removed the "dockershim" in v1.24. This means Kubernetes can no longer use
        the Docker Engine directly as a runtime. It now uses <strong>containerd</strong> or <strong>CRI-O</strong> via
        the CRI.</p>
      <ul>
        <li><strong>containerd:</strong> An industry-standard container runtime. It was part of Docker but is now
          independent.</li>
        <li><strong>crictl:</strong> A CLI tool for CRI-compatible container runtimes. Used mainly for debugging on the
          node.</li>
        <li><strong>nerdctl:</strong> A Docker-compatible CLI for containerd (supports <code>docker run</code> style
          commands).</li>
      </ul>
      <div class="snippet-label">Useful crictl commands</div>
      <pre><code>crictl ps -a                        # list all containers
crictl images                       # list images
crictl pods                         # list pods (sandbox containers)
crictl logs &lt;container-id&gt;          # view logs directly from runtime
crictl inspect &lt;container-id&gt;       # deep dive into container status</code></pre>

      <figure class="diagram">
        <img src="assets/k8s-enterprise-architecture.png" alt="Kubernetes Cluster Architecture" width="960" height="560"
          style="max-width:100%;height:auto;display:block;">
        <figcaption>
          <strong>Kubernetes Cluster Architecture</strong> â€” The <strong>kube-apiserver</strong> is the single gateway:
          every component communicates exclusively through it. <strong>etcd</strong> is the sole persistent store and is
          only accessed by the API server. Each worker node runs a <strong>kubelet</strong> (receives pod specs via
          watch), <strong>kube-proxy</strong> (programs iptables/IPVS for Service routing), and a <strong>container
            runtime</strong> (executes containers via CRI).
        </figcaption>
      </figure>

      <h3>Workloads</h3>

      <h4>Pods</h4>
      <div class="cka-definition">
        <div class="cka-def-term">Pod</div>
        <div class="cka-def-body">Smallest deployable unit. One or more containers sharing network and storage. Ephemeral â€” when a pod dies, it is not resurrected; a controller creates a new one. CKA: use <code>kubectl run ... --dry-run=client -o yaml</code> to generate YAML fast.</div>
      </div>
      <p>The smallest deployable object in Kubernetes. A Pod determines how to run a container.
        <br><em>Key Concept:</em> Containers in a pod share the <strong>same network namespace</strong> (IP address),
        storage volumes, and process namespace (sometimes).
        <br><em>Life:</em> Pods are <strong>ephemeral</strong>. They are born, they run, and they die. They are not
        resurrected; a new one is created to replace them.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-pod-lifecycle.png" alt="Pod lifecycle states: Pending, Running, Succeeded, Failed, Unknown" width="480" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption>Pod lifecycle: <strong>Pending</strong> (waiting for scheduling) &rarr; <strong>Running</strong> (containers running) &rarr; <strong>Succeeded</strong> / <strong>Failed</strong> / <strong>Unknown</strong>.</figcaption>
      </figure>

      <h4>Pods with YAML</h4>
      <p>Define pods in YAML with <code>apiVersion</code>, <code>kind: Pod</code>, <code>metadata</code>, and <code>spec.containers</code>. Use <code>kubectl apply -f pod.yaml</code> or generate with <code>kubectl run ... --dry-run=client -o yaml</code>.</p>
      <div class="snippet-label">Run a pod (imperative)</div>
      <pre><code>kubectl run nginx --image nginx
kubectl get pods -wide</code></pre>

      <h4>ReplicaSet</h4>
      <p>Ensures a specified number of pod replicas are running at any given time.
        <br><em>Selector:</em> Uses <code>selector</code> to match Pod labels. If a pod dies, RS starts a new one.
      </p>

      <h4>Deployments</h4>
      <div class="cka-definition">
        <div class="cka-def-term">Deployment</div>
        <div class="cka-def-body">Manages ReplicaSets for stateless apps. Declarative updates: rolling update, rollback. CKA: <code>kubectl rollout status/history/undo</code>, <code>kubectl set image</code>, <code>kubectl scale</code>. Deployment creates ReplicaSet which creates Pods.</div>
      </div>
      <p>The standard way to manage stateless applications. A Deployment manages ReplicaSets and provides declarative
        updates (Rolling Updates, Rollbacks).
        <br><em>Capabilities:</em> Scaling, pausing/resuming updates, rolling back to previous revisions.
      </p>

      <div class="snippet-label">Create Deployment</div>
      <pre><code>kubectl create deployment webapp --image=nginx --replicas=3
kubectl get deploy,rs,po</code></pre>

      <h4>Scaling Applications</h4>
      <p>Scale: <code>kubectl scale --replicas=5 deployment/webapp</code> or edit via
        <code>kubectl edit deploy webapp</code>. HPA for auto-scaling (see Application Lifecycle).
      </p>
      <figure class="diagram">
        <img src="assets/k8s-deployment-replicaset-pod.png" alt="Workloads hierarchy: Deployment â†’ ReplicaSet â†’ Pod" width="400" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Workloads hierarchy:</strong> Deployment manages ReplicaSets; ReplicaSet maintains pod count via selector; Pod runs containers.</figcaption>
      </figure>

      <h3>Kubectl & Configuration</h3>
      <h4>Imperative vs Declarative</h4>
      <p>Imperative: <code>kubectl run</code>, <code>kubectl create</code> â€” direct commands. Declarative: <code>kubectl apply -f file.yaml</code> â€” desired state in YAML. Exam tip: use imperative with <code>--dry-run=client -o yaml</code> to generate, then edit.</p>
      <h4>Kubectl Explain</h4>
      <p>Documentation in terminal: <code>kubectl explain pod</code>, <code>kubectl explain pod.spec.containers</code>. Use <code>--recursive</code> for full structure.</p>
      <h4>Kubectl Apply</h4>
      <p>Declarative updates: <code>kubectl apply -f file.yaml</code>. Creates or updates resources. Use <code>-f -</code> to read from stdin.</p>
      <h4>Imperative commands â€” exam speed reference</h4>
      <p>In the CKA exam, speed matters. Use imperative commands with <code>--dry-run=client -o yaml</code> to
        generate
        YAML quickly, then edit as needed.</p>
      <div class="snippet-label">Generate YAML templates fast</div>
      <pre><code># Pod
kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml

# Deployment
kubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml > deploy.yaml

# Service (ClusterIP)
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

# Service (NodePort)
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

# Job
kubectl create job my-job --image=busybox --dry-run=client -o yaml -- echo hello

# CronJob
kubectl create cronjob my-cron --image=busybox --schedule="*/5 * * * *" --dry-run=client -o yaml -- echo hi

# ConfigMap
kubectl create configmap app-config --from-literal=KEY=VALUE --dry-run=client -o yaml

# Secret
kubectl create secret generic app-secret --from-literal=PASSWORD=pass --dry-run=client -o yaml

# ServiceAccount
kubectl create serviceaccount my-sa --dry-run=client -o yaml

# Namespace
kubectl create namespace dev

# Role & RoleBinding
kubectl create role pod-reader --verb=get,list --resource=pods -n dev
kubectl create rolebinding dev-read --role=pod-reader --user=jane -n dev

# ClusterRole & ClusterRoleBinding
kubectl create clusterrole node-reader --verb=get,list --resource=nodes
kubectl create clusterrolebinding node-read --clusterrole=node-reader --user=jane

# Ingress
kubectl create ingress my-ingress --rule="host/path=svc:80" --dry-run=client -o yaml</code></pre>
      <div class="snippet-label">Quick edit and apply pattern</div>
      <pre><code># Generate â†’ Edit â†’ Apply
kubectl run mypod --image=nginx --dry-run=client -o yaml > mypod.yaml
vi mypod.yaml   # add labels, resources, volumes, etc.
kubectl apply -f mypod.yaml

# Edit running resource
kubectl edit deployment nginx

# Replace (force update)
kubectl replace --force -f pod.yaml</code></pre>

      <h3>Namespaces</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Namespace</div>
        <div class="cka-def-body">Virtual cluster within a physical cluster. Isolates resources; most objects are namespaced. System namespaces: <code>kube-system</code>, <code>kube-public</code>, <code>kube-node-lease</code>. CKA: <code>-n</code>/<code>--namespace</code>, <code>--all-namespaces</code>, <code>kubectl config set-context --current --namespace=dev</code>.</div>
      </div>
      <p>Objects live in a namespace (default is <code>default</code>). List pods in a namespace:
        <code>kubectl get pods --namespace=kube-system</code> or <code>-n kube-system</code>. Create in a namespace:
        <code>kubectl create -f pod-definition.yaml --namespace=dev</code>. Add <code>namespace: dev</code> under
        <code>metadata</code> in the YAML to fix the namespace.
      </p>
      <div class="snippet-label">Create namespace</div>
      <pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev</code></pre>
      <pre><code>kubectl create -f namespace-dev.yaml
# or
kubectl create namespace dev</code></pre>
      <p>Switch default namespace:
        <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>. All namespaces:
        <code>kubectl get pods --all-namespaces</code>.
      </p>
      <div class="snippet-label">ResourceQuota (limit resources in a namespace)</div>
      <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi</code></pre>
      <pre><code>kubectl create -f compute-quota.yaml</code></pre>

      <h3>Services</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Service</div>
        <div class="cka-def-body">Abstracts a set of Pods behind a stable network endpoint. Types: ClusterIP (internal VIP, default), NodePort (node IP + port 30000â€“32767), LoadBalancer (cloud LB). Selector matches Pod labels to form Endpoints.</div>
      </div>
      <h4>Services & Networking Basics</h4>
      <p>Three types: <strong>NodePort</strong> (expose on node port), <strong>ClusterIP</strong> (virtual IP inside
        cluster), <strong>LoadBalancer</strong> (cloud load balancer).</p>
      <h4>ClusterIP</h4>
      <p>Default type. Internal VIP; only reachable from within the cluster. Use for backend services.</p>
      <h4>LoadBalancer</h4>
      <p>Cloud-provider LB; creates external IP. Requires cloud integration (e.g. AWS ELB, GCP LB).</p>
      <div class="snippet-label">NodePort service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  selector:
    app: myapp
    type: front-end</code></pre>
      <div class="snippet-label">ClusterIP service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: back-end</code></pre>
      <pre><code>kubectl create -f service-definition.yaml
kubectl get services
curl http://&lt;node-ip&gt;:30008   # for NodePort</code></pre>
      <figure class="diagram">
        <img src="assets/k8s-service-types.png" alt="Kubernetes Service Types: NodePort, ClusterIP, LoadBalancer" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption>Service types: NodePort (expose on node port), ClusterIP (internal VIP), LoadBalancer (cloud LB).
        </figcaption>
      </figure>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 02 Core Concepts</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 02 Core Concepts" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 03 Scheduling -->
      <h2 id="section-03">03 &mdash; Scheduling</h2>
      <h3>How Scheduling Works</h3>
      <p>The <strong>kube-scheduler</strong> is a control-plane component that continuously watches the API server for
        newly created Pods that have no <code>spec.nodeName</code> set. When it finds one, it runs through a
        multi-phase pipeline to decide which node is the best fit:</p>
      <ul>
        <li><strong>1. Filtering (Predicates):</strong> Eliminates nodes that <em>cannot</em> run the pod. Common
          reasons: insufficient CPU/memory, unmatched taints, anti-affinity conflicts, unresolvable volume topology,
          node is cordoned. After filtering you get a shortlist of <em>feasible</em> nodes.</li>
        <li><strong>2. Scoring (Priorities):</strong> Ranks each feasible node 0&ndash;100 using weighted plugins:
          <code>LeastAllocated</code> (prefer emptier nodes), <code>ImageLocality</code> (prefer nodes that already
          have the image), <code>NodeAffinity</code>, <code>InterPodAffinity</code>,
          <code>PodTopologySpread</code>. The node with the highest total score wins.
        </li>
        <li><strong>3. Binding:</strong> The scheduler creates a <strong>Binding</strong> object that writes
          <code>spec.nodeName</code> on the pod. The kubelet on that node then pulls the images and starts the
          containers.
        </li>
      </ul>
      <p><strong>Key detail:</strong> If <em>no</em> feasible nodes remain after filtering, the pod stays in
        <code>Pending</code> state with a <code>FailedScheduling</code> event. Use
        <code>kubectl describe pod &lt;name&gt;</code> to check events.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-scheduling-flow.png" alt="Pod scheduling flow: Pod created â†’ Scheduler â†’ Binding â†’ Kubelet" width="560" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>Scheduling flow: Filter + Score &rarr; Binding object &rarr; kubelet starts the pod.</figcaption>
      </figure>
      <div class="snippet-label">Debug scheduling decisions</div>
      <pre><code># Why is my pod pending? Check events
kubectl describe pod my-pending-pod | grep -A 10 Events

# See scheduler logs for detailed scoring
kubectl logs -n kube-system kube-scheduler-controlplane

# Check which scheduler placed a pod
kubectl get events --field-selector reason=Scheduled,involvedObject.name=my-pod</code></pre>

      <h3>Manual Scheduling (nodeName)</h3>
      <p>Pin a pod to a specific node by setting <code>spec.nodeName</code>. This <strong>completely bypasses the
          scheduler</strong> &mdash; no filtering, no scoring, no admission of scheduling plugins.
        <br><strong>Important rules:</strong>
      </p>
      <ul>
        <li><code>nodeName</code> can only be set at <strong>creation time</strong>. You cannot change it on a running
          pod &mdash; you must delete and recreate.</li>
        <li>The node must exist; if it doesn&rsquo;t, the pod is rejected immediately.</li>
        <li>Taints on the target node are <strong>still respected</strong> &mdash; the kubelet will reject the pod if
          tolerations don&rsquo;t match.</li>
        <li>In the CKA exam, the &ldquo;force reschedule&rdquo; pattern is:
          <code>kubectl get pod X -o yaml &gt; pod.yaml</code> &rarr; edit &rarr; delete old &rarr; recreate.
        </li>
      </ul>
      <div class="snippet-label">Pod with hardcoded nodeName</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  nodeName: node02        # bypasses scheduler</code></pre>
      <div class="snippet-label">Binding object (alternative &mdash; POST to API)</div>
      <pre><code>apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02</code></pre>
      <div class="snippet-label">Force-move a running pod to a different node</div>
      <pre><code># Step 1 - Export the running pod YAML
kubectl get pod nginx -o yaml &gt; nginx-moved.yaml

# Step 2 - Edit: change spec.nodeName to the target node, remove status/uid/resourceVersion
vi nginx-moved.yaml

# Step 3 - Delete the original and create the new one
kubectl delete pod nginx
kubectl apply -f nginx-moved.yaml

# Step 4 - Verify placement
kubectl get pod nginx -o wide</code></pre>

      <h3>Labels and Selectors</h3>
      <p>Labels are <strong>key-value pairs</strong> attached to Kubernetes objects. They are the primary mechanism for
        organizing and selecting groups of objects. Selectors filter objects by label and are critical for how
        ReplicaSets, Services, Deployments, and NetworkPolicies discover their target objects.</p>
      <p><strong>Two types of selectors:</strong></p>
      <ul>
        <li><strong>Equality-based:</strong> <code>=</code>, <code>==</code>, <code>!=</code> &mdash; used in
          <code>nodeSelector</code> and <code>kubectl --selector</code>.
        </li>
        <li><strong>Set-based:</strong> <code>In</code>, <code>NotIn</code>, <code>Exists</code>,
          <code>DoesNotExist</code> &mdash; used in <code>matchExpressions</code> (Deployments, Node Affinity).
        </li>
      </ul>
      <div class="snippet-label">Pod with labels</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    env: production
    tier: frontend
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080</code></pre>
      <div class="snippet-label">Deployment using matchLabels selector</div>
      <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
spec:
  replicas: 3
  selector:
    matchLabels:            # these labels MUST match template.metadata.labels
      app: App1
      tier: frontend
  template:
    metadata:
      labels:
        app: App1
        tier: frontend
    spec:
      containers:
      - name: webapp
        image: simple-webapp</code></pre>
      <pre><code># Filter by single label
kubectl get pods --selector app=App1

# Filter by multiple labels (AND logic)
kubectl get pods --selector app=App1,env=production

# All resource types matching a label
kubectl get all --selector env=production

# Count pods matching a selector
kubectl get pods --selector env=production --no-headers | wc -l

# Add/change a label on a running pod
kubectl label pod nginx tier=frontend
kubectl label pod nginx tier=backend --overwrite

# Remove a label (trailing minus)
kubectl label pod nginx tier-

# Show labels as columns
kubectl get pods --show-labels
kubectl get pods -L app,env    # show specific labels as columns</code></pre>
      <p><strong>Annotations</strong> (under <code>metadata.annotations</code>) store non-identifying metadata like
        build version, last-applied-config, contact info &mdash; they are <strong>NOT</strong> used for selection.
        Common annotations include <code>kubernetes.io/change-cause</code> (rollout history) and
        <code>kubectl.kubernetes.io/last-applied-configuration</code>.
      </p>

      <h3>Taints and Tolerations</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Taints & Tolerations</div>
        <div class="cka-def-body"><strong>Taints</strong> on nodes repel pods; <strong>tolerations</strong> on pods allow them onto tainted nodes. Effects: NoSchedule (block new), PreferNoSchedule (soft avoid), NoExecute (block + evict existing). CKA: <code>kubectl taint nodes node01 key=value:NoSchedule</code>, remove with <code>key=value:NoSchedule-</code>.</div>
      </div>
      <p>Taints on <strong>nodes</strong> repel pods. Tolerations on <strong>pods</strong> allow them onto tainted
        nodes. Think of taints as a &ldquo;keep out&rdquo; sign on the node and tolerations as a &ldquo;VIP
        pass&rdquo; on the pod.</p>
      <p><em>Critical: Tolerations do NOT guarantee the pod will land on that node &mdash; they only
          &ldquo;allow&rdquo; it. Use Node Affinity to <strong>attract</strong> pods to specific nodes.</em></p>
      <figure class="diagram">
        <img src="assets/k8s-taints-tolerations.png" alt="Taints repel pods from nodes; Tolerations allow pods onto tainted nodes" width="500" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Taints and Tolerations:</strong> Node taints &ldquo;keep out&rdquo; pods; pod tolerations act as a &ldquo;VIP pass&rdquo; to land on tainted nodes.</figcaption>
      </figure>
      <ul>
        <li><strong>NoSchedule:</strong> New pods without a matching toleration are <em>never</em> scheduled here.
          Existing pods stay untouched.</li>
        <li><strong>PreferNoSchedule:</strong> Scheduler <em>tries</em> to avoid this node, but if no other nodes are
          available the pod can still land here. Soft constraint.</li>
        <li><strong>NoExecute:</strong> New pods blocked <strong>AND</strong> existing pods without the toleration are
          <strong>evicted immediately</strong>. You can add <code>tolerationSeconds</code> to allow a grace period
          before eviction.
        </li>
      </ul>
      <pre><code># Apply a taint to a node
kubectl taint nodes node01 app=blue:NoSchedule

# Apply multiple taints
kubectl taint nodes node01 env=staging:NoSchedule
kubectl taint nodes node01 dedicated=gpu:NoExecute

# Check taints on a node
kubectl describe node node01 | grep -i taint

# Remove a specific taint (trailing minus removes it)
kubectl taint nodes node01 app=blue:NoSchedule-

# Remove ALL taints with a specific key (regardless of effect)
kubectl taint nodes node01 app-</code></pre>
      <div class="snippet-label">Pod with toleration (Equal operator)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "app"
    operator: "Equal"     # matches only when key=value
    value: "blue"
    effect: "NoSchedule"</code></pre>
      <div class="snippet-label">Pod with toleration (Exists operator &mdash; matches any value)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: tolerant-pod
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  # Tolerate any taint with key "dedicated", regardless of value
  - key: "dedicated"
    operator: "Exists"
    effect: "NoSchedule"
  # Tolerate NoExecute with a grace period (stay 300s then evict)
  - key: "node.kubernetes.io/not-ready"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 300</code></pre>
      <p><strong>Control-plane taint:</strong> Master/control plane nodes automatically receive
        <code>node-role.kubernetes.io/control-plane:NoSchedule</code> so user workloads never land there.
        To allow scheduling on control plane (single-node cluster):
      </p>
      <pre><code># Remove the control-plane taint (single-node clusters)
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-</code></pre>

      <h3>Node Selectors</h3>
      <p>The simplest form of node selection constraint. Label a node, then reference those labels with
        <code>nodeSelector</code> in the pod spec. The scheduler only considers nodes that match
        <strong>all</strong> the specified labels.
      </p>
      <p><strong>Limitation:</strong> Supports only equality matching &mdash; you cannot say &ldquo;Large OR
        Medium&rdquo; or &ldquo;NOT Small&rdquo;. For complex expressions, use Node Affinity.</p>
      <pre><code># Label a node
kubectl label nodes node01 size=Large

# Label multiple nodes at once
kubectl label nodes node01 node02 disktype=ssd

# Verify the label
kubectl get nodes --show-labels | grep size

# Check labels on a specific node
kubectl describe node node01 | grep -A 5 Labels

# Remove a label
kubectl label nodes node01 size-</code></pre>
      <div class="snippet-label">Pod with nodeSelector</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: data-processor
spec:
  containers:
  - name: data-processor
    image: data-processor
  nodeSelector:
    size: Large       # only scheduled on nodes with this label</code></pre>
      <div class="snippet-label">Deployment with nodeSelector (common exam pattern)</div>
      <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gpu-app
  template:
    metadata:
      labels:
        app: gpu-app
    spec:
      containers:
      - name: cuda
        image: nvidia/cuda:latest
      nodeSelector:
        accelerator: nvidia-gpu   # all replicas land on GPU nodes</code></pre>

      <h3>Node Affinity</h3>
      <p>More expressive than <code>nodeSelector</code>. Supports <code>In</code>, <code>NotIn</code>,
        <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code>, <code>Lt</code> operators and both
        <strong>hard</strong> and <strong>soft</strong> rules.
      </p>
      <ul>
        <li><strong>requiredDuringSchedulingIgnoredDuringExecution:</strong> <em>Hard rule.</em> Pod stays
          <code>Pending</code> forever if no node matches. Running pods are NOT evicted if node labels change
          later ("IgnoredDuringExecution").
        </li>
        <li><strong>preferredDuringSchedulingIgnoredDuringExecution:</strong> <em>Soft rule.</em> Scheduler tries to
          match but places pod elsewhere if no nodes qualify. Use <code>weight</code> (1&ndash;100) to
          rank preferences.</li>
      </ul>
      <div class="snippet-label">Node Affinity &mdash; In operator (node must have size=Large OR Medium)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: data-processor
spec:
  containers:
  - name: data-processor
    image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50               # 1-100; higher = more preferred
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd</code></pre>
      <div class="snippet-label">Node Affinity &mdash; NotIn operator (avoid Small nodes)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: no-small-nodes
spec:
  containers:
  - name: app
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: NotIn               # NOT Small
            values:
            - Small</code></pre>
      <div class="snippet-label">Node Affinity &mdash; Exists operator (node has the key, any value)</div>
      <pre><code>  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu                      # any node that has a "gpu" label
            operator: Exists              # value doesn't matter</code></pre>
      <p><strong>Combining Taints + Node Affinity (Dedicated Nodes Pattern):</strong></p>
      <ul class="step-list">
        <li><strong>Step 1 &mdash; Taint the node:</strong>
          <code>kubectl taint nodes node01 dedicated=team-a:NoSchedule</code>
        </li>
        <li><strong>Step 2 &mdash; Label the node:</strong>
          <code>kubectl label nodes node01 dedicated=team-a</code>
        </li>
        <li><strong>Step 3 &mdash; Pod spec:</strong> Add both a toleration for the taint AND a node affinity rule
          for the label. This ensures <em>only</em> your pods can run there, and they always prefer that node.</li>
      </ul>
      <h4>Taints vs Affinity</h4>
      <p>Taints <em>repel</em> pods (unless tolerated); Affinity <em>attracts</em> pods to nodes. Use both for dedicated nodes: taint to block others, affinity to pull your workload.</p>
      <figure class="diagram">
        <img src="assets/k8s-taints-vs-affinity.png" alt="Taints repel pods; Node Affinity attracts pods" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Taints vs Affinity:</strong> Taints = keep out (unless tolerated). Node Affinity = prefer this node.</figcaption>
      </figure>

      <h3>Resource Limits</h3>
      <h4>Editing Pods & Deployments</h4>
      <p>Use <code>kubectl edit deployment &lt;name&gt;</code> or <code>kubectl edit pod &lt;name&gt;</code>. For immutable pod fields, delete and recreate. Deployments support in-place edits (triggers rollout).</p>
      <p><strong>Requests</strong> &mdash; the <em>guaranteed</em> allocation. The scheduler uses requests to decide
        which node has enough capacity.<br>
        <strong>Limits</strong> &mdash; the <em>maximum</em> a container can consume at runtime.
      </p>
      <p><strong>CPU vs Memory behaviour:</strong></p>
      <ul>
        <li>CPU exceeds limit &rarr; <strong>throttled</strong> (never killed for CPU). Container gets fewer cycles.
        </li>
        <li>Memory exceeds limit &rarr; container is <strong>OOMKilled</strong> (exit code 137) and restarted per
          <code>restartPolicy</code>.
        </li>
        <li>No requests set &rarr; scheduler treats it as 0 (pod placed anywhere &mdash; risky for stability).</li>
      </ul>
      <p><strong>CPU units:</strong> <code>1</code> = 1 vCPU/core. <code>500m</code> = 0.5 CPU = 500 millicores.
        Minimum is <code>1m</code>.<br>
        <strong>Memory units:</strong> <code>Mi</code> (mebibytes), <code>Gi</code> (gibibytes). Use power-of-2
        units, not <code>M</code>/<code>G</code> (decimal).
      </p>
      <div class="snippet-label">Pod with resource requests and limits</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "64Mi"        # scheduler looks for node with &ge;64Mi free
        cpu: "250m"           # 0.25 vCPU (millicores)
      limits:
        memory: "128Mi"       # OOMKilled if exceeded
        cpu: "500m"           # throttled if exceeded</code></pre>
      <p><strong>QoS Classes</strong> (determined automatically by Kubernetes):</p>
      <ul>
        <li><strong>Guaranteed:</strong> Every container has requests == limits for both CPU and memory. Highest
          priority &mdash; last to be evicted.</li>
        <li><strong>Burstable:</strong> At least one container has requests &lt; limits. Middle priority.</li>
        <li><strong>BestEffort:</strong> No requests or limits set at all. First to be evicted under pressure.</li>
      </ul>
      <pre><code># Check QoS class of a pod
kubectl get pod resource-demo -o jsonpath='{.status.qosClass}'

# Check resource usage on nodes
kubectl top nodes
kubectl top pods --sort-by=memory

# Describe node to see resource allocation
kubectl describe node node01 | grep -A 10 "Allocated resources"</code></pre>

      <h3>LimitRange</h3>
      <p>A <strong>namespace-scoped</strong> policy that automatically injects default resource requests/limits into
        containers that don&rsquo;t specify them, and enforces min/max boundaries.</p>
      <p><strong>Key behaviour:</strong></p>
      <ul>
        <li>Applied at <strong>admission time</strong> (when pod is created) &mdash; existing pods are NOT retroactively
          changed.</li>
        <li>If a container specifies only <code>limits</code> but not <code>requests</code>, requests default to the
          limit value (not the <code>defaultRequest</code>).</li>
        <li>If a container specifies resources that exceed <code>max</code> or fall below <code>min</code>, the pod
          creation is <strong>rejected</strong>.</li>
        <li>Can also be applied to Pods (total resources) and PersistentVolumeClaims (storage size).</li>
      </ul>
      <div class="snippet-label">LimitRange for containers</div>
      <pre><code>apiVersion: v1
kind: LimitRange
metadata:
  name: mem-cpu-limits
  namespace: dev
spec:
  limits:
  - type: Container
    default:              # used when container omits limits
      cpu: "500m"
      memory: "128Mi"
    defaultRequest:       # used when container omits requests
      cpu: "250m"
      memory: "64Mi"
    max:                  # container cannot request/limit above this
      cpu: "2"
      memory: "1Gi"
    min:                  # container must request/limit at least this
      cpu: "100m"
      memory: "16Mi"</code></pre>
      <pre><code>kubectl apply -f limitrange.yaml
kubectl describe limitrange mem-cpu-limits -n dev

# Test: create a pod without resources and check what defaults are applied
kubectl run test-lr --image=nginx -n dev
kubectl get pod test-lr -n dev -o jsonpath='{.spec.containers[0].resources}'</code></pre>

      <h3>ResourceQuota</h3>
      <p>A <strong>namespace-scoped</strong> policy that limits the <em>total</em> amount of resources (CPU, memory,
        storage, object count) that can be consumed in a namespace. Unlike LimitRange (per-container), ResourceQuota
        constrains the entire namespace.</p>
      <div class="snippet-label">ResourceQuota YAML</div>
      <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "4"          # total CPU requests across all pods
    requests.memory: "4Gi"     # total memory requests
    limits.cpu: "8"            # total CPU limits
    limits.memory: "8Gi"       # total memory limits
    pods: "10"                 # max 10 pods in this namespace
    services: "5"
    persistentvolumeclaims: "4"</code></pre>
      <pre><code># Apply and check quota usage
kubectl apply -f resourcequota.yaml
kubectl get resourcequota compute-quota -n dev
kubectl describe resourcequota compute-quota -n dev

# When quota is active, every pod MUST specify requests/limits
# or use a LimitRange to set defaults â€” otherwise creation fails</code></pre>


      <h3>DaemonSets</h3>
      <div class="cka-definition">
        <div class="cka-def-term">DaemonSet</div>
        <div class="cka-def-body">Ensures one pod per node (or subset via nodeSelector). Used for log collectors, node monitoring, CNI, CSI node plugin. Auto-deploys to new nodes. CKA: <code>kubectl get ds</code>, tolerations for control-plane nodes, <code>nodeSelector</code> for GPU-only nodes.</div>
      </div>
      <p>Ensures <strong>exactly one copy of a Pod</strong> runs on every node (or a subset of nodes selected via
        <code>nodeSelector</code> or node affinity). When a new node is added to the cluster, the DaemonSet
        controller automatically creates a pod on it. When a node is removed, the pod is garbage collected.
      </p>
      <p><strong>Common use cases:</strong></p>
      <ul>
        <li><strong>Log collectors:</strong> Fluentd, Filebeat, Fluent Bit</li>
        <li><strong>Node monitoring:</strong> Prometheus node-exporter, Datadog agent</li>
        <li><strong>Networking:</strong> kube-proxy, CNI plugins (Calico, Flannel, Weave)</li>
        <li><strong>Storage:</strong> CSI node drivers (like EBS CSI, Ceph CSI)</li>
      </ul>
      <p><strong>How it works internally:</strong> The DaemonSet controller uses the default scheduler (since K8s
        1.12+).
        It sets <code>nodeAffinity</code> on each pod to target a specific node, so the scheduler places it correctly.
        This is why DaemonSet pods show the <code>default-scheduler</code> in events.</p>
      <div class="snippet-label">DaemonSet YAML</div>
      <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: monitoring-daemon       # must match template labels
  template:
    metadata:
      labels:
        app: monitoring-daemon     # must match selector
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule         # also run on master nodes
      containers:
      - name: monitoring-agent
        image: prom/node-exporter
        ports:
        - containerPort: 9100
        resources:
          limits:
            memory: "200Mi"
            cpu: "100m"</code></pre>
      <div class="snippet-label">DaemonSet on specific nodes only (nodeSelector)</div>
      <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-monitor
spec:
  selector:
    matchLabels:
      app: gpu-monitor
  template:
    metadata:
      labels:
        app: gpu-monitor
    spec:
      nodeSelector:
        accelerator: nvidia-gpu   # only nodes with GPU label
      containers:
      - name: dcgm-exporter
        image: nvcr.io/nvidia/k8s/dcgm-exporter:latest</code></pre>
      <pre><code># Create and manage DaemonSets
kubectl apply -f daemonset.yaml
kubectl get daemonsets -n kube-system
kubectl get ds -A                           # shorthand, all namespaces
kubectl describe daemonset monitoring-daemon -n kube-system

# Check which nodes have the DaemonSet pod
kubectl get pods -l app=monitoring-daemon -o wide

# Rollback a DaemonSet update
kubectl rollout undo daemonset monitoring-daemon -n kube-system</code></pre>

      <h3>Static Pods</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Static Pod</div>
        <div class="cka-def-body">Managed by kubelet from a manifest directory (<code>/etc/kubernetes/manifests/</code>). Not created by API server; appear as mirror pods. Control-plane components run as static pods via kubeadm. To delete: remove manifest file. CKA: check <code>staticPodPath</code> in kubelet config.</div>
      </div>
      <p>Pods managed <strong>directly by the kubelet</strong> on a specific node, without going through the API
        server or any controller. The kubelet watches a local manifest directory and automatically creates/deletes
        pods based on the YAML files it finds there.</p>
      <p><strong>Key characteristics:</strong></p>
      <ul>
        <li>Default path: <code>/etc/kubernetes/manifests/</code></li>
        <li>The API server shows them as read-only <strong>mirror pods</strong> (you can see them with
          <code>kubectl get pods</code> but cannot control them through the API).
        </li>
        <li>Control-plane components (etcd, kube-apiserver, controller-manager, scheduler) are all deployed as
          static pods on the control plane node via kubeadm.</li>
        <li>To delete: <strong>remove the manifest file</strong> from the directory &mdash;
          <code>kubectl delete pod</code> will NOT work because the kubelet recreates the mirror pod immediately.
        </li>
        <li><strong>Naming convention:</strong> Static pods have the node name appended, e.g.,
          <code>kube-apiserver-controlplane</code>, <code>etcd-controlplane</code>.
        </li>
      </ul>
      <div class="snippet-label">Configure staticPodPath in kubelet</div>
      <pre><code># /var/lib/kubelet/config.yaml
staticPodPath: /etc/kubernetes/manifests

# OR as a kubelet start flag:
--pod-manifest-path=/etc/kubernetes/manifests

# Apply changes
systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code></pre>
      <div class="snippet-label">Example static pod manifest (drop into manifests dir)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: static-nginx
  namespace: kube-system
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80</code></pre>
      <div class="snippet-label">Exam techniques for static pods</div>
      <pre><code># Static pods have the node name appended: kube-apiserver-controlplane
kubectl get pods -n kube-system

# Confirm it's static (ownerReferences.kind == Node)
kubectl get pod kube-apiserver-controlplane -n kube-system \
  -o jsonpath='{.metadata.ownerReferences[*].kind}'

# Locate the staticPodPath on a node
ps aux | grep kubelet | grep config
# then read the config file
cat /var/lib/kubelet/config.yaml | grep staticPodPath

# Create a static pod on a worker node via SSH
ssh node01
cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["sleep", "3600"]
EOF

# Delete a static pod: remove the file
rm /etc/kubernetes/manifests/static-busybox.yaml</code></pre>
      <p><strong>Static Pods vs DaemonSets:</strong> Both run one pod per node, but static pods are managed by the
        kubelet (no controller) while DaemonSets are managed by the DaemonSet controller via the API server.
        Static pods cannot be managed with <code>kubectl</code>.</p>

      <h3>Node Maintenance &mdash; Cordon, Drain, Uncordon</h3>
      <p>Safely move workloads off a node before maintenance (OS upgrade, kernel patch, reboot, hardware replacement).
        This is a <strong>very common CKA exam scenario</strong>.</p>
      <ul>
        <li><strong>cordon:</strong> Marks node <code>SchedulingDisabled</code>. No new pods placed; existing pods
          <strong>keep running</strong> undisturbed.
        </li>
        <li><strong>drain:</strong> Cordon + graceful eviction of all pods. Respects <code>PodDisruptionBudgets</code>.
          DaemonSet pods are ignored by default (<code>--ignore-daemonsets</code> flag needed to proceed).
          Pods with <code>emptyDir</code> volumes need <code>--delete-emptydir-data</code>.</li>
        <li><strong>uncordon:</strong> Re-enables scheduling on the node after maintenance. Note: previously evicted
          pods do <em>not</em> automatically come back &mdash; only new pods can be scheduled here.</li>
      </ul>
      <pre><code># Mark as unschedulable only (pods keep running)
kubectl cordon node01

# Evict pods and mark unschedulable
kubectl drain node01 --ignore-daemonsets --delete-emptydir-data

# Force drain when pods have no controller (standalone pods)
kubectl drain node01 --ignore-daemonsets --delete-emptydir-data --force

# Re-enable scheduling after work is done
kubectl uncordon node01

# Check node status and conditions
kubectl get nodes
kubectl describe node node01 | grep -E "Taints|Unschedulable"

# Typical maintenance workflow:
# 1. Drain the node (moves pods to other nodes)
# 2. Perform maintenance (upgrade OS, reboot, etc.)
# 3. Uncordon the node
# 4. Verify: kubectl get nodes</code></pre>
      <div class="snippet-label">PodDisruptionBudget (controls voluntary evictions)</div>
      <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 2            # OR use maxUnavailable: 1
  selector:
    matchLabels:
      app: webapp</code></pre>
      <pre><code># Check PDBs
kubectl get pdb
kubectl describe pdb webapp-pdb</code></pre>

      <h3>Pod Priority and Preemption</h3>
      <p>When the cluster is full and a new pod cannot be scheduled, the scheduler checks if the pod has a higher
        <strong>priority</strong> than any currently running pods. If so, it can <strong>preempt</strong> (evict)
        lower-priority pods to make room.
      </p>
      <p><strong>How preemption works:</strong></p>
      <ul>
        <li>The scheduler identifies the lowest-priority pods on feasible nodes.</li>
        <li>Those pods receive a graceful termination signal (respecting <code>terminationGracePeriodSeconds</code>).
        </li>
        <li>Once evicted, the high-priority pod is scheduled onto the freed node.</li>
        <li>If <code>preemptionPolicy: Never</code>, the pod waits in the queue without evicting anyone.</li>
      </ul>
      <p><strong>Built-in PriorityClasses:</strong> <code>system-cluster-critical</code> (2 billion) and
        <code>system-node-critical</code> (2 billion + 1000) are reserved for control-plane components. You should
        never use values that high for user workloads.
      </p>
      <div class="snippet-label">PriorityClass definition</div>
      <pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000              # higher = more important (system max ~2 billion)
globalDefault: false        # if true, applies to pods without a class
description: "Critical production workloads"
preemptionPolicy: PreemptLowerPriority   # or Never (no eviction)</code></pre>
      <div class="snippet-label">Low-priority class (non-preempting &mdash; waits instead of evicting)</div>
      <pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-low
value: 100
preemptionPolicy: Never     # will NOT evict other pods; just waits
description: "Non-critical batch jobs"</code></pre>
      <div class="snippet-label">Pod using PriorityClass</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: nginx</code></pre>
      <pre><code># List all priority classes (including built-in ones)
kubectl get priorityclasses

# Check which priority class a pod uses
kubectl get pod critical-app -o jsonpath='{.spec.priorityClassName}'

# See preemption events
kubectl get events --field-selector reason=Preempted</code></pre>

      <h3>Multiple Schedulers</h3>
      <p>Kubernetes supports running <strong>multiple schedulers simultaneously</strong>. You deploy a custom scheduler
        as a Pod or Deployment alongside the default <code>default-scheduler</code>. Each scheduler has a unique
        <code>schedulerName</code>. Pods opt into a specific scheduler using <code>spec.schedulerName</code>.
      </p>
      <p><strong>When to use:</strong> When you need different scheduling logic for different workloads (e.g., GPU-aware
        scheduling, custom bin-packing, rack-aware placement).</p>
      <div class="snippet-label">Custom scheduler configuration (ConfigMap)</div>
      <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-config
  namespace: kube-system
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    leaderElection:
      leaderElect: false          # set true if running multiple replicas
    profiles:
    - schedulerName: my-custom-scheduler</code></pre>
      <div class="snippet-label">Custom scheduler Pod manifest</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler        # needs RBAC to watch/bind pods
  containers:
  - name: kube-scheduler
    image: registry.k8s.io/kube-scheduler:v1.29.0
    command:
    - kube-scheduler
    - --config=/etc/kubernetes/my-scheduler-config.yaml
    volumeMounts:
    - name: config
      mountPath: /etc/kubernetes
  volumes:
  - name: config
    configMap:
      name: my-scheduler-config</code></pre>
      <div class="snippet-label">Pod assigned to custom scheduler</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: custom-scheduled-pod
spec:
  schedulerName: my-custom-scheduler    # must match name in scheduler config
  containers:
  - name: nginx
    image: nginx</code></pre>
      <pre><code># Verify which scheduler placed this pod
kubectl get events --field-selector reason=Scheduled
kubectl describe pod custom-scheduled-pod | grep "Node:"

# If pod stays Pending, check the custom scheduler is running
kubectl get pods -n kube-system | grep scheduler
kubectl logs my-custom-scheduler -n kube-system

# List all pods and their scheduler
kubectl get pods -o custom-columns="NAME:.metadata.name,SCHEDULER:.spec.schedulerName"</code></pre>

      <h3>Scheduler Profiles</h3>
      <p>From Kubernetes 1.18+ you can run <strong>multiple scheduling profiles in a single scheduler binary</strong>
        instead of deploying separate scheduler pods. Each profile has its own <code>schedulerName</code> and can
        customize which <strong>plugins</strong> are enabled/disabled at each scheduling phase.</p>
      <p><strong>Scheduling extension points (plugin phases):</strong></p>
      <ul>
        <li><strong>QueueSort:</strong> Determines order in the scheduling queue.</li>
        <li><strong>PreFilter / Filter:</strong> Eliminates ineligible nodes (e.g., NodeResources, TaintToleration,
          NodeAffinity).</li>
        <li><strong>PreScore / Score:</strong> Ranks feasible nodes (e.g., LeastAllocated, ImageLocality,
          PodTopologySpread).</li>
        <li><strong>Reserve / Permit / PreBind / Bind / PostBind:</strong> Handle the binding lifecycle.</li>
      </ul>
      <div class="snippet-label">KubeSchedulerConfiguration with two profiles</div>
      <pre><code>apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler
  plugins:
    score:
      disabled:
      - name: PodTopologySpread    # disable a score plugin for this profile
- schedulerName: high-throughput-scheduler
  plugins:
    filter:
      disabled:
      - name: TaintToleration      # relax taint check for this profile
    score:
      enabled:
      - name: ImageLocality        # prefer nodes with cached images
        weight: 2</code></pre>
      <div class="snippet-label">Pod using a specific profile</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: throughput-pod
spec:
  schedulerName: high-throughput-scheduler   # matches profile name
  containers:
  - name: app
    image: nginx</code></pre>

      <h3>Admission Controllers (2025)</h3>
      <h4>Validating Admission Controllers</h4>
      <p>Validate requests; accept or reject. Examples: PodSecurity, ResourceQuota, LimitRanger.</p>
      <h4>Mutating Admission Controllers</h4>
      <p>Modify objects before persistence. Examples: DefaultStorageClass, ServiceAccount injection.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Admission Controller</div>
        <div class="cka-def-body">Intercepts requests to the API server after authentication/authorization, before persistence. <strong>Validating:</strong> accept or reject (e.g., PodSecurity, ResourceQuota). <strong>Mutating:</strong> modify objects (e.g., DefaultStorageClass, ServiceAccount). CKA: know they run in-order; use <code>kube-apiserver --enable-admission-plugins</code>.</div>
      </div>
      <p>Admission controllers enforce cluster policies. <strong>Validating</strong> controllers only validate (e.g., <code>PodSecurity</code>, <code>LimitRanger</code>). <strong>Mutating</strong> controllers can modify objects (e.g., <code>DefaultStorageClass</code>, inject default values). Configure via <code>--enable-admission-plugins</code> and <code>--disable-admission-plugins</code> on kube-apiserver.      </p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 03 Scheduling</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 03 Scheduling" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 04 Logging & Monitoring -->
      <h2 id="section-04">04 â€” Logging and Monitoring</h2>
      <p>Logging and monitoring are essential for running and troubleshooting workloads. The CKA exam expects you to know how to <strong>retrieve application logs</strong>, interpret <strong>cluster and node metrics</strong>, and understand how the <strong>Metrics Server</strong> enables <code>kubectl top</code> and autoscaling. You do not need to install or configure Prometheus/Grafana for the exam; focus on built-in tools.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Logging & Monitoring (CKA)</div>
        <div class="cka-def-body">Application logs via <code>kubectl logs</code> (stdout/stderr). Use <code>--previous</code> for crashed containers. Cluster metrics: <strong>Metrics Server</strong> powers <code>kubectl top nodes/pods</code>. HPA requires Metrics Server for CPU-based scaling.</div>
      </div>
      <h3>Monitor Cluster Components</h3>
      <p>Before debugging application issues, confirm the control plane and nodes are healthy. Run <code>kubectl get pods -n kube-system</code> to see API server, scheduler, controller-manager, etcd, and (if present) CoreDNS and CNI pods. Use <code>kubectl top nodes</code> to see CPU and memory usage per node â€” this requires the Metrics Server to be installed. If <code>kubectl top</code> returns "Metrics API not available", the Metrics Server is either not deployed or not ready. Verify scheduler and API server are running; many exam scenarios assume a working control plane.</p>
      <h3>Managing Application Logs</h3>
      <p>Containers write to stdout and stderr; the <strong>kubelet</strong> on each node captures these streams and exposes them through the Kubernetes API. You use <code>kubectl logs &lt;pod-name&gt;</code> to read them. For a <strong>multi-container</strong> pod you must specify the container with <code>-c &lt;container-name&gt;</code>, otherwise kubectl picks the first container.</p>
      <p>When a container restarts (e.g. after a crash), the previous instance's logs are still available for a short time. Use <code>kubectl logs &lt;pod-name&gt; --previous</code> to see what the crashed container wrote before it exited. This is one of the most common exam patterns for diagnosing CrashLoopBackOff.</p>
      <pre><code>kubectl logs -f &lt;pod-name&gt;
kubectl logs -f &lt;pod-name&gt; &lt;container-name&gt;
kubectl logs &lt;pod-name&gt; --previous
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>
      <p><code>-f</code> streams logs (like <code>tail -f</code>); <code>--previous</code> shows logs from the <strong>previous</strong> container instance (e.g. after a crash or restart). Combine both when debugging a pod that keeps restarting.</p>
      <h3>Cluster metrics</h3>
      <p>The <strong>Metrics Server</strong> is a cluster add-on that scrapes resource usage (CPU and memory) from the kubelet on each node and exposes them via the Kubernetes Metrics API. That API powers <code>kubectl top nodes</code> and <code>kubectl top pods</code>. Without the Metrics Server, these commands fail. The Metrics Server is <strong>not</strong> for long-term storage or alerting; it keeps only a short window of data. For historical metrics and dashboards you would use a full monitoring stack (e.g. Prometheus, Grafana). The legacy Heapster project is deprecated. For the CKA, know that HPA (Horizontal Pod Autoscaler) based on CPU or memory also depends on the Metrics Server being installed and working.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 04 Logging &amp; Monitoring</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 04 Logging and Monitoring" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 05 Application Lifecycle -->
      <h2 id="section-05">05 â€” Application Lifecycle Management</h2>
      <h3>Deployment Strategies</h3>
      <p><strong>RollingUpdate</strong> (default): Gradually replace old pods. <strong>Recreate</strong>: Kill all, then create new. Configure via <code>spec.strategy</code>.</p>
      <h3>Rolling Updates</h3>
      <p>Gradually replace old ReplicaSet pods with new ones. Configure <code>maxSurge</code> and <code>maxUnavailable</code>.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-rolling-update.png" alt="Rolling update: gradually replace old ReplicaSet with new" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Rolling update:</strong> Deployment gradually replaces old ReplicaSet pods with new version pods, maintaining availability.</figcaption>
      </figure>
      <pre><code>kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
kubectl apply -f deployment-definition.yaml
kubectl rollout undo deployment/myapp-deployment</code></pre>
      <h3>Rollbacks</h3>
      <p>If a rollout introduces a bad change, you can roll back to the previous revision with <code>kubectl rollout undo deployment/&lt;name&gt;</code>. Kubernetes keeps a history of ReplicaSets (revisions); <code>kubectl rollout history deployment/&lt;name&gt;</code> shows them. To roll back to a specific revision use <code>kubectl rollout undo deployment/&lt;name&gt; --to-revision=2</code>. Rollback is another rolling update, so availability is preserved.</p>
      <h3>Scaling</h3>
      <p>Change the number of replicas with <code>kubectl scale deployment/&lt;name&gt; --replicas=N</code>. Alternatively edit <code>spec.replicas</code> in the Deployment YAML and run <code>kubectl apply -f deployment.yaml</code>. For automatic scaling based on CPU or memory, use a HorizontalPodAutoscaler (HPA); the Metrics Server must be installed for CPU/memory metrics.</p>

      <h3>Application Configuration</h3>
      <h4>ConfigMaps</h4>
      <div class="cka-definition">
        <div class="cka-def-term">ConfigMap</div>
        <div class="cka-def-body">Stores non-sensitive config as key-value pairs. Inject via <code>env</code>, <code>envFrom</code>, or volume mount. CKA: <code>kubectl create configmap x --from-literal=K=V</code>, <code>--from-file</code>, <code>configMapKeyRef</code>/<code>configMapRef</code> in pod spec.</div>
      </div>
      <p>ConfigMaps decouple configuration from container images. Create one from literal key-value pairs: <code>kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod</code>, or from a file: <code>--from-file=app_config.properties</code> (key becomes the filename). You can also define them in YAML and <code>kubectl apply -f</code>. Pods consume ConfigMaps via <code>env</code>, <code>envFrom</code>, or <code>volumes</code> + <code>volumeMounts</code>.</p>
      <div class="snippet-label">ConfigMap YAML</div>
      <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod</code></pre>
      <pre><code>kubectl create -f config-map.yaml
kubectl get configmaps
kubectl get cm
kubectl describe configmaps app-config</code></pre>
      <div class="snippet-label">Inject ConfigMap into pod (envFrom)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config</code></pre>

      <h3>Secrets</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Secret</div>
        <div class="cka-def-body">Stores sensitive data (base64-encoded in YAML). Types: generic, tls, docker-registry. Inject via <code>secretKeyRef</code>/<code>secretRef</code>. CKA: <code>kubectl create secret generic x --from-literal=K=V</code>, <code>secret tls</code> for Ingress. Enable encryption at rest for prod.</div>
      </div>
      <p>Secrets hold sensitive data such as passwords, tokens, or TLS certs. In YAML they are stored base64-encoded (not encrypted by default). Create with <code>kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd</code>. To embed in YAML, encode values: <code>echo -n "paswrd" | base64</code>. Pods reference secrets the same way as ConfigMaps: <code>envFrom: - secretRef: name: app-secret</code> or <code>valueFrom.secretKeyRef</code> for a single key. For production, enable encryption at rest via the API server.</p>
      <pre><code>apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk</code></pre>
      <pre><code>kubectl create -f secret-data.yaml
kubectl get secrets
kubectl describe secret app-secret
kubectl get secret app-secret -o yaml</code></pre>
      <p>Decode: <code>echo -n "bX1zcWw=" | base64 --decode</code>. Inject into pod with
        <code>envFrom: - secretRef: name: app-secret</code> (same structure as configMapRef).
      </p>
      <h4>Encrypting Secrets at Rest</h4>
      <p>Enable encryption in <code>kube-apiserver</code> via <code>--encryption-provider-config</code>. Define which resources (e.g. secrets) are encrypted. CKA: know the concept; config is in a YAML file.</p>

      <h4>Commands & Arguments (Docker)</h4>
      <p>Docker: <code>ENTRYPOINT</code> + <code>CMD</code>. Kubernetes <code>command</code> overrides ENTRYPOINT, <code>args</code> overrides CMD.</p>
      <p>Override image <strong>CMD</strong> with <code>command</code> (args to entrypoint) and
        <strong>ENTRYPOINT</strong> with <code>args</code> in the container spec. In Kubernetes, <code>command</code>
        corresponds to Docker ENTRYPOINT and <code>args</code> to CMD. Example: <code>command: ["sleep"]</code>,
        <code>args: ["3600"]</code>.
      </p>
      <div class="snippet-label">Docker â†’ Kubernetes mapping</div>
      <pre><code># Dockerfile          â†’ Pod spec
# ENTRYPOINT ["python"] â†’ command: ["python"]
# CMD ["app.py"]       â†’ args: ["app.py"]

apiVersion: v1
kind: Pod
metadata:
  name: command-demo
spec:
  containers:
  - name: demo
    image: ubuntu
    command: ["sleep"]    # overrides ENTRYPOINT
    args: ["5000"]        # overrides CMD</code></pre>
      <h3>Environment Variables</h3>
      <p>Set env vars directly, from ConfigMaps, or from Secrets. Three injection methods:</p>
      <pre><code>containers:
- name: app
  image: myapp
  env:
  # 1. Plain key-value
  - name: APP_COLOR
    value: pink
  # 2. From ConfigMap key
  - name: APP_ENV
    valueFrom:
      configMapKeyRef:
        name: app-config
        key: APP_ENV
  # 3. From Secret key
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password</code></pre>

      <h3>Pod Design</h3>
      <h3>Multi-Container Pods</h3>
      <h4>Multi-Container Design Patterns</h4>
      <p>CKA tests multi-container pods. Three main patterns:</p>
      <figure class="diagram">
        <img src="assets/k8s-multi-container-patterns.png" alt="Multi-container patterns: Sidecar, Ambassador, Adapter" width="540" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Multi-container patterns:</strong> Sidecar (auxiliary), Ambassador (proxy to external services), Adapter (transforms output).</figcaption>
      </figure>
      <ul>
        <li><strong>Sidecar:</strong> auxiliary container that enhances the main container (e.g. log shipper that
          reads
          shared volume, Istio proxy).</li>
        <li><strong>Ambassador:</strong> proxy container that handles connections to external services on behalf of
          the
          main container (e.g. localhost proxy to reach different DB environments).</li>
        <li><strong>Adapter:</strong> transforms data from the main container before exporting it (e.g. log format
          normaliser, metrics adapter).</li>
      </ul>
      <div class="snippet-label">Sidecar example (log streamer)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'while true; do echo "$(date) INFO app running" >> /var/log/app.log; sleep 5; done']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  - name: sidecar
    image: busybox
    command: ['sh', '-c', 'tail -f /var/log/app.log']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  volumes:
  - name: log-volume
    emptyDir: {}</code></pre>
      <h3>Init containers</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Init Container</div>
        <div class="cka-def-body">Runs to completion before main containers start. Runs sequentially; if one fails, pod restarts. Use for setup (clone repo, wait for DB). CKA: <code>initContainers</code> in pod spec, ordered execution, shared volumes with main containers.</div>
      </div>
      <p>Run to completion before main containers start. Use for one-time setup or waiting for dependencies. If an
        init
        container fails, the pod is restarted (all init containers re-run). Order: initContainers run sequentially;
        then
        main containers start.</p>
      <figure class="diagram">
        <img src="assets/k8s-init-containers.png" alt="Init containers run sequentially before main containers" width="500" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Init containers:</strong> Run sequentially to completion; only then do main containers start. Failure restarts the pod.</figcaption>
      </figure>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']</code></pre>

      <h3>Self-Healing</h3>
      <p>Liveness and readiness probes make deployments self-healing. Liveness restarts unhealthy containers; readiness removes from Service endpoints until ready.</p>
      <h4>Liveness and readiness probes</h4>
      <p>Probes make deployments <strong>self-healing</strong>. <strong>livenessProbe</strong>: if it fails, the
        container is restarted. <strong>readinessProbe</strong>: if it fails, the pod is removed from Service
        endpoints
        (no traffic until ready). Types: <code>httpGet</code> (HTTP path), <code>exec</code> (command),
        <code>tcpSocket</code> (port open).
      </p>
      <figure class="diagram">
        <img src="assets/k8s-liveness-readiness.png" alt="Liveness vs Readiness probes" width="500" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Probes:</strong> Liveness fails &rarr; restart container. Readiness fails &rarr; remove from Service endpoints (no traffic).</figcaption>
      </figure>
      <div class="snippet-label">Probes example</div>
      <pre><code>containers:
- name: app
  image: myapp:1.0
  livenessProbe:
    httpGet:
      path: /healthz
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 20
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10</code></pre>

      <h3>Autoscaling (2025)</h3>
      <h4>Introduction to Autoscaling</h4>
      <p>HPA scales replicas by CPU/custom metrics. VPA adjusts resource requests. Manual scaling via <code>kubectl scale</code>.</p>
      <h4>Manual Scaling</h4>
      <p><code>kubectl scale deployment/&lt;name&gt; --replicas=N</code>.</p>
      <h4>In-Place Resize</h4>
      <p>Kubernetes 1.27+: resize CPU/memory without restart. Update <code>resources</code> in pod spec; kubelet applies if supported.</p>
      <h4>Vertical Pod Autoscaler (VPA)</h4>
      <p>Recommends or auto-updates resource requests/limits based on usage. Install VPA controller; create VPA resource targeting deployment.</p>
      <figure class="diagram">
        <img src="assets/k8s-hpa-flow.png" alt="HPA flow: Metrics Server to HPA to Deployment scaling" width="560" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>HPA flow:</strong> Metrics Server collects CPU â†’ HPA compares to target â†’ scales Deployment replicas.</figcaption>
      </figure>
      <h3>HorizontalPodAutoscaler (HPA)</h3>
      <div class="cka-definition">
        <div class="cka-def-term">HPA (Horizontal Pod Autoscaler)</div>
        <div class="cka-def-body">Scales Deployment/ReplicaSet based on CPU or custom metrics. Requires Metrics Server. Set <code>resources.requests</code> on deployment. CKA: <code>kubectl autoscale deploy x --min=2 --max=10 --cpu-percent=80</code>, <code>kubectl get hpa</code>. VPA (Vertical Pod Autoscaler) adjusts CPU/memory requests.</div>
      </div>
      <p><strong>Workload autoscaling</strong>: HPA scales a Deployment/ReplicaSet based on CPU (or custom metrics).
        Requires <strong>Metrics Server</strong> in the cluster for <code>kubectl top</code> and CPU-based scaling.
        Set
        <code>resources.requests</code> on the deployment so HPA can compute utilization.
      </p>
      <div class="snippet-label">HPA YAML</div>
      <pre><code>apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80</code></pre>
      <pre><code>kubectl autoscale deployment myapp-deployment --min=2 --max=10 --cpu-percent=80
kubectl get hpa
kubectl describe hpa myapp-hpa</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 05 Application Lifecycle</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 05 Application Lifecycle" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 06 Cluster Maintenance -->
      <h2 id="section-06">06 â€” Cluster Maintenance</h2>
      <p>Cluster maintenance covers <strong>OS upgrades</strong> on nodes, <strong>Kubernetes version upgrades</strong> (control plane and workers), <strong>backup and restore</strong> of etcd and resource configs, and <strong>node lifecycle</strong> (cordon, drain, uncordon). You must know the order of operations and how to avoid downtime.</p>

      <h3>OS upgrades and node maintenance</h3>
      <p><strong>Important:</strong> If a node is down for <strong>more than 5 minutes</strong>, the control plane may
        terminate pods that were on that node (they will be recreated elsewhere if managed by a controller).</p>
      <ul>
        <li><strong>kubectl cordon &lt;node&gt;</strong>: Marks the node <strong>unschedulable</strong> only. No new pods are placed; existing pods keep running. Use when you want to stop new workload without moving current pods.</li>
        <li><strong>kubectl drain &lt;node&gt;</strong>: Same as cordon, plus it <strong>evicts</strong> all pods from the node (they are rescheduled elsewhere). Use before OS upgrade, kernel patch, or hardware maintenance. Always use <code>--ignore-daemonsets</code> (DaemonSet pods stay on the node) and <code>--delete-emptydir-data</code> (pods with emptyDir can be evicted).</li>
        <li><strong>kubectl uncordon &lt;node&gt;</strong>: Marks the node schedulable again after maintenance. Does not bring back evicted pods; only allows new pods to be placed on the node.</li>
      </ul>
      <figure class="diagram">
        <img src="assets/k8s-cordon-drain-uncordon.png" alt="Node maintenance: cordon, drain, uncordon" width="420" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption>cordon = no new pods; drain = cordon + evict; uncordon = allow scheduling again.</figcaption>
      </figure>
      <div class="snippet-label">Typical node maintenance workflow</div>
      <pre><code># 1. Drain (evict pods, mark unschedulable)
kubectl drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data

# 2. Perform OS upgrade, reboot, or hardware work on the node

# 3. Bring node back into rotation
kubectl uncordon &lt;node&gt;

# 4. Verify
kubectl get nodes</code></pre>

      <h3>Kubernetes versions and upgrade policy</h3>
      <p>Kubernetes supports the <strong>last 3 minor versions</strong>. Upgrade <strong>one minor version at a time</strong> (e.g. 1.27 â†’ 1.28 â†’ 1.29). During an upgrade, kubeadm, kubelet, and control-plane components can be at different versions temporarily; align them to the target version when done.</p>
      <p><strong>Upgrade order:</strong> Control plane (master) first, then worker nodes. On each control-plane node: upgrade kubeadm â†’ <code>kubeadm upgrade plan</code> â†’ <code>kubeadm upgrade apply</code> â†’ upgrade kubelet and kubectl â†’ restart kubelet.</p>

      <h3>Cluster upgrade (kubeadm)</h3>
      <p><strong>Control plane (master):</strong></p>
      <pre><code>kubeadm upgrade plan
apt-get upgrade -y kubeadm=1.28.0-00
kubeadm upgrade apply v1.28.0
apt-get upgrade -y kubelet=1.28.0-00 kubectl=1.28.0-00
systemctl restart kubelet
kubectl get nodes</code></pre>
      <p><strong>Worker nodes:</strong> Drain the node, upgrade kubeadm and kubelet, update the node config, restart kubelet, then uncordon.</p>
      <pre><code>kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data
apt-get upgrade -y kubeadm=1.28.0-00 kubelet=1.28.0-00
kubeadm upgrade node config --kubelet-version v1.28.0
systemctl restart kubelet
kubectl uncordon node-1</code></pre>

      <h3>Backup and restore</h3>
      <p><strong>Resource configs:</strong> Store declarative YAML in Git, or export with <code>kubectl get all --all-namespaces -o yaml &gt; all-resources.yaml</code> (partial; use for reference). Tools like <strong>Velero</strong> can backup and restore resources and volumes.</p>
      <p><strong>ETCD snapshot (required for CKA):</strong> Always backup etcd before upgrading the control plane. etcd holds cluster state (resources, configs). Without a snapshot, a failed upgrade can leave the cluster unrecoverable.</p>
      <pre><code># Save snapshot (API v3)
ETCDCTL_API=3 etcdctl snapshot save snapshot.db

# Check snapshot integrity
ETCDCTL_API=3 etcdctl snapshot status snapshot.db</code></pre>
      <p>With kubeadm, etcd runs as a static pod. Use the correct endpoint and certificates (paths may differ; adjust for your setup):</p>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
      <p><strong>Restore:</strong> Restore to a new data directory, then point etcd to it and restart control-plane components. Example:</p>
      <pre><code>etcdutl snapshot restore snapshot.db --data-dir=/var/lib/etcd-restore</code></pre>
      <p>After restore, update etcd manifest to use <code>--data-dir=/var/lib/etcd-restore</code>, then restart the kubelet so the etcd pod picks up the new data-dir.</p>

      <h3>PodDisruptionBudget (PDB)</h3>
      <p>PDB limits <strong>voluntary</strong> disruptions (e.g. <code>kubectl drain</code>, node upgrades) so that a minimum number of pods stay available. The drain command respects PDBs and may block or delay eviction until the constraint can be satisfied.</p>
      <ul>
        <li><strong>minAvailable:</strong> At least N pods (number or percentage) must remain available.</li>
        <li><strong>maxUnavailable:</strong> At most N pods (number or percentage) can be down at once.</li>
      </ul>
      <div class="snippet-label">PDB example (minAvailable)</div>
      <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp</code></pre>
      <div class="snippet-label">PDB with maxUnavailable</div>
      <pre><code>spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: myapp</code></pre>
      <pre><code>kubectl get pdb
kubectl describe pdb myapp-pdb</code></pre>

      <h3>Certification tips</h3>
      <p>Set aliases at exam start (<code>alias k=kubectl</code>). Use imperative + dry-run for YAML. Verify answers with <code>kubectl get</code> / <code>describe</code>. Don't get stuck â€” flag and move on.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 06 Cluster Maintenance</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 06 Cluster Maintenance" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 07 Security -->
      <h2 id="section-07">07 â€” Security</h2>
      <p>Kubernetes security centers on <strong>authentication</strong> (who you are) and <strong>authorization</strong> (what you can do). The API server enforces both; it does not store user identities itself but relies on certificates, tokens, or external identity providers. For the CKA you must know <strong>RBAC</strong> (Roles, RoleBindings, ClusterRoles, ClusterRoleBindings), <strong>ServiceAccounts</strong>, <strong>TLS and certificates</strong> (including the Certificates API and kubeadm cert layout), and how to check permissions with <code>kubectl auth can-i</code>.</p>
      <div class="cka-definition">
        <div class="cka-def-term">RBAC</div>
        <div class="cka-def-body">Role-Based Access Control. Defines who can do what via Roles/ClusterRoles (permissions) and RoleBindings/ClusterRoleBindings (who gets them). CKA: create Role (namespace-scoped), ClusterRole (cluster-wide), bind to User/Group/ServiceAccount. <code>kubectl auth can-i --as=user get pods</code>.</div>
      </div>

      <h3>Security Fundamentals</h3>
      <h3>Kubernetes Security Primitives</h3>
      <p>Security starts with the <strong>API server</strong> â€” it is the gatekeeper. Two key questions: <strong>who
          can
          access?</strong> (authentication) and <strong>what can they do?</strong> (authorization). All access goes
        through the API server, making it the single chokepoint for security.</p>

      <h3>Authentication</h3>
      <h3>Authorization</h3>
      <p>Kubernetes does <strong>not</strong> manage user accounts natively (no User object). It relies on external
        mechanisms. Two types of accounts:</p>
      <ul>
        <li><strong>Users</strong> (humans): admins, developers â€” managed externally (certificates, LDAP, OIDC
          tokens).
        </li>
        <li><strong>Service Accounts</strong> (machines): used by pods and processes â€” managed by Kubernetes.</li>
      </ul>
      <p>Authentication methods: <strong>static password/token files</strong> (deprecated),
        <strong>certificates</strong> (most common with kubeadm), <strong>OIDC</strong>, <strong>webhook</strong>. All
        requests are authenticated by the API server before authorization.
      </p>

      <h3>Service Accounts</h3>
      <p>ServiceAccounts are namespace-scoped. Every namespace has a <code>default</code> SA. Pods automatically mount
        the SA token. Use custom SAs to give pods specific RBAC permissions.</p>
      <pre><code># Create
kubectl create serviceaccount dashboard-sa
kubectl get serviceaccount

# Use in pod
apiVersion: v1
kind: Pod
metadata:
  name: my-dashboard
spec:
  serviceAccountName: dashboard-sa
  automountServiceAccountToken: false   # opt-out of auto-mount
  containers:
  - name: dashboard
    image: my-dashboard:v1</code></pre>
      <p>Since Kubernetes 1.24, ServiceAccount tokens are no longer auto-created as Secrets. Use
        <code>kubectl create token &lt;sa-name&gt;</code> for short-lived tokens, or create a Secret with annotation
        <code>kubernetes.io/service-account.name</code>.
      </p>

      <h3>TLS &amp; Certificates</h3>

      <p>Every connection to the Kubernetes API server (and between control-plane components) is protected by <strong>TLS</strong>. This section explains what TLS and certificates are, why Kubernetes uses them, and how to work with them for the CKA exam â€” even if you have never used them before.</p>

      <figure class="diagram">
        <img src="assets/tls-encryption-authentication.png" alt="TLS: Encryption and Authentication" width="960" height="280" style="max-width:100%;height:auto;display:block;" loading="lazy">
        <figcaption><strong>TLS in one picture:</strong> Encryption protects data; authentication ensures you are talking to the right server and that the client is who it claims to be. Certificates are used for both.</figcaption>
      </figure>

      <h4>What is TLS?</h4>
      <p><strong>TLS (Transport Layer Security)</strong> does two things over the network:</p>
      <ul>
        <li><strong>Encryption:</strong> Data between the client and server is encrypted so anyone intercepting traffic cannot read it.</li>
        <li><strong>Authentication:</strong> Each side can prove who it is. The server proves it really is the API server; the client (e.g. kubelet) proves it is a legitimate component â€” not an imposter.</li>
      </ul>
      <p>Without TLS, someone on the network could read or alter cluster traffic and pretend to be the API server or a node.</p>

      <h4>What is a certificate?</h4>
      <p>A <strong>certificate</strong> is a digital document that says: <em>â€œThis public key belongs to this identity (e.g. api-server or kubelet).â€</em> It is signed by a <strong>Certificate Authority (CA)</strong>. If you trust the CA, you can trust any certificate it has signed.</p>
      <ul>
        <li><strong>Server certificate:</strong> Presented by the server (e.g. API server) when a client connects. The client checks that the cert is signed by a CA it trusts and that the name in the cert matches the server it contacted.</li>
        <li><strong>Client certificate:</strong> Presented by the client (e.g. kubelet, scheduler) to prove its identity to the server. The server checks that the cert is signed by the cluster CA.</li>
      </ul>
      <p>Each certificate has a <strong>private key</strong> (kept secret) and a <strong>public key</strong> (in the certificate). Only the holder of the private key can prove they own that certificate.</p>

      <figure class="diagram">
        <img src="assets/tls-k8s-cert-chain.png" alt="Kubernetes TLS certificate chain: CA, API server, and clients" width="1120" height="520" style="max-width:100%;height:auto;display:block;" loading="lazy">
        <figcaption><strong>Certificate chain in Kubernetes:</strong> The cluster CA is the root of trust. The API server has a <em>server</em> certificate (so clients know they are talking to the real API server). Components like the kubelet and scheduler have <em>client</em> certificates (so the API server knows they are legitimate). All certificates are signed by the same CA.</figcaption>
      </figure>

      <h4>Why does Kubernetes use TLS?</h4>
      <p>The API server is the gateway to the cluster. If someone could impersonate it or read traffic, they could control or steal data. TLS ensures that:</p>
      <ul>
        <li>Only clients with a valid <strong>client certificate</strong> (or other allowed auth) can call the API server.</li>
        <li>Clients can verify they are talking to the real API server using its <strong>server certificate</strong>.</li>
        <li>Traffic between the API server and etcd, and between the API server and kubelets, is encrypted.</li>
      </ul>
      <p>With <strong>kubeadm</strong>, all certificates are created automatically and stored under <code>/etc/kubernetes/pki/</code>. You need to know how to <em>inspect</em> them (for troubleshooting) and how to <em>create</em> new ones for users (via the Certificates API).</p>

      <h4>TLS in Kubernetes</h4>
      <p>Cluster components use <strong>TLS</strong>: servers use <strong>server certificates</strong>, and clients use <strong>client certificates</strong> for authentication. The API server, etcd, kubelet, and other control-plane components each have their own certs (with kubeadm, under <code>/etc/kubernetes/pki/</code>). The Kubernetes <strong>Certificates API</strong> can issue new client certificates for users; the controller manager signs them using the cluster CA.</p>
      <h4>Key certificate files (kubeadm)</h4>
      <ul>
        <li><code>ca.crt / ca.key</code> â€” Cluster CA (root of trust). Used to sign all other certs.</li>
        <li><code>apiserver.crt / apiserver.key</code> â€” API server TLS (server cert). Presented to clients when they connect.</li>
        <li><code>apiserver-kubelet-client.crt</code> â€” Client cert used by the API server when it talks to the kubelet.</li>
        <li><code>apiserver-etcd-client.crt</code> â€” Client cert used by the API server when it talks to etcd.</li>
        <li><code>etcd/ca.crt</code>, <code>etcd/server.crt</code> â€” etcdâ€™s own CA and server cert (etcd can use a separate CA).</li>
        <li><code>front-proxy-ca.crt</code> â€” Used for the aggregation layer (extension API servers).</li>
      </ul>

      <h4>Certificate inspection (troubleshooting)</h4>
      <p>To check a certificateâ€™s subject, issuer, and <strong>validity dates</strong> (expiry is a common cause of failures):</p>
      <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout</code></pre>
      <p>Look for <strong>Not Before</strong> / <strong>Not After</strong>, <strong>Subject</strong>, <strong>Issuer</strong>, and <strong>Subject Alternative Name (SAN)</strong>. Use the same command for other certs (e.g. <code>apiserver-kubelet-client.crt</code>, <code>ca.crt</code>). If <strong>Not After</strong> has passed, the cert has expired and must be renewed (kubeadm can do this).</p>

      <h4>Certificate creation for users (Certificates API)</h4>
      <p>To give a new user (or service) access to the cluster, you can issue a <strong>client certificate</strong> signed by the cluster CA. Steps:</p>
      <ol>
        <li>User generates a private key and a <strong>Certificate Signing Request (CSR)</strong> with OpenSSL.</li>
        <li>Admin creates a Kubernetes <strong>CertificateSigningRequest</strong> object with the CSR (base64-encoded).</li>
        <li>Admin approves the CSR. The controller manager signs it and puts the signed certificate in the CSR objectâ€™s status.</li>
        <li>User extracts the certificate and uses it (with the private key) in kubeconfig.</li>
      </ol>
      <div class="snippet-label">Create and approve a user certificate</div>
      <pre><code># 1. User generates key and CSR
openssl genrsa -out jane.key 2048
openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr

# 2. Admin creates CSR object (base64-encode the CSR)
cat jane.csr | base64 | tr -d '\n'</code></pre>
      <pre><code># 3. CSR YAML
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: jane
spec:
  request: &lt;base64-encoded-csr&gt;
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth</code></pre>
      <pre><code># 4. Approve and extract certificate
kubectl create -f jane-csr.yaml
kubectl get csr
kubectl certificate approve jane
kubectl get csr jane -o jsonpath='{.status.certificate}' | base64 --decode > jane.crt

# 5. Deny a CSR
kubectl certificate deny bad-user</code></pre>
      <h3>kubeconfig</h3>
      <p>kubeconfig holds clusters (API server URL, CA), users (credentials: cert/key or token), and
        <strong>contexts</strong> (which cluster + which user + optional namespace). Default file:
        <code>~/.kube/config</code>.
      </p>
      <pre><code>kubectl config view
kubectl config view --kubeconfig=my-custom-config
kubectl config use-context &lt;context-name&gt;
kubectl config current-context
kubectl get pods --kubeconfig config</code></pre>
      <p>To set default namespace for current context:
        <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>.
      </p>
      <h3>API Groups</h3>
      <p>Kubernetes API is organized into groups so related resources are versioned and managed together. The
        <strong>core group</strong> is special and uses just <code>v1</code> (for example:
        <code>Pod</code>, <code>Service</code>, <code>ConfigMap</code>), while named groups use
        <code>&lt;group&gt;/&lt;version&gt;</code> (for example: <code>apps/v1</code>,
        <code>networking.k8s.io/v1</code>).</p>
      <pre><code># Core group (no group prefix)
apiVersion: v1
kind: Pod

# apps group
apiVersion: apps/v1
kind: Deployment

# networking group
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy</code></pre>
      <pre><code># Discover groups and resources
kubectl api-versions
kubectl api-resources</code></pre>
      <h3>KubeConfig</h3>
      <p><code>kubeconfig</code> stores <strong>clusters</strong> (API server endpoints),
        <strong>users</strong> (credentials), and <strong>contexts</strong> (cluster + user + namespace mapping).
        Default file is <code>~/.kube/config</code>. In CKA, switch context at the start of each question to avoid
        working in the wrong cluster.</p>
      <figure class="diagram">
        <img src="assets/k8s-request-workflow.png" alt="kubectl request workflow using kubeconfig context and credentials" width="1024" height="576" style="max-width:100%;height:auto;display:block;">
        <figcaption>KubeConfig picks cluster/user/context so <code>kubectl</code> can authenticate to the correct API server.</figcaption>
      </figure>
      <pre><code># List and verify contexts
kubectl config get-contexts
kubectl config current-context

# Switch context for a question
kubectl config use-context ckad-cluster-1

# Optional: set namespace for current context
kubectl config set-context --current --namespace=dev</code></pre>
      <h3>Access Control</h3>
      <p>Access control is mainly: <strong>Authentication</strong> (who you are) and
        <strong>Authorization</strong> (what you are allowed to do). Kubernetes commonly uses
        <strong>RBAC</strong> for authorization.</p>
      <h3>Role-Based Access Control (RBAC)</h3>
      <p>RBAC objects: <code>Role</code>/<code>ClusterRole</code> define permissions;
        <code>RoleBinding</code>/<code>ClusterRoleBinding</code> attach those permissions to a
        <code>User</code>, <code>Group</code>, or <code>ServiceAccount</code>.</p>
      <figure class="diagram">
        <img src="assets/k8s-rbac-flow.png" alt="RBAC flow from Role to RoleBinding to user, group, or service account" width="400" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>RBAC flow: define permissions in Role/ClusterRole, then grant via Binding.</figcaption>
      </figure>
      <pre><code># Quick permission checks
kubectl auth can-i list pods -n dev --as system:serviceaccount:dev:app-sa
kubectl auth can-i get nodes --as jane</code></pre>
      <h4>Roles</h4>
      <p>Role: apiGroups, resources, verbs. RoleBinding: links subject (user/group/serviceaccount) to Role. Both are
        namespace-scoped.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "update", "delete", "create"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f developer-role.yaml
kubectl create -f devuser-developer-binding.yaml
kubectl get roles
kubectl get rolebindings
kubectl describe role developer
kubectl describe rolebinding devuser-developer-binding
kubectl auth can-i create pods --as=dev-user -n default</code></pre>

      <h4>RoleBindings</h4>
      <p>RoleBinding links a Role to a subject (user, group, ServiceAccount). Scope: same namespace as RoleBinding.</p>
      <h4>ClusterRoles</h4>
      <p>ClusterRole can define access to cluster-scoped resources (e.g. nodes) or to namespaced resources across all
        namespaces.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f cluster-admin-role.yaml
kubectl create -f cluster-admin-role-binding.yaml
kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false</code></pre>

      <h3>Network Security</h3>
      <h4>Network Policies</h4>
      <div class="cka-definition">
        <div class="cka-def-term">NetworkPolicy</div>
        <div class="cka-def-body">Controls ingress/egress to pods by selector. Requires CNI with policy support (Calico, Cilium). Default: allow all. Once a policy selects a pod, traffic not explicitly allowed is denied. CKA: <code>podSelector</code>, <code>policyTypes</code> (Ingress, Egress), <code>from</code>/<code>to</code> (podSelector, namespaceSelector, ipBlock).</div>
      </div>
      <p>Controls ingress/egress to pods by selector. Default: no restrictions. Once a NetworkPolicy selects a pod,
        that
        pod denies traffic not allowed by any policy.</p>
      <figure class="diagram">
        <img src="assets/k8s-pod-networking-cni.png" alt="Pod networking and policy enforcement with CNI across nodes" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption>NetworkPolicy enforcement depends on a CNI plugin that supports policy (for example, Calico or Cilium).</figcaption>
      </figure>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: api-pod
    ports:
    - protocol: TCP
      port: 3306</code></pre>
      <pre><code>kubectl create -f policy-definition.yaml</code></pre>
      <h4>Developing Network Policies</h4>
      <p>Use an iterative approach: start with a deny-all baseline, then add the minimum allow rules your app needs.</p>
      <figure class="diagram">
        <img src="assets/k8s-pod-networking-cni.png" alt="Network policy development: start deny-all and progressively allow required pod-to-pod paths" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption>Build policies incrementally: deny by default, then allow only required pod and namespace traffic.</figcaption>
      </figure>
      <pre><code># 1) Default deny in namespace (both directions)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress</code></pre>
      <pre><code># 2) Allow only API pods to reach DB pods on 3306
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-db
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: api
    ports:
    - protocol: TCP
      port: 3306</code></pre>
      <pre><code># 3) Cross-namespace access with namespaceSelector
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        name: monitoring</code></pre>
      <pre><code># 4) External CIDR access with ipBlock
ingress:
- from:
  - ipBlock:
      cidr: 203.0.113.0/24
      except:
      - 203.0.113.128/25</code></pre>
      <pre><code># 5) Validate with temporary debug pods
kubectl run test-a -n default --rm -it --image=busybox:1.36 -- sh
kubectl run test-b -n monitoring --rm -it --image=busybox:1.36 -- sh

# From inside pod
nc -zv db 3306
wget -qO- http://web.default.svc.cluster.local:80</code></pre>
      <p><strong>Selector usage:</strong> <code>podSelector</code> selects pods, <code>namespaceSelector</code> selects namespaces, and <code>ipBlock</code> matches IP CIDRs. In ingress rules use <code>from</code>; in egress rules use <code>to</code>.</p>
      <h3>Image Security</h3>
      <p>Default image registry is <strong>docker.io</strong>. Image <code>nginx</code> is really
        <code>docker.io/library/nginx</code>. For private registries, create a <code>docker-registry</code> secret and
        reference it in the pod spec.
      </p>
      <div class="snippet-label">Pull from private registry</div>
      <pre><code># Create registry secret
kubectl create secret docker-registry regcred \
  --docker-server=private-registry.io \
  --docker-username=registry-user \
  --docker-password=registry-password \
  --docker-email=registry-user@org.com</code></pre>
      <pre><code># Use in pod
apiVersion: v1
kind: Pod
metadata:
  name: private-app
spec:
  containers:
  - name: app
    image: private-registry.io/apps/internal-app
  imagePullSecrets:
  - name: regcred</code></pre>

      <h3>Security Contexts</h3>
      <p>Restrict pod/container behavior: run as non-root user, read-only root filesystem, drop capabilities. Set at
        <code>spec.securityContext</code> (pod) or <code>spec.containers[].securityContext</code> (container).
        Container-level settings override pod-level. Capabilities can only be set at <strong>container</strong> level.
      </p>
      <pre><code>spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  containers:
  - name: ubuntu
    image: ubuntu
    securityContext:
      runAsUser: 1000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
        drop: ["ALL"]</code></pre>
      <pre><code># Check who's running inside the container
kubectl exec my-pod -- whoami
kubectl exec my-pod -- id</code></pre>
      <h3>Advanced (2025)</h3>
      <p>Custom Resource Definitions (CRD), Custom Controllers, Operator Framework. See section 15 (Advanced Kubectl) for CRDs.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 07 Security</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 07 Security" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 08 Storage -->
      <h2 id="section-08">08 â€” Storage</h2>
      <p>Kubernetes gives workloads access to storage through <strong>volumes</strong> (pod-level) and <strong>PersistentVolumes (PV) / PersistentVolumeClaims (PVC)</strong> (cluster-level persistence). The CKA exam tests your ability to create PVs and PVCs, bind them to pods, and understand <strong>StorageClasses</strong> for dynamic provisioning. You should also know how ConfigMaps and Secrets are mounted as volumes.</p>
      <figure class="diagram">
        <img src="assets/k8s-storage-architecture.png" alt="Kubernetes storage architecture: control plane, workers, CSI, and storage backends" width="960" height="560" style="max-width:100%;height:auto;display:block;">
        <figcaption>Storage in context: control plane (API server, etcd) and workers (kubelet, CSI node plugin) integrate with PV/PVC and external storage backends (NFS, cloud disks, SAN).</figcaption>
      </figure>
      <h3>Docker Storage</h3>
      <p>Docker uses a <strong>layered filesystem</strong> (copy-on-write): each image layer and container layer is stacked. Data written inside a container is lost when the container is removed unless it is stored in a <strong>volume</strong>. Docker volumes persist data outside the container lifecycle. Use <code>docker volume create</code>, <code>docker volume ls</code>, and <code>docker run -v</code> for volume management. Kubernetes has its own volume model and does not rely on Docker volumes.</p>
      <h4>Volume Drivers</h4>
      <p>Docker volume drivers (local, nfs, cloud-provider plugins) provide different storage backends. In Kubernetes, storage is abstracted through the <strong>Container Storage Interface (CSI)</strong> and the in-tree volume types (hostPath, emptyDir, persistentVolumeClaim, etc.). You configure storage in the Pod and PVC spec, not via a separate driver CLI.</p>
      <div class="cka-definition">
        <div class="cka-def-term">PV & PVC</div>
        <div class="cka-def-body"><strong>PersistentVolume (PV):</strong> Cluster-wide storage resource. <strong>PersistentVolumeClaim (PVC):</strong> User request for storage. Binding matches PVC (size, accessMode, storageClass) to PV. CKA: static provisioning (create PV + PVC) or dynamic (StorageClass). Know <code>accessModes</code> (RWO, ROX, RWX) and <code>reclaimPolicy</code>.</div>
      </div>
      <h3>Kubernetes Storage</h3>
      <h4>Container Storage Interface (CSI)</h4>
      <p>The <strong>CSI</strong> is a standard for exposing block and file storage to Kubernetes. CSI drivers run as DaemonSets or StatefulSets and handle create/attach/mount of volumes. When you use a StorageClass with a <code>provisioner</code> like <code>kubernetes.io/aws-ebs</code> or a CSI driver name, the driver dynamically provisions a PV when a PVC is created. For the CKA, you typically work with static PV/PVC or a given StorageClass; you are not expected to write a CSI driver.</p>
      <h4>Volumes</h4>
      <p>Pod-level volumes are defined in <code>spec.volumes</code> and mounted into containers via <code>volumeMounts</code>. Common types: <strong>emptyDir</strong> (temporary, shared between containers in a pod; lost when pod is removed), <strong>hostPath</strong> (node filesystem; use with care for debugging or static data), <strong>persistentVolumeClaim</strong> (binds to a PVC for persistent data), and <strong>configMap</strong> / <strong>secret</strong> (inject config or secrets as files). Choosing the right type matters: use emptyDir for scratch space, PVC for data that must survive pod restarts and rescheduling.</p>
      <figure class="diagram">
        <img src="assets/k8s-storage-volume-types.png" alt="Pod volume types: emptyDir, hostPath, persistentVolumeClaim" width="560" height="200" style="max-width:100%;height:auto;display:block;">
        <figcaption>Common volume types: emptyDir (temporary), hostPath (node path), persistentVolumeClaim (persistent data).</figcaption>
      </figure>
      <h3>Persistent Volumes (PV) and Persistent Volume Claims (PVC)</h3>
      <p>A <strong>PersistentVolume (PV)</strong> is a cluster-scoped resource that represents a piece of storage (e.g. an NFS share, a cloud disk). An administrator provisions PVs. A <strong>PersistentVolumeClaim (PVC)</strong> is a namespace-scoped request for storage by a user or a workload. The control plane <strong>binds</strong> a PVC to a PV when capacity and <code>accessModes</code> match. If no suitable PV exists, the PVC stays <strong>Pending</strong> until one is created or (with dynamic provisioning) until the StorageClass provisioner creates a new PV. Access modes: <strong>ReadWriteOnce (RWO)</strong> â€” one node can mount read-write; <strong>ReadOnlyMany (ROX)</strong> â€” many nodes read-only; <strong>ReadWriteMany (RWX)</strong> â€” many nodes read-write (typically NFS or distributed storage).</p>
      <figure class="diagram">
        <img src="assets/k8s-storage-pv-pvc-pod.png" alt="Storage flow: PV â†’ PVC â†’ Pod" width="500" height="110" style="max-width:100%;height:auto;display:block;">
        <figcaption>Storage flow: PV (pool) â†’ PVC (claim) â†’ Pod (volumeMounts). PVC and Pod must be in same namespace.
        </figcaption>
      </figure>
      <div class="snippet-label">PV definition</div>
      <pre><code>kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-vol1
spec:
  accessModes: ["ReadWriteOnce"]
  capacity:
    storage: 1Gi
  hostPath:
    path: /tmp/data</code></pre>
      <div class="snippet-label">PVC definition</div>
      <pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 1Gi</code></pre>
      <pre><code>kubectl create -f pv-definition.yaml
kubectl create -f pvc-definition.yaml
kubectl get pv
kubectl get pvc
kubectl delete pvc myclaim
kubectl delete pv pv-vol1</code></pre>

      <h4>Using PVC in Pods</h4>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: myfrontend
    image: nginx
    volumeMounts:
    - mountPath: "/var/www/html"
      name: web
  volumes:
  - name: web
    persistentVolumeClaim:
      claimName: myclaim</code></pre>
      <pre><code>kubectl create -f pod-definition.yaml
kubectl get pod,pvc,pv</code></pre>

      <h3>Storage Classes</h3>
      <p>A <strong>StorageClass</strong> defines a class of storage (e.g. "fast SSD", "standard HDD") and a <strong>provisioner</strong> that can create PVs on demand. When you create a PVC with <code>storageClassName: google-storage</code> (for example), the StorageClass's provisioner automatically creates a PV that satisfies the claim. This is <strong>dynamic provisioning</strong>; you do not create the PV manually. If you omit <code>storageClassName</code> or set it to <code>""</code>, only pre-provisioned (static) PVs can bind. For the exam, you may need to create a StorageClass YAML or reference an existing one in a PVC.</p>
      <figure class="diagram">
        <img src="assets/k8s-storage-dynamic-provisioning.png" alt="Dynamic provisioning flow: PVC to StorageClass provisioner to PV to Pod" width="560" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption>With a StorageClass, creating a PVC triggers the provisioner to create a PV and bind it; the Pod then mounts the PVC.</figcaption>
      </figure>
      <pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd</code></pre>
      <pre><code>kubectl create -f sc-definition.yaml
kubectl get sc</code></pre>
      <p>In PVC add <code>storageClassName: google-storage</code>. Then create PVC and use <code>claimName</code> in
        pod
        volume as above.</p>
      <h4>Application Configuration Topics</h4>
      <p>ConfigMaps and Secrets can be injected into pods as <strong>environment variables</strong> (<code>env</code> with <code>valueFrom.configMapKeyRef</code> / <code>secretKeyRef</code>, or <code>envFrom</code> with <code>configMapRef</code> / <code>secretRef</code>) or as <strong>files</strong> via <code>volumes</code> and <code>volumeMounts</code>. File injection is useful when the application reads config from a path (e.g. <code>/etc/config/app.ini</code>). Use <code>defaultMode</code> on the volume to set file permissions for mounted ConfigMap/Secret files.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 08 Storage</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 08 Storage" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 09 Networking -->
      <h2 id="section-09">09 â€” Networking</h2>
      <p>Kubernetes networking is built on a few principles: every pod gets a unique IP, pods can talk to each other without NAT, and Services provide stable virtual IPs that load-balance to pods. The CKA exam tests CNI, Services, DNS (CoreDNS), and basic Linux networking (ip, route, DNS). You should understand how traffic flows from a Service to a Pod and how to troubleshoot DNS and connectivity.</p>
      <div class="cka-definition">
        <div class="cka-def-term">CNI (Container Network Interface)</div>
        <div class="cka-def-body">Standard for pod networking. Plugins (Calico, Flannel, Weave) assign pod IPs and configure routing. kubelet calls CNI when pod is created/destroyed. CKA: verify CNI pods in kube-system, check DNS (CoreDNS), service endpoints.</div>
      </div>
      <h3>Networking Fundamentals</h3>
      <p>Understanding basic Linux networking helps when troubleshooting cluster or node issues. Key concepts:</p>
      <h4>Switching</h4>
      <p><strong>Layer 2 (data link):</strong> Switches forward frames by MAC address. On Linux, <code>ip link</code> shows interfaces and <code>ip addr</code> shows IP addresses. Virtual Ethernet pairs (veth) connect network namespaces (e.g. pod to bridge).</p>
      <h4>Routing</h4>
      <p><strong>Layer 3 (network):</strong> Routers forward packets by IP address using the routing table. Use <code>ip route</code> or <code>route -n</code> to view routes. In Kubernetes, nodes and CNI plugins set up routes so pod traffic can reach other pods and external IPs.</p>
      <h4>Gateways</h4>
      <p>The <strong>default gateway</strong> is the next-hop for destinations not in the local routing table. Set it with <code>ip route add default via &lt;gateway-ip&gt;</code>. For a node to forward traffic between networks (e.g. between pod and external), <strong>IP forwarding</strong> must be enabled: <code>net.ipv4.ip_forward=1</code> (in sysctl). CNI plugins typically ensure this is set.</p>
      <h3>Linux networking for CKA</h3>
      <p>You may need to inspect interfaces, routes, or DNS on a node or inside a pod. These commands are essential:</p>
      <div class="snippet-label">Switching, routing, gateways</div>
      <pre><code># List network interfaces
ip link
ip addr

# Assign IP address to interface
ip addr add 192.168.1.10/24 dev eth0

# View routing table
ip route show
route

# Add route
ip route add 192.168.2.0/24 via 192.168.1.1

# Default gateway
ip route add default via 192.168.1.1

# Enable IP forwarding (required for routing between networks)
cat /proc/sys/net/ipv4/ip_forward   # 0=disabled, 1=enabled
echo 1 > /proc/sys/net/ipv4/ip_forward
# Permanent: set net.ipv4.ip_forward=1 in /etc/sysctl.conf
sysctl --system</code></pre>
      <h4>DNS Basics</h4>
      <p><strong>DNS</strong> resolves domain names to IP addresses. In Kubernetes, cluster-internal DNS is provided by <strong>CoreDNS</strong>. Services get a DNS name like <code>svc-name.namespace.svc.cluster.local</code>; pods can resolve this to the Service's ClusterIP. Understanding FQDN structure and how to test DNS (<code>nslookup</code>, <code>dig</code> from a debug pod) is important for the exam.</p>
      <h4>CoreDNS</h4>
      <p><strong>CoreDNS</strong> is the default cluster DNS server. It runs as pods in <code>kube-system</code> and is configured via a ConfigMap named <code>coredns</code>. The config file (Corefile) uses plugins: <code>kubernetes</code> (to serve DNS records from Services and Pods), <code>forward</code> (to forward external queries to upstream DNS), <code>errors</code>, <code>health</code>, etc. For the CKA, know how to check if CoreDNS is running (<code>kubectl get pods -n kube-system -l k8s-app=kube-dns</code>), how to test resolution from a pod (<code>nslookup kubernetes</code>), and that the Corefile is stored in the <code>coredns</code> ConfigMap.</p>
      <h4>Network Namespaces</h4>
      <pre><code># Create network namespace
ip netns add red
ip netns add blue

# List namespaces
ip netns

# Run command in namespace
ip netns exec red ip link
ip -n red link

# Connect namespaces with veth pair
ip link add veth-red type veth peer name veth-blue
ip link set veth-red netns red
ip link set veth-blue netns blue
ip -n red addr add 192.168.15.1/24 dev veth-red
ip -n blue addr add 192.168.15.2/24 dev veth-blue
ip -n red link set veth-red up
ip -n blue link set veth-blue up</code></pre>

      <h3>Cluster Networking</h3>
      <h4>Pod Networking</h4>
      <figure class="diagram">
        <img src="assets/k8s-pod-networking-cni.png" alt="Pod networking: CNI assigns IPs, pods communicate across nodes" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Pod networking:</strong> CNI assigns each pod a unique IP from Pod CIDR. Pods on different nodes communicate directly.</figcaption>
      </figure>
      <p>Kubernetes uses the <strong>Container Network Interface (CNI)</strong> standard to manage networking.
        <br><em>Core Responsibilities of CNI Plugin:</em>
        <br>1. <strong>Connectivity:</strong> Insert a network interface into the container's namespace.
        <br>2. <strong>IPAM (IP Address Management):</strong> Assign an IP address to the pod from the Pod CIDR.
      </p>

      <h4>CNI Fundamentals / CNI in Kubernetes</h4>
      <p><em>Golden Rules:</em>
        <br>- Every Pod gets a unique IP.
        <br>- Pods on the same node talk via <code>localhost</code>/bridge.
        <br>- Pods on different nodes talk directly without NAT (Network Address Translation).
      </p>

      <pre><code># Check CNI configuration
ls /etc/cni/net.d/
cat /etc/cni/net.d/10-flannel.conflist

# Check CNI binaries
ls /opt/cni/bin/

# Identify Pod CIDR (allocated to node)
kubectl get node -o jsonpath='{.spec.podCIDR}'</code></pre>

      <h3>Service Networking</h3>
      <p>A Service provides a stable <strong>ClusterIP</strong> (Virtual IP) that load balances traffic to dynamic Pod
        IPs.
        <br><strong>Service:</strong> The abstraction (stable VIP).
        <br><strong>Endpoints:</strong> The actual list of backend Pod IPs. Checked via
        <code>kubectl get endpoints</code>.
      </p>
      <figure class="diagram">
        <img src="assets/k8s-service-to-pods.png" alt="Service flow: ClusterIP â†’ kube-proxy â†’ Endpoints â†’ Pods" width="560" height="120" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Service to Pods:</strong> Client &rarr; ClusterIP (VIP) &rarr; kube-proxy (iptables/IPVS) &rarr; Endpoints &rarr; backend Pods.</figcaption>
      </figure>
      <p><em>Mechanism:</em> <strong>kube-proxy</strong> runs on every node and watches the API server. It updates
        node
        network rules (usually <strong>iptables</strong> or <strong>IPVS</strong>) to trap traffic destined for the
        Service IP and redirect it to a random backing Pod IP.</p>

      <pre><code># View iptables rules for a service
iptables-save | grep &lt;service-name&gt;

# Check kube-proxy mode
kubectl logs -n kube-system -l k8s-app=kube-proxy</code></pre>

      <h3>DNS in Kubernetes / CoreDNS in Kubernetes</h3>
      <figure class="diagram">
        <img src="assets/k8s-dns-coredns.png" alt="DNS resolution: Pod â†’ CoreDNS â†’ Service IP" width="540" height="160" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>DNS resolution:</strong> Pod lookup (e.g. <code>my-svc.default.svc.cluster.local</code>) â†’ CoreDNS â†’ returns Service ClusterIP.</figcaption>
      </figure>
      <p><strong>CoreDNS</strong> is the cluster's internal DNS server. It watches the API server for new Services and
        Pods and creates DNS records.
        <br><em>Config:</em> The <strong>Corefile</strong> (inside a ConfigMap) defines behavior (plugins like
        <code>errors</code>, <code>health</code>, <code>kubernetes</code>, <code>forward</code>).
      </p>

      <p><strong>FQDN (Fully Qualified Domain Name) Structure:</strong>
        <br>Service: <code>my-svc.my-ns.svc.cluster.local</code>
        <br>Pod: <code>1-2-3-4.my-ns.pod.cluster.local</code> (IP with dashes)
      </p>

      <pre><code># Test DNS from a debug pod
kubectl run -it --rm debug --image=busybox:1.28 -- nslookup kubernetes
kubectl get configmap coredns -n kube-system -o yaml</code></pre>

      <h3>Ingress</h3>
      <div class="cka-definition">
        <div class="cka-def-term">Ingress</div>
        <div class="cka-def-body">API object that manages external HTTP/HTTPS access to Services. Requires an Ingress controller (e.g., nginx-ingress). Rules map host/path to backend services. CKA: know <code>rules</code>, <code>pathType</code> (Prefix, Exact), TLS with <code>secretName</code>, and <code>rewrite-target</code> annotation.</div>
      </div>
      <div class="cka-definition">
        <div class="cka-def-term">Gateway API (2025)</div>
        <div class="cka-def-body">Successor to Ingress. Uses <code>Gateway</code> (infra) + <code>HTTPRoute</code> (routing rules). Role-oriented (GatewayClass, Gateway, Routes). More expressive than Ingress. CKA 2025: know it exists; Ingress still primary for exam.</div>
      </div>
      <p>Need an Ingress controller (e.g. nginx-ingress) and Ingress resources (rules). Ingress exposes HTTP/HTTPS
        routes to services.</p>
      <figure class="diagram">
        <img src="assets/k8s-ingress-flow.png" alt="Ingress flow: External â†’ Ingress Controller â†’ Service â†’ Pods" width="560" height="140" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Ingress flow:</strong> External traffic &rarr; Ingress Controller &rarr; Ingress rules (host/path) &rarr; Service &rarr; Pods.</figcaption>
      </figure>
      <h4>Ingress Annotations</h4>
      <p>Controller-specific: e.g. <code>nginx.ingress.kubernetes.io/rewrite-target</code>, <code>cert-manager.io/cluster-issuer</code>.</p>
      <h4>Rewrite Target</h4>
      <p>nginx-ingress: <code>nginx.ingress.kubernetes.io/rewrite-target: /</code> rewrites path before forwarding to backend.</p>
      <div class="snippet-label">Ingress with paths</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <div class="snippet-label">Ingress with host rules</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
  - host: watch.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <pre><code>kubectl create -f ingress.yaml
kubectl get ingress
kubectl describe ingress ingress-wear-watch</code></pre>

      <h3>Gateway API</h3>
      <p>The <strong>Gateway API</strong> is the newer, more expressive way to expose HTTP/gRPC/TCP routes (successor
        concept to Ingress). Key resources: <code>Gateway</code> (infrastructure that receives traffic),
        <code>HTTPRoute</code> (rules for routing to backends). Install a Gateway controller (e.g. Gateway API
        implementation) and create Gateway + HTTPRoute resources. The exam may reference it; know it exists as the
        evolution of Ingress.
      </p>

      <h4>Introduction to Gateway API</h4>
      <p>Gateway API is the evolution of Ingress. Resources: GatewayClass, Gateway, HTTPRoute, etc.</p>
      <h4>Practical Guide to Gateway API</h4>
      <p>Create Gateway + HTTPRoute. Install a Gateway controller (e.g. istio, envoy). Route traffic to Services.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 09 Networking</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 09 Networking" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 10 Design & Install -->
      <h2 id="section-10">10 â€” Cluster Design & Installation</h2>
      <p>Before installing a cluster, you need a clear design: how many control-plane nodes, how etcd is run, what network ranges to use, and how workers will join. The CKA exam may ask you to interpret or extend an existing design rather than install from scratch; understanding high availability and the role of each component is key.</p>
      <h3>Design Kubernetes Cluster</h3>
      <p>Design decisions include: <strong>Control-plane count</strong> â€” single master (dev/test) vs multiple masters for HA. <strong>Node count and sizing</strong> â€” enough workers for your workloads and failure domains. <strong>Networking</strong> â€” Pod CIDR (e.g. 10.244.0.0/16) and Service CIDR (e.g. 10.96.0.0/12) must not overlap with node networks. <strong>Storage</strong> â€” default StorageClass, CSI drivers if needed. <strong>Add-ons</strong> â€” CNI (required for pod networking), CoreDNS, Metrics Server, optional Ingress controller.</p>
      <h3>Choosing Infrastructure</h3>
      <p>Clusters can run on <strong>VMs</strong> (on-prem or cloud), <strong>bare metal</strong>, or <strong>managed Kubernetes</strong> (EKS, GKE, AKS). For the CKA you typically work with VMs or a lab environment. Consider: network latency between nodes, firewall rules for control-plane and node ports, and whether you need persistent storage (cloud disks, NFS, or CSI).</p>
      <h3>Configure High Availability</h3>
      <p>High availability (HA) means the cluster keeps running even if one or more components fail. For the <strong>control plane</strong>: run multiple API server replicas behind a load balancer so that if one master fails, the others serve traffic. For <strong>etcd</strong>: run a cluster of 3 or 5 nodes so that a minority failure does not lose quorum. The <strong>scheduler</strong> and <strong>controller-manager</strong> can run as active/standby (only one leader at a time via etcd lease). Worker nodes are stateless from the control plane's perspective; losing one node affects only the pods on that node, which can be rescheduled elsewhere.</p>
      <h4>ETCD in HA</h4>
      <p>etcd uses the <strong>RAFT</strong> consensus algorithm. Run an <strong>odd number</strong> of members (3 or 5) so that a majority can always be reached. <strong>Stacked</strong> topology: etcd runs on the same nodes as the API server (simpler, but node failure affects both). <strong>External</strong> topology: etcd runs on a separate cluster (more resilient, more operational overhead). For the CKA, know that etcd is the single source of truth and that backup/restore is done with etcdctl/etcdutl snapshots.</p>
      <h3>Kubernetes the Hard Way</h3>
      <p><strong>Kubernetes the Hard Way</strong> is a tutorial that installs Kubernetes from scratch without kubeadm: you provision VMs, install binaries (kubelet, kube-apiserver, etcd, scheduler, controller-manager, kube-proxy), generate certificates, configure kubeconfig, and start each component. It is excellent for understanding how the pieces fit together. The CKA exam does not require you to perform a full manual install; you are more likely to use kubeadm or work with an existing cluster. The design concepts â€” multiple API servers, etcd cluster, load balancer, node join flow â€” still apply.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 10 Cluster Design</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 10 Cluster Design" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 11 kubeadm -->
      <h2 id="section-11">11 â€” kubeadm Installation</h2>
      <p><strong>kubeadm</strong> is the standard tool for bootstrapping a conformant Kubernetes cluster. It generates certificates, static pod manifests for the control plane (API server, etcd, scheduler, controller-manager), and the initial kubeconfig. You run <code>kubeadm init</code> on the first control-plane node, then <code>kubeadm join</code> on workers (and optional additional control-plane nodes). The CKA exam rarely asks for a full from-scratch install; more often you will <strong>upgrade</strong> an existing cluster (<code>kubeadm upgrade plan</code>, <code>kubeadm upgrade apply</code>, drain/uncordon nodes) or fix a join problem (expired token, wrong CA hash).</p>
      <h3>Deployment with kubeadm</h3>
      <p><strong>kubeadm</strong> bootstraps the control plane and generates certs, manifests, and kubeconfig. You
        then
        join worker nodes with <code>kubeadm join</code>. The exam rarely asks for a full install; <strong>upgrading
          an
          existing cluster</strong> (kubeadm upgrade plan / apply, drain/uncordon workers) is frequently tested.</p>
      <figure class="diagram">
        <img src="assets/k8s-kubeadm-flow.png" alt="kubeadm init and join flow" width="560" height="180" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>kubeadm flow:</strong> Control plane: <code>kubeadm init</code>; workers: <code>kubeadm join</code> with token and CA hash.</figcaption>
      </figure>
      <h3>Provision VMs</h3>
      <p>Create VMs (or use cloud instances). Install container runtime, disable swap, configure networking. See Prerequisites below.</p>
      <h3>Deploy Control Plane</h3>
      <h3>Join Worker Nodes</h3>
      <p>Use <code>kubeadm join</code> with token and CA hash from <code>kubeadm init</code> output. Or: <code>kubeadm token create --print-join-command</code>.</p>
      <h4>kubeadm init steps (reference)</h4>
      <div class="snippet-label">Prerequisites on all nodes</div>
      <pre><code># Disable swap (required)
sudo swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# Enable kernel modules
cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

# Sysctl params
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system

# Install containerd, kubeadm, kubelet, kubectl
# (follow official docs for your OS)
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl</code></pre>
      <div class="snippet-label">Control plane init</div>
      <pre><code># Initialize control plane
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=&lt;master-ip&gt;

# Copy kubeconfig
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install CNI (e.g. Flannel)
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml</code></pre>
      <div class="snippet-label">Join workers</div>
      <pre><code># On worker nodes (use token from kubeadm init output):
sudo kubeadm join &lt;master-ip&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;

# If token expired, create a new one on master:
kubeadm token create --print-join-command</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 11 kubeadm</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 11 kubeadm" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 12 Helm -->
      <h2 id="section-12">12 â€” Helm</h2>
      <p>Helm is the de facto <strong>package manager</strong> for Kubernetes. It lets you install, upgrade, and uninstall applications as pre-packaged <strong>charts</strong> (templated YAML + default values). The CKA curriculum includes Helm; you should know basic install/upgrade/uninstall and how <code>values.yaml</code> customizes a chart.</p>
      <h3>Helm Installation</h3>
      <p>Install Helm 3 via the official script: <code>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</code>, or use your OS package manager. Helm 3 does not use Tiller; it talks to the cluster using your kubeconfig.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Helm</div>
        <div class="cka-def-body">The package manager for Kubernetes. Packages charts (pre-configured K8s resources). Helm 3 removed Tiller; uses direct API calls. CKA: know <code>helm install</code>, <code>helm list</code>, <code>helm upgrade</code>, <code>helm uninstall</code>, and <code>values.yaml</code> for customization.</div>
      </div>
      <h3>Helm Introduction</h3>
      <p>Helm packages Kubernetes manifests as <strong>charts</strong>: a directory with <code>Chart.yaml</code> (metadata), <code>values.yaml</code> (default config), and <code>templates/</code> (YAML templates with Go templating). Installing a chart creates a <strong>release</strong> â€” a named instance of that chart in the cluster. Helm 3 stores release state in Secrets in the cluster; there is no server-side Tiller.</p>
      <h3>Helm Charts</h3>
      <p>A chart is a versioned package of templated Kubernetes resources. Structure: <code>Chart.yaml</code> (name, version, description), <code>values.yaml</code> (default values), <code>templates/*.yaml</code> (manifests with <code>{{ .Values.xxx }}</code>). You override values with <code>--set</code> or a custom <code>values.yaml</code> file.</p>
      <h3>Helm Components</h3>
      <ul>
        <li><strong>Chart:</strong> Package of templated K8s manifests (the application definition).</li>
        <li><strong>Release:</strong> Instance of a chart deployed to the cluster (each <code>helm install</code> creates a release).</li>
        <li><strong>Values:</strong> Configuration that fills in the chart templates; override via <code>--values</code> or <code>--set</code>.</li>
      </ul>
      <h3>Working with Helm</h3>
      <div class="snippet-label">Essential Helm commands (CKA)</div>
      <pre><code># Add a repo
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# Install a chart
helm install my-nginx bitnami/nginx
helm install my-nginx bitnami/nginx -f values.yaml
helm install my-nginx bitnami/nginx --set service.port=8080

# List releases
helm list
helm list -A

# Upgrade
helm upgrade my-nginx bitnami/nginx --set replicaCount=3

# Uninstall
helm uninstall my-nginx

# Template (render YAML without installing)
helm template my-nginx bitnami/nginx -f values.yaml

# Customizing values
helm show values bitnami/nginx
helm install my-nginx bitnami/nginx --set image.tag=1.25</code></pre>
      <h3>Helm Lifecycle Management</h3>
      <p><code>helm install</code> (create) â†’ <code>helm upgrade</code> (update) â†’ <code>helm rollback</code> (revert) â†’ <code>helm uninstall</code> (delete).</p>
      <h3>Helm2 vs Helm3</h3>
      <p>Helm 3 removed Tiller (no server-side component). Release data stored in Kubernetes Secrets in the release namespace. RBAC applies to Helm as a regular kubectl user.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 12 Helm</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 12 Helm" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 13 Kustomize -->
      <h2 id="section-13">13 â€” Kustomize</h2>
      <h3>Installation</h3>
      <p>Built into kubectl: <code>kubectl kustomize</code>. Standalone: <code>curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash</code>.</p>
      <div class="cka-definition">
        <div class="cka-def-term">Kustomize</div>
        <div class="cka-def-body">Declarative customization of Kubernetes manifests without templating. Uses <code>kustomization.yaml</code> to patch, overlay, and compose resources. Built into kubectl: <code>kubectl apply -k .</code>. CKA: know overlays, patches, image transformers, and <code>kubectl kustomize</code>.</div>
      </div>
      <h3>Kustomize Ideology</h3>
      <p>Kustomize keeps base manifests unchanged and applies <strong>overlays</strong> (dev, staging, prod) for environment-specific config. No templating â€” pure YAML patches.</p>
      <h3>Kustomize vs Helm</h3>
      <p>Kustomize: native kubectl, no extra binary, patch-based. Helm: templating, chart ecosystem, versioned releases. Both are in the CKA 2025 curriculum.</p>
      <div class="snippet-label">kustomization.yaml and overlays</div>
      <pre><code># Base: kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml

# Overlay: overlays/prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: replace
        path: /spec/replicas
        value: 5
images:
  - name: myapp
    newTag: v2.0

# Apply
kubectl apply -k overlays/prod
kubectl kustomize overlays/prod   # preview</code></pre>
      <h4>Image Transformers</h4>
      <p>In kustomization.yaml: <code>images: - name: myapp; newTag: v2</code>. Replace image by name.</p>
      <h4>Components</h4>
      <p>Reusable kustomize fragments. Reference with <code>resources: - ../components/my-component</code>.</p>
      <h3>Transformers & Patches</h3>
      <ul>
        <li><strong>namePrefix/nameSuffix:</strong> Add prefix/suffix to resource names</li>
        <li><strong>images:</strong> Replace container images by name</li>
        <li><strong>patches:</strong> JSON Patch or strategic merge</li>
        <li><strong>configMapGenerator/secretGenerator:</strong> Generate ConfigMaps/Secrets from files</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 13 Kustomize</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 13 Kustomize" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 14 Troubleshooting -->
      <h2 id="section-14">14 â€” Troubleshooting</h2>
      <p>Troubleshooting has the <strong>highest weight (30%)</strong> on the CKA exam. You must systematically debug application failures (pod status, logs, Service/Endpoints), control-plane failures (API server, etcd, scheduler), worker-node failures (kubelet, runtime, NotReady), and network issues (DNS, CNI, kube-proxy). Always start with <code>kubectl get</code> and <code>kubectl describe</code>, then check logs and events.</p>
      <figure class="diagram">
        <img src="assets/k8s-troubleshooting-flowchart.png" alt="Troubleshooting decision flowchart for CKA" width="560" height="280" style="max-width:100%;height:auto;display:block;">
        <figcaption><strong>Troubleshooting flow:</strong> Pending â†’ scheduler/taints/resources. Running but no traffic â†’ Service/Endpoints. CrashLoopBackOff â†’ logs --previous. No connectivity â†’ DNS/CNI/kube-proxy.</figcaption>
      </figure>
      <div class="cka-definition">
        <div class="cka-def-term">Troubleshooting (30% of CKA)</div>
        <div class="cka-def-body">Highest-weight domain. Application: check pod status, describe, logs (<code>--previous</code>), endpoints, selector mismatch. Control plane: kube-apiserver, etcd, scheduler. Worker: kubelet, container runtime. Network: CNI, CoreDNS, kube-proxy, firewall. Use <code>kubectl describe</code>, <code>journalctl -u kubelet</code>, <code>crictl</code>.</div>
      </div>
      <h3>Application Failure</h3>
      <p>When an application is not working, follow a clear order: Is the Service reachable? Does it have Endpoints? Do pod labels match the Service selector? What does the pod status and <code>describe</code> show? What do the container logs (and <code>--previous</code> for crashed containers) say? Many exam tasks are "fix the broken deployment" or "find why the service returns no data" â€” selector mismatch and missing/crashing pods are common causes.</p>
      <h4>Debug Pod Failures</h4>
      <ul class="step-list">
        <li><strong>Is the service reachable?</strong> <code>curl http://&lt;node-ip&gt;:&lt;nodePort&gt;</code> or
          from
          inside cluster <code>curl http://&lt;svc-name&gt;.&lt;ns&gt;.svc.cluster.local:port</code>.</li>
        <li><strong>Does the service have endpoints?</strong> <code>kubectl get endpoints &lt;service-name&gt;</code>.
          If empty, the service selector does not match any pod (wrong labels or pods not ready).</li>
        <li><strong>Compare selector with pod labels:</strong> <code>kubectl describe service &lt;name&gt;</code> (see
          Selector) and <code>kubectl get pods --show-labels</code>. Fix selector or pod labels.</li>
        <li><strong>Pod status:</strong> <code>kubectl get pods</code>,
          <code>kubectl describe pod &lt;pod-name&gt;</code>. Check events (image pull, CrashLoopBackOff, etc.).
        </li>
        <li><strong>Container logs:</strong> <code>kubectl logs &lt;pod-name&gt;</code>,
          <code>kubectl logs &lt;pod-name&gt; -c &lt;container&gt;</code>,
          <code>kubectl logs &lt;pod-name&gt; --previous</code> for crashed container.
        </li>
      </ul>
      <pre><code>kubectl get pods -o wide
kubectl describe pod &lt;pod-name&gt;
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>

      <h3>Control Plane Failure</h3>
      <p>If the API server is down, <code>kubectl</code> will not work. Control-plane components (with kubeadm) run as static pods in <code>kube-system</code>; with a manual install they are often systemd services. Check pod status first, then logs. Certificate expiry is a common cause of kubelet or API server issues.</p>
      <h4>Debug API Server</h4>
      <p>Verify the API server pod is Running: <code>kubectl get pods -n kube-system -l component=kube-apiserver</code>. View logs: <code>kubectl logs kube-apiserver-&lt;node&gt; -n kube-system</code>. Check for certificate errors, wrong endpoints, or admission-controller failures. Ensure the server cert is valid: <code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout</code>.</p>
      <h4>Debug ETCD</h4>
      <p>etcd is the cluster's data store; if it is down or corrupted, the control plane cannot function. Check the etcd pod status and logs. Use <code>ETCDCTL_API=3 etcdctl endpoint health</code> (with correct <code>--endpoints</code> and certs) to verify connectivity. Ensure the data directory is writable and that certs have not expired.</p>
      <pre><code>kubectl get nodes
kubectl get pods -n kube-system
service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
service kubelet status
kubectl logs kube-apiserver-master -n kube-system
sudo journalctl -u kube-apiserver</code></pre>

      <h3>Worker Node Failure</h3>
      <p>When a node shows <strong>NotReady</strong>, workloads on it may be rescheduled elsewhere (if managed by a controller). The cause is usually the kubelet not reporting to the API server: it might be stopped, misconfigured, or unable to authenticate (e.g. expired client cert). Check the node conditions and events with <code>kubectl describe node</code>, then on the node itself inspect kubelet and the container runtime.</p>
      <h4>Debug kubelet</h4>
      <p>On the node: <code>systemctl status kubelet</code> and <code>journalctl -u kubelet -f</code>. Common issues: wrong kubeconfig or API server address, expired or missing client certificate, config errors in <code>/var/lib/kubelet/config.yaml</code> or <code>/etc/kubernetes/kubelet.conf</code>.</p>
      <h4>Node NotReady</h4>
      <p>kubelet not reporting. Check kubelet, container runtime, network. <code>kubectl describe node</code> for conditions.</p>
      <p>If a node is <strong>NotReady</strong>, check <code>kubectl describe node &lt;node&gt;</code> (conditions,
        LastHeartbeatTime). On the node: kubelet status, logs, and cert validity (kubelet may have expired cert).</p>
      <pre><code>kubectl get nodes
kubectl describe node worker-1
service kubelet status
sudo journalctl -u kubelet
openssl x509 -in /var/lib/kubelet/worker-1.crt -text</code></pre>
      <h4>Common worker node issues checklist</h4>
      <ul>
        <li><code>kubelet</code> not running â†’ <code>systemctl start kubelet</code>, check
          <code>journalctl -u kubelet</code>
        </li>
        <li>Config error â†’ check <code>/var/lib/kubelet/config.yaml</code> and
          <code>/etc/kubernetes/kubelet.conf</code>
        </li>
        <li>Expired certificates â†’ check cert dates with <code>openssl x509 -in &lt;cert&gt; -text -noout</code></li>
        <li>Node conditions: <code>MemoryPressure</code>, <code>DiskPressure</code>, <code>PIDPressure</code> â†’ check
          resources</li>
        <li>Container runtime down â†’ <code>systemctl status containerd</code></li>
      </ul>

      <h3>Network Troubleshooting</h3>
      <p>Network issues show up as "pod can't reach service", "DNS not resolving", or "pods on different nodes can't talk". Always verify: (1) Service has Endpoints; (2) CoreDNS pods are running and resolvable; (3) CNI plugin is healthy; (4) kube-proxy is running and programming iptables/IPVS. Use a debug pod (e.g. <code>nicolaka/netshoot</code> or <code>busybox</code>) to run <code>nslookup</code>, <code>curl</code>, and <code>ping</code> from inside the cluster.</p>
      <h4>Debug Service Issues</h4>
      <p>If a Service is not routing traffic, first check <code>kubectl get endpoints &lt;svc-name&gt;</code>. Empty endpoints mean no pod matches the Service selector â€” fix the selector or add the correct labels to the pods. Compare <code>kubectl describe svc</code> (Selector) with <code>kubectl get pods --show-labels</code>.</p>
      <h4>Debug DNS</h4>
      <p>Ensure CoreDNS pods are running: <code>kubectl get pods -n kube-system -l k8s-app=kube-dns</code>. Test resolution from a temporary pod: <code>kubectl run test --rm -it --image=busybox:1.28 -- nslookup kubernetes</code>. If this fails, check CoreDNS logs and the <code>coredns</code> ConfigMap (Corefile).</p>
      <h4>Debug CNI</h4>
      <p>When pods cannot communicate across nodes or DNS fails:</p>
      <ul class="step-list">
        <li><strong>Check CNI plugin:</strong> <code>kubectl get pods -n kube-system</code> â€” are Calico/Flannel/Weave
          pods running? Check logs.</li>
        <li><strong>CoreDNS running?</strong> <code>kubectl get pods -n kube-system -l k8s-app=kube-dns</code>. If
          CrashLooping, check <code>kubectl logs &lt;coredns-pod&gt; -n kube-system</code>.</li>
        <li><strong>DNS resolution:</strong>
          <code>kubectl run test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes</code>
        </li>
        <li><strong>kube-proxy running?</strong> <code>kubectl get ds kube-proxy -n kube-system</code>. Check iptables
          rules: <code>iptables -L -t nat | grep &lt;svc&gt;</code></li>
        <li><strong>Node networking:</strong> <code>ip link</code>, <code>ip addr</code>, <code>ip route</code> on
          affected node. Check IP forwarding: <code>cat /proc/sys/net/ipv4/ip_forward</code></li>
        <li><strong>Firewall/port issues:</strong> Ensure required ports are open (6443, 2379-2380, 10250-10252,
          30000-32767)</li>
      </ul>
      <pre><code># Debug networking from inside a pod
kubectl run netshoot --image=nicolaka/netshoot -it --rm --restart=Never -- bash
# Inside: ping, nslookup, curl, traceroute, tcpdump, iperf, etc.

# Test service from inside cluster
kubectl run test --image=busybox:1.28 --rm -it --restart=Never -- wget -qO- http://&lt;svc&gt;.&lt;ns&gt;.svc.cluster.local:&lt;port&gt;</code></pre>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 14 Troubleshooting</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 14 Troubleshooting" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 13 Other Topics -->
      <h2 id="section-15">15 â€” Advanced Kubectl & JSON Path</h2>
      <div class="cka-definition">
        <div class="cka-def-term">JSONPath</div>
        <div class="cka-def-body">Query language for extracting data from JSON. Used with <code>kubectl get -o jsonpath='{...}'</code>. CKA: know <code>{.items[*].metadata.name}</code>, <code>{range .items[*]}...{end}</code>, <code>{.status.addresses[?(@.type=="InternalIP")].address}</code>, and <code>custom-columns</code>.</div>
      </div>
      <h3>JSON Path in Kubernetes</h3>
      <pre><code>kubectl get nodes -o json
kubectl get pods -o json
kubectl get pods -o=jsonpath='{.items[0].spec.containers[0].image}'
kubectl get pods -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu
kubectl get nodes --sort-by=.metadata.name
kubectl get nodes --sort-by=.status.capacity.cpu</code></pre>

      <h3>Advanced Kubectl Commands</h3>
      <p><code>kubectl get</code> with <code>-o wide</code>, <code>--sort-by</code>, <code>--selector</code>. <code>kubectl describe</code>, <code>kubectl logs -f</code>, <code>kubectl exec -it</code>.</p>
      <h3>CRDs and operators (2025)</h3>
      <p><strong>Custom Resource Definitions (CRDs)</strong> extend the API with custom resources. After defining a
        CRD,
        you get new resource types (e.g. <code>kubectl get myresource</code>). <strong>Operators</strong> are
        controllers that manage custom resources and their lifecycle (install, upgrade, backup). They use CRDs + a
        controller process. List CRDs: <code>kubectl get crd</code>.</p>

      <h3>Extension interfaces: CNI, CSI, CRI</h3>
      <p>Kubernetes uses pluggable interfaces. <strong>CNI</strong> (Container Network Interface): pod networking;
        plugins (Calico, Flannel, Weave) implement the pod network. <strong>CSI</strong> (Container Storage
        Interface):
        storage drivers for PVs; dynamic provisioning via StorageClass uses a CSI driver. <strong>CRI</strong>
        (Container Runtime Interface): container runtimes (containerd, CRI-O); kubelet talks to the runtime via CRI.
        Configure/validate these when installing or troubleshooting the cluster.</p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 15 Advanced Kubectl</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 15 Advanced Kubectl" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 16 Practice & Exam Preparation -->
      <h2 id="section-16">16 â€” Practice &amp; Exam Preparation</h2>
      <h3>Lightning Labs</h3>
      <h4>Playground &amp; Lightning Labs</h4>
      <p>Use the links below to practice in a real cluster, then try the scenarios on your own before revealing the
        solution.</p>

      <h3>Online playgrounds</h3>
      <p><a href="lab.html" class="lab-link-btn" style="display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.6rem 1rem; background: var(--accent); color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">ðŸ“‹ Open CKA Lab</a> â€” 60+ copyable kubectl commands + practice scenarios. Run in Killercoda or your cluster.</p>
      <ul class="playground-links">
        <li>
          <a href="https://killercoda.com/kubernetes" target="_blank" rel="noopener noreferrer">Killercoda â€”
            Kubernetes</a>
          <div class="link-desc">Free browser-based Kubernetes labs; no local install. Multiple CKA-focused scenarios.
          </div>
        </li>
        <li>
          <a href="https://www.katacoda.com/courses/kubernetes" target="_blank" rel="noopener noreferrer">Katacoda
            (legacy)</a>
          <div class="link-desc">Kubernetes playgrounds (some scenarios may be deprecated; Killercoda is the
            successor).
          </div>
        </li>
        <li>
          <a href="https://labs.play-with-k8s.com/" target="_blank" rel="noopener noreferrer">Play with Kubernetes</a>
          <div class="link-desc">Spin up a temporary cluster in the browser; good for quick tests.</div>
        </li>
        <li>
          <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/" target="_blank"
            rel="noopener noreferrer">Kubernetes Basics (official)</a>
          <div class="link-desc">Official interactive tutorial with an in-browser terminal.</div>
        </li>
      </ul>

      <h3>Practice scenarios (try yourself, then reveal solution)</h3>

      <div class="playground-scenario">
        <h4>Scenario 1: Deployment and Service</h4>
        <p class="playground-context">Namespace <code>app</code> exists. Create a deployment and expose it.</p>
        <p class="playground-task"><strong>Task:</strong> Create a deployment named <code>nginx-deploy</code> with
          image
          <code>nginx:1.21</code>, 3 replicas. Expose it as a ClusterIP service <code>nginx-svc</code> on port 80. Use
          namespace <code>app</code>.
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create deployment nginx-deploy --image=nginx:1.21 --replicas=3 -n app
kubectl expose deployment nginx-deploy --name=nginx-svc --port=80 -n app
kubectl get deploy,svc -n app</code></pre>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 2: ConfigMap and Pod</h4>
        <p class="playground-context">You need a pod that reads a config value from a ConfigMap.</p>
        <p class="playground-task"><strong>Task:</strong> Create a ConfigMap <code>app-cm</code> with
          <code>APP_ENV=production</code>. Create a pod <code>busybox-cm</code> (image <code>busybox:1.28</code>) that
          runs <code>sleep 3600</code> and has environment variable <code>APP_ENV</code> from the ConfigMap.
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create configmap app-cm --from-literal=APP_ENV=production
kubectl run busybox-cm --image=busybox:1.28 --restart=Never -- sleep 3600 --overrides='
{"spec":{"containers":[{"name":"busybox-cm","image":"busybox:1.28","command":["sleep","3600"],"env":[{"name":"APP_ENV","valueFrom":{"configMapKeyRef":{"name":"app-cm","key":"APP_ENV"}}}]}]}}'</code></pre>
          <p style="margin-top:0.75rem;font-size:0.9rem;">Or create a YAML file with <code>env</code> and
            <code>valueFrom.configMapKeyRef</code> and <code>kubectl apply -f</code>.
          </p>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 3: Node selector and drain</h4>
        <p class="playground-context">Node <code>worker-1</code> has label <code>disk=ssd</code>. You need to run a
          pod
          there and later drain the node.</p>
        <p class="playground-task"><strong>Task:</strong> Create a pod <code>ssd-pod</code> (image
          <code>nginx:alpine</code>) that is scheduled only on nodes with <code>disk=ssd</code>. Then cordon and drain
          <code>worker-1</code> (ignore DaemonSet pods).
        </p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl run ssd-pod --image=nginx:alpine --restart=Never --overrides='
{"spec":{"nodeSelector":{"disk":"ssd"}}}'
kubectl cordon worker-1
kubectl drain worker-1 --ignore-daemonsets --delete-emptydir-data</code></pre>
        </div>
      </div>

      <!-- 15 Mock Exams -->
      <h3>Mock Exam 1, 2, 3</h3>
      <p>Time yourself (e.g. 10â€“15 minutes per question). Complete the task in your cluster or playground, then check
        the solution.</p>

      <div class="mock-question">
        <h4>Mock 1: RBAC â€” read-only pods in a namespace</h4>
        <p class="mock-context">Namespace <code>dev</code> exists. A user/service account should only list and get
          pods
          in <code>dev</code>, no create/delete.</p>
        <p class="mock-task"><strong>Task:</strong> Create a Role that allows <code>get</code>, <code>list</code> on
          <code>pods</code> in namespace <code>dev</code>. Create a RoleBinding binding that Role to a user named
          <code>jane</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl create role pod-reader --verb=get,list --resource=pods -n dev
kubectl create rolebinding jane-pod-reader --role=pod-reader --user=jane -n dev</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Or apply YAML with <code>kind: Role</code> (rules with
            apiGroups <code>[""]</code>, resources <code>["pods"]</code>, verbs <code>["get","list"]</code>) and
            <code>kind: RoleBinding</code> (roleRef, subjects with name <code>jane</code>).
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 2: Multi-container pod and logs</h4>
        <p class="mock-context">A pod has two containers: <code>main</code> and <code>sidecar</code>. It is failing
          and
          you need to inspect the previous instanceâ€™s logs.</p>
        <p class="mock-task"><strong>Task:</strong> Write the exact <code>kubectl logs</code> command to stream logs
          from the <strong>previous</strong> instance of the <code>sidecar</code> container in pod
          <code>myapp-pod</code> (namespace <code>default</code>).
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl logs myapp-pod -c sidecar --previous -f</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;"><code>-c sidecar</code> selects the container,
            <code>--previous</code> shows the crashed/previous instance, <code>-f</code> streams.
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 3: Static pod</h4>
        <p class="mock-context">The control plane uses static pods. You need to run a static pod that runs
          <code>busybox</code> with <code>sleep 3600</code>.
        </p>
        <p class="mock-task"><strong>Task:</strong> Create a static pod manifest (YAML) named
          <code>static-busybox</code> on the control plane node. Assume the static pod path is
          <code>/etc/kubernetes/manifests</code>. Provide the manifest path and key fields.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># File: /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command: ["sleep", "3600"]</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Copy this file to the control plane node at
            <code>/etc/kubernetes/manifests/</code>. Kubelet will create the pod. No <code>kubectl apply</code>
            needed.
          </p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 4: Service and endpoints</h4>
        <p class="mock-context">A service <code>web-svc</code> in namespace <code>prod</code> is not getting traffic.
          Pods are running with label <code>app=web</code>.</p>
        <p class="mock-task"><strong>Task:</strong> (1) Check if the service has endpoints. (2) If empty, fix the
          service so it targets pods with label <code>app=web</code>. Assume the service exists but has the wrong
          selector.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl get endpoints web-svc -n prod
kubectl get svc web-svc -n prod -o yaml   # inspect selector
# Fix: patch or edit so selector is app=web
kubectl patch svc web-svc -n prod -p '{"spec":{"selector":{"app":"web"}}}'
# Or kubectl edit svc web-svc -n prod and set spec.selector.app: web
kubectl get endpoints web-svc -n prod     # should list pod IPs now</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 5: etcd backup</h4>
        <p class="mock-context">You need to take a snapshot of etcd for disaster recovery. The cluster was set up with
          kubeadm; etcd runs as a pod and listens on <code>https://127.0.0.1:2379</code> with certs under
          <code>/etc/kubernetes/pki/etcd/</code>.
        </p>
        <p class="mock-task"><strong>Task:</strong> Run the <code>etcdctl snapshot save</code> command with the
          correct
          API version, endpoint, and cert flags. Save to <code>/tmp/etcd-snapshot.db</code>.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Run from the control plane node (or from inside the etcd
            pod).
            Verify with <code>ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-snapshot.db</code>.</p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 6: PV and PVC binding</h4>
        <p class="mock-context">You need to create a PersistentVolume and a PersistentVolumeClaim, then mount it in a
          pod.</p>
        <p class="mock-task"><strong>Task:</strong> Create a PV named <code>pv-log</code> with 100Mi capacity,
          accessMode <code>ReadWriteMany</code>, hostPath <code>/pv/log</code>. Create a PVC named
          <code>claim-log-1</code> requesting 50Mi with <code>ReadWriteMany</code>. Create a pod <code>logger</code>
          (image <code>nginx</code>) mounting the PVC at <code>/var/log/nginx</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  hostPath:
    path: /pv/log
---
# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
---
# pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: logger
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - mountPath: /var/log/nginx
      name: log-vol
  volumes:
  - name: log-vol
    persistentVolumeClaim:
      claimName: claim-log-1</code></pre>
          <pre><code>kubectl apply -f pv.yaml -f pvc.yaml -f pod.yaml
kubectl get pv,pvc,pod</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 7: NetworkPolicy â€” deny all ingress</h4>
        <p class="mock-context">Namespace <code>secure</code> exists with pods labeled <code>app=db</code>. You need
          to
          deny all ingress traffic except from pods labeled <code>app=api</code> on port 5432.</p>
        <p class="mock-task"><strong>Task:</strong> Create a NetworkPolicy in namespace <code>secure</code> that
          selects
          pods with label <code>app=db</code>, denies all ingress, and only allows ingress from pods with
          <code>app=api</code> on TCP port 5432.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
  namespace: secure
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 5432</code></pre>
          <pre><code>kubectl apply -f netpol.yaml
kubectl get networkpolicy -n secure
kubectl describe netpol db-policy -n secure</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 8: Cluster upgrade (kubeadm)</h4>
        <p class="mock-context">Your cluster runs v1.30.0. You need to upgrade the control plane to v1.31.0.</p>
        <p class="mock-task"><strong>Task:</strong> Upgrade kubeadm, then the control plane, then kubelet and kubectl
          on
          the control plane node. Show the commands in order.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># 1. Upgrade kubeadm
sudo apt-mark unhold kubeadm
sudo apt-get update && sudo apt-get install -y kubeadm=1.31.0-*
sudo apt-mark hold kubeadm

# 2. Plan and apply
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.31.0

# 3. Drain the node
kubectl drain &lt;control-plane-node&gt; --ignore-daemonsets

# 4. Upgrade kubelet and kubectl
sudo apt-mark unhold kubelet kubectl
sudo apt-get install -y kubelet=1.31.0-* kubectl=1.31.0-*
sudo apt-mark hold kubelet kubectl
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 5. Uncordon
kubectl uncordon &lt;control-plane-node&gt;
kubectl get nodes</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 9: Ingress with TLS</h4>
        <p class="mock-context">Service <code>webapp-svc</code> exists on port 80 in namespace <code>apps</code>. You
          have a TLS cert and key.</p>
        <p class="mock-task"><strong>Task:</strong> Create a TLS secret <code>webapp-tls</code> and an Ingress
          <code>webapp-ingress</code> that terminates TLS on host <code>webapp.example.com</code> and routes to
          <code>webapp-svc</code>.
        </p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># Create TLS secret
kubectl create secret tls webapp-tls --cert=tls.crt --key=tls.key -n apps

# Ingress YAML
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp-ingress
  namespace: apps
spec:
  tls:
  - hosts:
    - webapp.example.com
    secretName: webapp-tls
  rules:
  - host: webapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webapp-svc
            port:
              number: 80</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 10: JSONPath and sorting</h4>
        <p class="mock-context">You need to extract specific information from the cluster using JSONPath.</p>
        <p class="mock-task"><strong>Task:</strong> (1) List all node names sorted by CPU capacity. (2) Output a
          custom-columns table showing pod name and container image for all pods in all namespaces. (3) Get the
          InternalIP of all nodes.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># 1. Nodes sorted by CPU
kubectl get nodes --sort-by=.status.capacity.cpu

# 2. Custom columns: pod name + image
kubectl get pods -A -o=custom-columns='NAME:.metadata.name,IMAGE:.spec.containers[*].image'

# 3. InternalIP of all nodes
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'

# Alternative using range for formatted output:
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.addresses[?(@.type=="InternalIP")].address}{"\n"}{end}'</code></pre>
        </div>
      </div>

      <h3>Ultimate Mocks</h3>
      <p>Extended scenario-based mocks: multi-step troubleshooting, storage (PV/PVC), networking (NetworkPolicy,
        Ingress), and cluster upgrades. Use the same format: try the scenario in a playground, then compare with the
        solutions in the course docs or your notes.</p>
      <p><strong>Ultimate mock strategy:</strong> Set a 2-hour timer, attempt all questions sequentially. Flag hard
        questions and return to them. Prioritize high-weight questions (troubleshooting = 30%). Practice with
        Killer.sh
        (free retake included with CKA exam purchase).</p>

      <h3>Certification Tips</h3>
      <p>Set aliases, use imperative + dry-run, verify answers. Prioritize high-weight questions. Bookmark kubernetes.io/docs.</p>
      <h3>Tips and Tricks</h3>
      <h3>Exam environment setup (do this first!)</h3>
      <p>The CKA exam uses a browser-based terminal. Set up shortcuts immediately:</p>
      <div class="snippet-label">Shell aliases and settings (~/.bashrc)</div>
      <pre><code># Essential aliases â€” set these at the START of the exam
alias k=kubectl
alias kn='kubectl config set-context --current --namespace'
alias kgp='kubectl get pods'
alias kgs='kubectl get svc'
alias kgn='kubectl get nodes'
alias kga='kubectl get all'
alias kaf='kubectl apply -f'
alias kdp='kubectl describe pod'
alias kl='kubectl logs'
alias ke='kubectl exec -it'

# Enable kubectl autocompletion
source <(kubectl completion bash)
complete -o default -F __start_kubectl k

# Set default editor
export KUBE_EDITOR=vi
# or
export EDITOR=vi</code></pre>
      <div class="snippet-label">Vim settings (~/.vimrc)</div>
      <pre><code># Paste this into ~/.vimrc for YAML editing
set tabstop=2
set shiftwidth=2
set expandtab
set number
set autoindent</code></pre>

      <h3>Time management</h3>
      <ul>
        <li><strong>2 hours, ~17 questions.</strong> Average ~7 minutes per question.</li>
        <li><strong>Don't get stuck:</strong> Flag difficult questions, move on, come back.</li>
        <li><strong>High-weight first:</strong> Prioritize questions worth more marks.</li>
        <li><strong>Use imperative commands:</strong> Faster than writing YAML from scratch.</li>
        <li><strong>Bookmark the docs:</strong> Kubernetes.io docs are allowed. Pre-bookmark key pages.</li>
      </ul>

      <h3>Essential bookmarks for the exam</h3>
      <ul>
        <li><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/" target="_blank" rel="noopener">kubectl
            Cheat Sheet</a></li>
        <li><a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">Concepts</a> â†’ Workloads,
          Services, Storage, Config</li>
        <li><a href="https://kubernetes.io/docs/tasks/" target="_blank" rel="noopener">Tasks</a> â†’ Administer a
          Cluster,
          Manage TLS, Configure Pods</li>
        <li><a href="https://kubernetes.io/docs/reference/" target="_blank" rel="noopener">API Reference</a></li>
      </ul>

      <h3>Test pods for networking</h3>
      <p>Run a temporary pod for DNS/connectivity tests:
        <code>kubectl run dnsutils --image=busybox:1.28 --restart=Never --rm -it -- nslookup kubernetes.default.svc.cluster.local</code>.
        For a long-lived debug pod:
        <code>kubectl run debug --image=nicolaka/netshoot -it --rm --restart=Never -- bash</code>.
      </p>

      <h3>Useful one-liners</h3>
      <pre><code># All pods with node info
kubectl get pods -A -o wide

# Events sorted by time
kubectl get events -A --sort-by='.lastTimestamp'

# Node resource usage
kubectl describe node | grep -A5 "Allocated resources"

# Set default namespace
kubectl config set-context --current --namespace=&lt;ns&gt;

# Find which node a pod runs on
kubectl get pod &lt;name&gt; -o jsonpath='{.spec.nodeName}'

# Watch pods (live updates)
kubectl get pods -w

# Delete a pod stuck in Terminating
kubectl delete pod &lt;name&gt; --force --grace-period=0

# Get all images running in the cluster
kubectl get pods -A -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort | uniq

# Check component statuses
kubectl get componentstatuses   # may be deprecated
kubectl get --raw='/readyz?verbose'

# Quick pod for testing
kubectl run tmp --image=busybox:1.28 --rm -it --restart=Never -- sh

# Decode a secret
kubectl get secret &lt;name&gt; -o jsonpath='{.data.password}' | base64 --decode

# Find static pod manifests path
ps aux | grep kubelet | grep config
grep staticPodPath /var/lib/kubelet/config.yaml</code></pre>

      <h3>What's Next</h3>
      <p>After CKA: CKAD (application development), CKS (security). Practice on Killer.sh, KillerCoda, or your own cluster.</p>
      <h3>Common exam mistakes to avoid</h3>
      <ul>
        <li>âŒ Forgetting to switch context (<code>kubectl config use-context &lt;context&gt;</code>) â€” each question
          may
          use a different cluster</li>
        <li>âŒ Wrong namespace â€” always check the question for namespace and use <code>-n &lt;ns&gt;</code></li>
        <li>âŒ YAML indentation errors â€” use <code>kubectl apply -f file.yaml</code> and read the error</li>
        <li>âŒ Not verifying the answer â€” always run <code>kubectl get</code> / <code>kubectl describe</code> to
          confirm
        </li>
        <li>âŒ Spending too long on one question â€” flag it and move on</li>
        <li>âŒ Forgetting <code>--dry-run=client -o yaml</code> â€” the fastest way to generate templates</li>
      </ul>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 16 Practice &amp; Exam Preparation</h3>
        <script src="https://utteranc.es/client.js" repo="SKCloudOps/k8s" issue-term="CKA Guide - 16 Practice & Exam" theme="github-light" crossorigin="anonymous" async></script>
      </section>

      <!-- 17 Cheat Sheet -->
      <h2 id="section-17">17 â€” CKA Exam Cheat Sheet</h2>
      <p>Quick-reference cards for every exam domain. The full cheat sheet is on a dedicated page so you can <strong>print it</strong> and use it as a single exam-day reference.</p>
      <p><a href="cheat-sheet.html" class="lab-link-btn" style="display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.6rem 1rem; background: var(--accent); color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">ðŸ“„ Open Cheat Sheet</a> â€” All commands and YAML patterns. Use <strong>Print Cheat Sheet</strong> on that page; the printed copy is designed to be enough for the CKA exam.</p>
      <p style="margin-top: 0.5rem;"><a href="cheat-sheet.html">cheat-sheet.html</a></p>

      <section class="comments-section" aria-label="Comments">
        <h3 class="comments-title">Comments â€” 17 Cheat Sheet</h3>
        <script src="https://utteranc.es/client.js"
                repo="SKCloudOps/k8s"
                issue-term="CKA Guide - 17 Cheat Sheet"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
      </section>

    </main>

  </div>
  <footer class="site-footer">
    CKA Study Guide â€” Reference for Certified Kubernetes Administrator exam preparation.
    &nbsp;Â·&nbsp; <span id="footer-stats"></span>
  </footer>

  <!-- Reading Progress Bar -->
  <div id="reading-progress"></div>

  <!-- Toast Container -->
  <div id="toast-container"></div>

  <!-- Search Overlay -->
  <div id="search-overlay" role="dialog" aria-modal="true" aria-label="Search">
    <div id="search-box">
      <div id="search-input-wrap">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
          stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"></circle>
          <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
        <input type="text" id="search-input" placeholder="Search topics, commands, conceptsâ€¦" autocomplete="off"
          spellcheck="false">
        <span id="search-shortcut-hint">ESC to close</span>
      </div>
      <div id="search-results">
        <div id="search-empty">Type to search across all sectionsâ€¦</div>
      </div>
      <div id="search-footer">
        <span><kbd>â†‘â†“</kbd> navigate</span>
        <span><kbd>â†µ</kbd> open</span>
        <span><kbd>ESC</kbd> close</span>
      </div>
    </div>
  </div>

  <!-- Keyboard Shortcuts Overlay -->
  <div id="shortcuts-overlay" role="dialog" aria-modal="true">
    <div id="shortcuts-box">
      <h2>âŒ¨ï¸ Keyboard Shortcuts</h2>
      <div class="shortcut-grid">
        <div class="shortcut-row"><span class="sc-desc">Search</span><span class="sc-keys"><kbd
              class="sc-key">Ctrl</kbd><kbd class="sc-key">K</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Dark Mode</span><span class="sc-keys"><kbd
              class="sc-key">D</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Next Section</span><span class="sc-keys"><kbd
              class="sc-key">â†’</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Prev Section</span><span class="sc-keys"><kbd
              class="sc-key">â†</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Notes Panel</span><span class="sc-keys"><kbd
              class="sc-key">N</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">This panel</span><span class="sc-keys"><kbd
              class="sc-key">?</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Mark Complete</span><span class="sc-keys"><kbd
              class="sc-key">C</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Bookmark Section</span><span class="sc-keys"><kbd
              class="sc-key">B</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Font Bigger</span><span class="sc-keys"><kbd
              class="sc-key">+</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Font Smaller</span><span class="sc-keys"><kbd
              class="sc-key">-</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Pomodoro Timer</span><span class="sc-keys"><kbd
              class="sc-key">T</kbd></span></div>
        <div class="shortcut-row"><span class="sc-desc">Close / Back</span><span class="sc-keys"><kbd
              class="sc-key">ESC</kbd></span></div>
      </div>
      <div id="shortcuts-close-hint">Press <kbd class="sc-key">ESC</kbd> or click outside to close</div>
    </div>
  </div>

  <!-- Notes Slide-in Panel -->
  <div id="notes-panel" role="complementary" aria-label="Section Notes">
    <div id="notes-header">
      <h3>ðŸ“ Section Notes</h3>
      <button id="notes-close" title="Close notes">Ã—</button>
    </div>
    <div id="notes-section-name">No section selected</div>
    <textarea id="notes-textarea"
      placeholder="Write your notes for this sectionâ€¦ They're saved automatically."></textarea>
    <div id="notes-footer">
      <span id="notes-char-count">0 characters</span>
      <button id="notes-export-btn" title="Export all notes as Markdown">â¬‡ Export</button>
      <button id="notes-save-btn">Save âœ“</button>
    </div>
  </div>



  <!-- Zen Mode Hint -->
  <div id="zen-hint">Press <kbd>Z</kbd> or move mouse to top to show header &nbsp;Â·&nbsp; Press <kbd>Z</kbd> again to
    exit Zen Mode</div>

  <!-- CKA Quiz Overlay -->
  <div id="quiz-overlay" role="dialog" aria-modal="true" aria-label="CKA Quiz">
    <div id="quiz-box">
      <div id="quiz-header">
        <h2>âš¡ CKA Quick Quiz</h2>
        <button id="quiz-close" title="Close quiz">Ã—</button>
      </div>
      <div id="quiz-progress-bar">
        <div id="quiz-progress-fill" style="width:0%"></div>
      </div>
      <div id="quiz-question"></div>
      <div id="quiz-options"></div>
      <div id="quiz-explanation"></div>
      <div id="quiz-nav">
        <span id="quiz-score">Question 1 of 15</span>
        <button id="quiz-next">Next â†’</button>
      </div>
      <div id="quiz-results">
        <div class="result-score" id="quiz-final-score"></div>
        <p id="quiz-result-msg"></p>
        <button id="quiz-restart">ðŸ”„ Try Again</button>
      </div>
    </div>
  </div>

  <!-- Diagram zoom lightbox -->
  <div id="diagram-zoom-overlay" class="diagram-zoom-overlay" aria-hidden="true">
    <div class="zoom-container">
      <img id="diagram-zoom-img" src="" alt="">
    </div>
    <div class="diagram-zoom-controls">
      <button type="button" id="diagram-zoom-in" title="Zoom in">+</button>
      <button type="button" id="diagram-zoom-out" title="Zoom out">âˆ’</button>
      <button type="button" id="diagram-zoom-reset" title="Reset zoom">âŸ²</button>
      <button type="button" id="diagram-zoom-close" class="zoom-close" title="Close">Close</button>
    </div>
  </div>

  <script src="script.js"></script>
</body>

</html>
