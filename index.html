<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CKA Exam Study Guide | Certified Kubernetes Administrator (CKA) Preparation &amp; Cheat Sheet</title>
  <meta name="description" content="Free CKA study guide: cluster architecture, scheduling, networking, storage, RBAC, troubleshooting. Commands, YAML examples &amp; diagrams for Certified Kubernetes Administrator exam.">
  <meta name="keywords" content="CKA, Certified Kubernetes Administrator, Kubernetes certification, CKA exam, CKA study guide, CKA preparation, Kubernetes admin, kubectl, K8s certification">
  <meta name="author" content="CKA Study Guide">
  <meta name="robots" content="index, follow">
  <!-- Replace with your real page URL when you publish the site -->
  <link rel="canonical" href="https://example.com/cka-preparation.html">
  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator Preparation">
  <meta property="og:description" content="Free CKA study guide with commands, YAML examples and diagrams. Covers cluster architecture, scheduling, networking, storage, security and troubleshooting for the CKA exam.">
  <meta property="og:url" content="https://example.com/cka-preparation.html">
  <meta property="og:locale" content="en_US">
  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CKA Exam Study Guide | Certified Kubernetes Administrator">
  <meta name="twitter:description" content="Free CKA study guide: commands, YAML snippets and diagrams for Certified Kubernetes Administrator (CKA) exam preparation.">
  <meta name="theme-color" content="#ffffff">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Outfit:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #ffffff;
      --surface: #f6f8fa;
      --surface2: #eaeef2;
      --text: #1f2328;
      --text-muted: #656d76;
      --accent: #1a7f37;
      --accent2: #0969da;
      --border: #d0d7de;
      --code-bg: #f6f8fa;
      --nav-width: 260px;
      --content-max: 720px;
      --header-h: 56px;
    }
    *, *::before, *::after { box-sizing: border-box; }
    html { font-size: 16px; }
    /* Content blocks: only one open at a time */
    .content-block {
      display: none;
      overflow: hidden;
      animation: blockOpen 0.4s ease-out;
    }
    .content-block.is-open {
      display: block;
    }
    @keyframes blockOpen {
      from {
        opacity: 0;
        transform: translateY(-10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    body {
      margin: 0;
      font-family: 'Outfit', -apple-system, BlinkMacSystemFont, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.65;
      -webkit-font-smoothing: antialiased;
    }
    .site-header {
      position: sticky;
      top: 0;
      z-index: 100;
      height: var(--header-h);
      background: var(--surface);
      border-bottom: 1px solid var(--border);
      display: flex;
      align-items: center;
      padding: 0 1.25rem;
    }
    .site-header h1 {
      margin: 0;
      font-size: clamp(1rem, 2.5vw, 1.25rem);
      font-weight: 600;
      color: var(--text);
    }
    .site-header .badge {
      margin-left: 0.5rem;
      font-size: 0.7rem;
      padding: 0.2em 0.5em;
      background: var(--accent);
      color: var(--bg);
      border-radius: 4px;
      font-weight: 600;
    }
    .nav-toggle {
      display: none;
      margin-left: auto;
      padding: 0.5rem;
      background: transparent;
      border: 1px solid var(--border);
      border-radius: 6px;
      color: var(--text);
      cursor: pointer;
      font-size: 1.25rem;
    }
    .layout { display: flex; min-height: calc(100vh - var(--header-h)); }
    nav {
      position: sticky;
      top: var(--header-h);
      width: var(--nav-width);
      min-width: var(--nav-width);
      height: calc(100vh - var(--header-h));
      overflow-y: auto;
      background: var(--surface);
      border-right: 1px solid var(--border);
      padding: 0.75rem 0;
      flex-shrink: 0;
    }
    nav a {
      display: block;
      padding: 0.5rem 1rem 0.5rem 1.25rem;
      color: var(--text-muted);
      text-decoration: none;
      font-size: clamp(0.8rem, 1.5vw, 0.9rem);
      border-left: 3px solid transparent;
      transition: color 0.15s, background 0.15s;
    }
    nav a:hover { color: var(--accent2); background: var(--surface2); }
    nav .nav-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 0.75rem 1rem 0.5rem;
      border-bottom: 1px solid var(--border);
      margin-bottom: 0.25rem;
    }
    nav .nav-section {
      font-size: 0.7rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--accent);
      padding: 0;
      margin: 0;
    }
    .nav-close {
      display: none;
      padding: 0.35rem 0.6rem;
      font-size: 1.4rem;
      line-height: 1;
      background: transparent;
      border: 1px solid var(--border);
      border-radius: 6px;
      color: var(--text-muted);
      cursor: pointer;
      font-family: inherit;
    }
    .nav-close:hover { color: var(--text); background: var(--surface2); }
    main {
      flex: 1;
      min-width: 0;
      padding: clamp(1.25rem, 4vw, 2.5rem) clamp(1.5rem, 5vw, 3rem) 4rem;
      max-width: var(--content-max);
    }
    .page-title { font-size: clamp(1.5rem, 4vw, 2rem); font-weight: 700; margin: 0 0 0.35rem; color: var(--text); }
    .subtitle { color: var(--text-muted); margin-bottom: 2rem; font-size: clamp(0.9rem, 1.5vw, 1.05rem); line-height: 1.5; }
    h2 {
      font-size: clamp(1.25rem, 2.5vw, 1.5rem);
      font-weight: 600;
      margin: 2.25rem 0 0.85rem;
      padding-bottom: 0.5rem;
      border-bottom: 1px solid var(--border);
      color: var(--accent);
      scroll-margin-top: calc(var(--header-h) + 0.5rem);
    }
    h3 { font-size: clamp(1.05rem, 2vw, 1.2rem); font-weight: 600; margin: 1.5rem 0 0.6rem; color: var(--text); }
    h4 { font-size: 1rem; margin: 1rem 0 0.4rem; color: var(--text-muted); font-weight: 500; }
    p { margin: 0.65rem 0; max-width: 70ch; }
    ul, ol { margin: 0.65rem 0; padding-left: 1.5rem; max-width: 70ch; }
    li { margin: 0.3rem 0; }
    pre, code { font-family: 'JetBrains Mono', 'Consolas', monospace; background: var(--code-bg); border-radius: 6px; border: 1px solid var(--border); }
    code { padding: 0.2em 0.4em; font-size: 0.875em; }
    pre {
      padding: 1rem;
      overflow-x: auto;
      margin: 1rem 0;
      font-size: clamp(0.8rem, 1.2vw, 0.85rem);
      max-width: 100%;
    }
    pre code { padding: 0; background: none; border: none; }
    .card { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 1.25rem; margin: 1rem 0; max-width: 70ch; }
    .snippet-label { font-size: 0.75rem; color: var(--text-muted); margin-bottom: 0.35rem; text-transform: uppercase; letter-spacing: 0.04em; }
    .diagram {
      margin: 1.5rem 0;
      padding: 1.5rem;
      border: 1px solid var(--border);
      border-radius: 10px;
      background: var(--surface);
      text-align: center;
    }
    .diagram svg {
      display: block;
      max-width: 100%;
      height: auto;
      margin: 0 auto;
    }
    .diagram figcaption {
      font-size: 0.8rem;
      color: var(--text-muted);
      margin-top: 0.75rem;
      padding-top: 0.75rem;
      border-top: 1px solid var(--border);
      text-align: center;
      max-width: 56ch;
      margin-left: auto;
      margin-right: auto;
    }
    .step-list { list-style: none; padding-left: 0; max-width: 70ch; }
    .step-list li { position: relative; padding-left: 1.5rem; margin: 0.5rem 0; }
    .step-list li::before { content: "→"; position: absolute; left: 0; color: var(--accent); font-weight: 600; }
    .site-footer {
      padding: 1rem 1.5rem;
      border-top: 1px solid var(--border);
      background: var(--surface);
      font-size: 0.8rem;
      color: var(--text-muted);
      text-align: center;
    }
    @media (max-width: 992px) {
      :root { --content-max: 100%; }
      main { padding-left: 1.5rem; padding-right: 1.5rem; }
    }
    @media (max-width: 768px) {
      .nav-toggle { display: block; }
      .nav-close { display: block; }
      nav {
        position: fixed;
        top: var(--header-h);
        left: 0;
        right: 0;
        height: auto;
        max-height: 70vh;
        z-index: 99;
        transform: translateY(-100%);
        opacity: 0;
        pointer-events: none;
        transition: transform 0.2s, opacity 0.2s;
        border-right: none;
        border-bottom: 1px solid var(--border);
        box-shadow: 0 8px 24px rgba(0,0,0,0.3);
      }
      nav.is-open {
        transform: translateY(0);
        opacity: 1;
        pointer-events: auto;
      }
      .layout { flex-direction: column; }
      main { padding: 1.25rem 1rem 3rem; }
      p, ul, ol, .card { max-width: none; }
    }
    @media (max-width: 576px) {
      .site-header { padding: 0 0.75rem; }
      main { padding: 1rem 0.75rem 2rem; }
      pre { padding: 0.75rem; font-size: 0.8rem; }
      .diagram { padding: 1rem; }
    }
    @media print {
      .site-header, nav, .nav-toggle, .site-footer { display: none !important; }
      body { background: #fff; color: #111; }
      main { max-width: none; padding: 0; }
      .diagram { break-inside: avoid; }
    }
    /* Mock exam & playground */
    .mock-question, .playground-scenario {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1.25rem 1.5rem;
      margin: 1rem 0;
    }
    .mock-question h4, .playground-scenario h4 {
      margin: 0 0 0.5rem;
      font-size: 1.05rem;
      color: var(--accent2);
    }
    .mock-context, .playground-context { color: var(--text-muted); margin: 0.5rem 0; }
    .mock-task, .playground-task { margin: 0.75rem 0; font-weight: 500; }
    .solution-toggle, .playground-toggle {
      display: inline-block;
      margin: 0.75rem 0 0;
      padding: 0.4rem 0.9rem;
      font-size: 0.9rem;
      font-family: inherit;
      font-weight: 600;
      color: var(--accent);
      background: var(--surface2);
      border: 1px solid var(--border);
      border-radius: 6px;
      cursor: pointer;
      transition: background 0.15s, color 0.15s;
    }
    .solution-toggle:hover, .playground-toggle:hover {
      background: var(--border);
      color: var(--text);
    }
    .mock-solution, .playground-solution {
      display: none;
      margin-top: 1rem;
      padding-top: 1rem;
      border-top: 1px dashed var(--border);
    }
    .mock-solution.is-open, .playground-solution.is-open { display: block; }
    .mock-solution pre, .playground-solution pre { margin: 0.5rem 0 0; }
    .playground-links {
      list-style: none;
      padding: 0;
      margin: 1rem 0;
    }
    .playground-links li {
      margin: 0.75rem 0;
      padding: 0.75rem 1rem;
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 8px;
    }
    .playground-links a {
      font-weight: 600;
      color: var(--accent2);
      text-decoration: none;
    }
    .playground-links a:hover { text-decoration: underline; }
    .playground-links .link-desc { font-size: 0.9rem; color: var(--text-muted); margin-top: 0.25rem; }
  </style>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "LearningResource",
    "name": "CKA Exam Study Guide | Certified Kubernetes Administrator Preparation",
    "description": "Free study guide for the Certified Kubernetes Administrator (CKA) exam. Covers cluster architecture, scheduling, workloads, networking, storage, security, and troubleshooting with commands, YAML examples and diagrams.",
    "learningResourceType": "Study Guide",
    "educationalLevel": "Professional",
    "teaches": ["Kubernetes", "Container orchestration", "kubectl", "CKA certification", "Cluster administration"],
    "about": [
      { "@type": "Thing", "name": "Certified Kubernetes Administrator" },
      { "@type": "Thing", "name": "Kubernetes" }
    ]
  }
  </script>
</head>
<body>
  <header class="site-header">
    <h1>CKA Study Guide</h1>
    <span class="badge">Certified Kubernetes Administrator</span>
    <button type="button" class="nav-toggle" aria-label="Open menu" id="nav-toggle">☰</button>
  </header>
  <div class="layout">
    <nav id="nav" aria-label="Section navigation">
      <div class="nav-header">
        <span class="nav-section">CKA Preparation</span>
        <button type="button" class="nav-close" aria-label="Close menu" id="nav-close">×</button>
      </div>
      <a href="#intro">Overview</a>
      <a href="#section-01">01 Introduction</a>
      <a href="#section-02">02 Core Concepts</a>
      <a href="#section-03">03 Scheduling</a>
      <a href="#section-04">04 Logging & Monitoring</a>
      <a href="#section-05">05 Application Lifecycle</a>
      <a href="#section-06">06 Cluster Maintenance</a>
      <a href="#section-07">07 Security</a>
      <a href="#section-08">08 Storage</a>
      <a href="#section-09">09 Networking</a>
      <a href="#section-10">10 Design & Install</a>
      <a href="#section-11">11 kubeadm</a>
      <a href="#section-12">12 Troubleshooting</a>
      <a href="#section-13">13 Other Topics</a>
      <a href="#section-14">14 Lightning Labs</a>
      <a href="#section-15">15 Mock Exams</a>
      <a href="#section-16">16 Ultimate Mocks</a>
      <a href="#section-17">17 Tips & Tricks</a>
    </nav>
    <main>

      <header id="intro">
        <h1 class="page-title">Certified Kubernetes Administrator (CKA)</h1>
        <p class="subtitle">Detailed standalone reference for CKA exam preparation—enough to prepare without other materials. Use the sidebar to jump to any section. All key concepts, commands, YAML snippets, and diagrams are included.</p>
      </header>

      <!-- 01 Introduction -->
      <h2 id="section-01">01 — Introduction</h2>
      <h3>Course focus</h3>
      <p>This material focuses on the <strong>administration</strong> part of Kubernetes, aligned with the CKA exam.</p>
      <h4>Pre-requisites</h4>
      <ul>
        <li>Docker</li>
        <li>Basics of Kubernetes (Pods, Deployments, Services)</li>
        <li>YAML</li>
        <li>Basic lab environment (e.g. VirtualBox)</li>
      </ul>
      <h4>Exam areas (weighted)</h4>
      <ul>
        <li><strong>Cluster Architecture, Installation & Validation</strong> (25%): Design, install, configure, validate, manage HA, etcd, upgrade.</li>
        <li><strong>Workloads & Scheduling</strong> (15%): Deployments, scaling, DaemonSets, resource limits, scheduling (manual, node selector, affinity, taints/tolerations), static pods.</li>
        <li><strong>Services & Networking</strong> (20%): Networking model, Services (ClusterIP, NodePort, LoadBalancer), DNS, CNI, Ingress.</li>
        <li><strong>Storage</strong> (10%): PV, PVC, StorageClass, volume mounts.</li>
        <li><strong>Troubleshooting</strong> (30%): Application, control plane, worker nodes, network.</li>
        <li>Plus: Security (RBAC, TLS, kubeconfig, NetworkPolicies), Logging/Monitoring, Application Lifecycle (rollouts, ConfigMaps, Secrets).</li>
        <li><strong>2025 curriculum additions:</strong> Helm and Kustomize for cluster components; Gateway API (with Ingress); CRDs and operators; extension interfaces (CNI, CSI, CRI); CoreDNS; workload autoscaling (HPA); PodDisruptionBudgets. See sections 05, 06, 09, and 13.</li>
      </ul>
      <h3>Certification</h3>
      <p>Certified Kubernetes Administrator (CKA): exam curriculum, candidate handbook, and exam tips are published by the Linux Foundation / CNCF. Use code <code>KUBERNETES15</code> when registering for CKA or CKAD for a 15% discount.</p>

      <!-- 02 Core Concepts -->
      <h2 id="section-02">02 — Core Concepts</h2>
      <h3>Cluster architecture</h3>
      <p>A Kubernetes cluster has a <strong>control plane</strong> (master) and one or more <strong>worker nodes</strong>. The control plane runs the API server, etcd, scheduler, and controller manager. Workers run kubelet, kube-proxy, and the container runtime (e.g. containerd). <strong>Only the API server talks to etcd</strong>; all other components (scheduler, controller manager, kubelet) talk to the cluster through the API server.</p>
      <figure class="diagram">
        <svg viewBox="0 0 700 320" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <defs>
            <marker id="arrow" markerWidth="8" markerHeight="8" refX="6" refY="4" orient="auto">
              <path d="M0 0 L8 4 L0 8 Z" fill="#0969da"/>
            </marker>
            <marker id="arrow-green" markerWidth="8" markerHeight="8" refX="6" refY="4" orient="auto">
              <path d="M0 0 L8 4 L0 8 Z" fill="#1a7f37"/>
            </marker>
          </defs>
          <!-- Title -->
          <text x="350" y="28" fill="#656d76" font-size="16" font-weight="600" text-anchor="middle">KUBERNETES CLUSTER</text>
          <!-- kubectl / client -->
          <rect x="280" y="44" width="140" height="36" rx="6" fill="#eaeef2" stroke="#0969da" stroke-width="1.5"/>
          <text x="350" y="66" fill="#1f2328" font-size="15" font-weight="600" text-anchor="middle">kubectl / clients</text>
          <line x1="350" y1="80" x2="350" y2="108" stroke="#0969da" stroke-width="2" marker-end="url(#arrow)"/>
          <text x="350" y="98" fill="#656d76" font-size="13" text-anchor="middle">HTTPS</text>
          <!-- Control plane box -->
          <rect x="40" y="118" width="620" height="100" rx="8" fill="#f6f8fa" stroke="#d0d7de" stroke-width="1.5"/>
          <text x="350" y="142" fill="#1a7f37" font-size="15" font-weight="700" text-anchor="middle">CONTROL PLANE (Master)</text>
          <!-- API Server (highlighted - only one that talks to etcd) -->
          <rect x="80" y="154" width="100" height="52" rx="6" fill="#ddf4ff" stroke="#0969da" stroke-width="1.5"/>
          <text x="130" y="178" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">kube-</text>
          <text x="130" y="192" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">apiserver</text>
          <text x="130" y="202" fill="#656d76" font-size="11" text-anchor="middle">(only → etcd)</text>
          <!-- Arrow API to etcd -->
          <line x1="180" y1="180" x2="228" y2="180" stroke="#0969da" stroke-width="2" marker-end="url(#arrow)"/>
          <!-- etcd -->
          <rect x="228" y="154" width="80" height="52" rx="6" fill="#fff8c5" stroke="#9a6700" stroke-width="1.5"/>
          <text x="268" y="180" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">etcd</text>
          <text x="268" y="198" fill="#656d76" font-size="11" text-anchor="middle">state store</text>
          <!-- Scheduler -->
          <rect x="340" y="154" width="88" height="52" rx="6" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="384" y="182" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">scheduler</text>
          <text x="384" y="198" fill="#656d76" font-size="11" text-anchor="middle">assigns node</text>
          <!-- Controller manager -->
          <rect x="458" y="154" width="88" height="52" rx="6" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="502" y="178" fill="#1f2328" font-size="12" font-weight="600" text-anchor="middle">controller</text>
          <text x="502" y="192" fill="#1f2328" font-size="12" font-weight="600" text-anchor="middle">manager</text>
          <!-- Dashed: scheduler &amp; controller use API server (not etcd) -->
          <path d="M384 154 L384 118 L130 118 L130 154" stroke="#656d76" stroke-width="1" stroke-dasharray="4 3" fill="none"/>
          <text x="255" y="112" fill="#656d76" font-size="11" text-anchor="middle">use API server</text>
          <!-- Worker nodes -->
          <rect x="40" y="238" width="300" height="72" rx="8" fill="#f6f8fa" stroke="#d0d7de" stroke-width="1.5"/>
          <text x="190" y="262" fill="#1a7f37" font-size="14" font-weight="700" text-anchor="middle">WORKER NODE 1</text>
          <rect x="56" y="272" width="88" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="100" y="292" fill="#1f2328" font-size="13" font-weight="600" text-anchor="middle">kubelet</text>
          <rect x="156" y="272" width="88" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="200" y="292" fill="#1f2328" font-size="13" font-weight="600" text-anchor="middle">kube-proxy</text>
          <rect x="256" y="272" width="72" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="292" y="292" fill="#1f2328" font-size="12" font-weight="600" text-anchor="middle">runtime</text>
          <rect x="360" y="238" width="300" height="72" rx="8" fill="#f6f8fa" stroke="#d0d7de" stroke-width="1.5"/>
          <text x="510" y="262" fill="#1a7f37" font-size="14" font-weight="700" text-anchor="middle">WORKER NODE 2</text>
          <rect x="376" y="272" width="88" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="420" y="292" fill="#1f2328" font-size="13" font-weight="600" text-anchor="middle">kubelet</text>
          <rect x="476" y="272" width="88" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="520" y="292" fill="#1f2328" font-size="13" font-weight="600" text-anchor="middle">kube-proxy</text>
          <rect x="576" y="272" width="72" height="32" rx="4" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="612" y="292" fill="#1f2328" font-size="12" font-weight="600" text-anchor="middle">Pods</text>
          <!-- Arrows from control plane to workers -->
          <line x1="350" y1="218" x2="190" y2="238" stroke="#0969da" stroke-width="1.5" marker-end="url(#arrow)"/>
          <line x1="350" y1="218" x2="510" y2="238" stroke="#0969da" stroke-width="1.5" marker-end="url(#arrow)"/>
          <text x="350" y="232" fill="#656d76" font-size="11" text-anchor="middle">API</text>
        </svg>
        <figcaption>Cluster layout: only the API server talks to etcd. Scheduler and controller manager use the API server. kubectl and other clients talk only to the API server. Workers run kubelet, kube-proxy, and container runtime (Pods).</figcaption>
      </figure>

      <h4>Control-plane components (kubeadm)</h4>
      <p>With <strong>kubeadm</strong>, API server, etcd, scheduler, and controller manager run as <strong>static pods</strong> in <code>kube-system</code>. Manifest files live in <code>/etc/kubernetes/manifests/</code>. To inspect:</p>
      <pre><code>kubectl get pods -n kube-system
cat /etc/kubernetes/manifests/kube-apiserver.yaml
cat /etc/kubernetes/manifests/kube-scheduler.yaml
ps -aux | grep kube-apiserver</code></pre>

      <h4>kube-apiserver</h4>
      <p>Primary component. <strong>Authenticates and validates</strong> requests, <strong>retrieves and updates</strong> data in etcd. It is the <strong>only</strong> component that talks to etcd. Scheduler, controller manager, and kubelet all interact with the cluster via the API server.</p>

      <h4>etcd</h4>
      <p>Distributed <strong>key-value store</strong>; holds all cluster state. Listens on port <strong>2379</strong>. Use <code>etcdctl</code> to read/write: <code>etcdctl put key1 value1</code>, <code>etcdctl get key1</code>. With kubeadm, etcd runs as a pod; use <code>kubectl logs etcd-master -n kube-system</code> or <code>journalctl -u etcd.service</code> on manual setups.</p>

      <h4>kube-scheduler</h4>
      <p>Responsible only for <strong>deciding which node</strong> a pod runs on. It does <strong>not</strong> place the pod on the node—that is done by <strong>kubelet</strong>. Scheduler watches for unscheduled pods and assigns a node by setting <code>nodeName</code> (or creating a Binding).</p>

      <h4>kube-controller-manager</h4>
      <p>Runs <strong>controllers</strong> that watch cluster state and drive it toward the desired state. Examples: <strong>Node Controller</strong> (node health), <strong>Replication Controller</strong> (keep desired pod count), Deployment, Endpoints, Service Account, and others. All run in one process.</p>

      <h4>kubelet</h4>
      <p>Runs on each worker (and often on control-plane nodes). <strong>Sole point of contact</strong> for the cluster on that node: creates and runs pods, reports status. Scheduler only assigns; kubelet actually starts the containers. Not deployed by kubeadm by default on workers—you install it when joining nodes.</p>

      <h4>kube-proxy</h4>
      <p>Runs on every node. Implements <strong>Service</strong> semantics: maintains network rules (iptables or IPVS) so that traffic to a Service IP is forwarded to backend pods. With kubeadm it usually runs as a <strong>DaemonSet</strong> in <code>kube-system</code>.</p>

      <h3>Pods</h3>
      <p>Kubernetes does <strong>not</strong> run containers directly on the node. The smallest deployable unit is a <strong>Pod</strong>. A pod wraps one or more containers that share the same network namespace (same IP) and can share volumes. Typically one main application container per pod; multi-container pods are used for sidecars (e.g. log agent) or init tasks. Containers in a pod are usually not multiple instances of the same image.</p>
      <div class="snippet-label">Run a pod (imperative)</div>
      <pre><code>kubectl run nginx --image nginx
kubectl get pods</code></pre>

      <h3>Pod definition (YAML)</h3>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-container
    image: nginx</code></pre>
      <pre><code>kubectl create -f pod-definition.yaml</code></pre>

      <h3>ReplicaSets</h3>
      <p>ReplicaSet maintains a stable set of replica pods. It requires a <code>selector</code> (matchLabels). ReplicationController is the older form.</p>
      <div class="snippet-label">ReplicaSet definition</div>
      <pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx</code></pre>
      <pre><code>kubectl create -f replicaset-definition.yaml
kubectl get replicaset
kubectl get pods</code></pre>
      <p>Scale: <code>kubectl scale --replicas=6 -f replicaset-definition.yaml</code> or <code>kubectl scale --replicas=6 replicaset myapp-replicaset</code>. Or edit YAML and <code>kubectl apply -f replicaset-definition.yaml</code>.</p>

      <h3>Deployments</h3>
      <p>Deployment manages ReplicaSets and Pods. Use it for declarative updates and rollouts.</p>
      <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx</code></pre>
      <pre><code>kubectl create -f deployment-definition.yaml
kubectl get deployment
kubectl get replicaset
kubectl get pods
kubectl get all</code></pre>
      <p>Imperative create: <code>kubectl create deployment nginx --image=nginx</code>.</p>

      <h3>Namespaces</h3>
      <p>Objects live in a namespace (default is <code>default</code>). List pods in a namespace: <code>kubectl get pods --namespace=kube-system</code> or <code>-n kube-system</code>. Create in a namespace: <code>kubectl create -f pod-definition.yaml --namespace=dev</code>. Add <code>namespace: dev</code> under <code>metadata</code> in the YAML to fix the namespace.</p>
      <div class="snippet-label">Create namespace</div>
      <pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev</code></pre>
      <pre><code>kubectl create -f namespace-dev.yaml
# or
kubectl create namespace dev</code></pre>
      <p>Switch default namespace: <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>. All namespaces: <code>kubectl get pods --all-namespaces</code>.</p>
      <div class="snippet-label">ResourceQuota (limit resources in a namespace)</div>
      <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi</code></pre>
      <pre><code>kubectl create -f compute-quota.yaml</code></pre>

      <h3>Services</h3>
      <p>Three types: <strong>NodePort</strong> (expose on node port), <strong>ClusterIP</strong> (virtual IP inside cluster), <strong>LoadBalancer</strong> (cloud load balancer).</p>
      <div class="snippet-label">NodePort service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  selector:
    app: myapp
    type: front-end</code></pre>
      <div class="snippet-label">ClusterIP service</div>
      <pre><code>apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: back-end</code></pre>
      <pre><code>kubectl create -f service-definition.yaml
kubectl get services
curl http://&lt;node-ip&gt;:30008   # for NodePort</code></pre>
      <figure class="diagram">
        <svg viewBox="0 0 540 160" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <rect x="20" y="20" width="155" height="120" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="97" y="46" fill="#1a7f37" font-size="16" font-weight="600" text-anchor="middle">NodePort</text>
          <text x="97" y="66" fill="#656d76" font-size="13" text-anchor="middle">Node:30008 → :80</text>
          <text x="97" y="82" fill="#656d76" font-size="13" text-anchor="middle">→ Pod targetPort</text>
          <text x="97" y="106" fill="#1f2328" font-size="13" text-anchor="middle">External via</text>
          <text x="97" y="120" fill="#1f2328" font-size="13" text-anchor="middle">any node + nodePort</text>
          <rect x="192" y="20" width="155" height="120" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="269" y="46" fill="#1a7f37" font-size="16" font-weight="600" text-anchor="middle">ClusterIP</text>
          <text x="269" y="66" fill="#656d76" font-size="13" text-anchor="middle">Virtual IP 10.96.x.x</text>
          <text x="269" y="82" fill="#656d76" font-size="13" text-anchor="middle">Cluster-internal</text>
          <text x="269" y="106" fill="#1f2328" font-size="13" text-anchor="middle">Frontend → Backend</text>
          <text x="269" y="120" fill="#1f2328" font-size="12" text-anchor="middle">svc.ns.svc.cluster.local</text>
          <rect x="364" y="20" width="155" height="120" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="441" y="46" fill="#1a7f37" font-size="16" font-weight="600" text-anchor="middle">LoadBalancer</text>
          <text x="441" y="66" fill="#656d76" font-size="13" text-anchor="middle">Cloud LB → NodePort</text>
          <text x="441" y="82" fill="#656d76" font-size="13" text-anchor="middle">→ Service → Pods</text>
          <text x="441" y="106" fill="#1f2328" font-size="13" text-anchor="middle">External IP</text>
          <text x="441" y="120" fill="#1f2328" font-size="13" text-anchor="middle">(AWS/GCP/Azure)</text>
        </svg>
        <figcaption>Service types: NodePort (expose on node port), ClusterIP (internal VIP), LoadBalancer (cloud LB).</figcaption>
      </figure>

      <!-- 03 Scheduling -->
      <h2 id="section-03">03 — Scheduling</h2>
      <h3>How scheduling works</h3>
      <p>The scheduler watches for pods with no <code>nodeName</code>. It picks a node (filtering + scoring), then either sets <code>spec.nodeName</code> or creates a <strong>Binding</strong> object. The <strong>kubelet</strong> on that node then creates the pod. Without a scheduler, you can set <code>nodeName</code> yourself to force a node.</p>
      <figure class="diagram">
        <svg viewBox="0 0 480 100" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <rect x="24" y="24" width="88" height="52" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="68" y="52" fill="#1f2328" font-size="14" text-anchor="middle">Pod created</text>
          <path d="M112 50 L136 50" stroke="#1a7f37" stroke-width="2"/>
          <rect x="136" y="24" width="88" height="52" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="180" y="50" fill="#1f2328" font-size="14" text-anchor="middle">Scheduler</text>
          <text x="180" y="66" fill="#656d76" font-size="12" text-anchor="middle">picks node</text>
          <path d="M224 50 L248 50" stroke="#1a7f37" stroke-width="2"/>
          <rect x="248" y="24" width="88" height="52" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="292" y="52" fill="#1f2328" font-size="14" text-anchor="middle">nodeName set</text>
          <path d="M336 50 L360 50" stroke="#1a7f37" stroke-width="2"/>
          <rect x="360" y="24" width="88" height="52" rx="6" fill="#eaeef2" stroke="#1a7f37"/>
          <text x="404" y="50" fill="#1f2328" font-size="14" text-anchor="middle">Kubelet</text>
          <text x="404" y="66" fill="#656d76" font-size="12" text-anchor="middle">runs pod</text>
        </svg>
        <figcaption>Pod scheduling flow: scheduler assigns node, kubelet runs the pod.</figcaption>
      </figure>
      <h4>Manual scheduling (no scheduler or override)</h4>
      <p>Set <code>nodeName</code> in the pod spec to pin a pod to a specific node. Kubernetes will not change it.</p>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
  nodeName: node02</code></pre>
      <p>Alternatively create a <strong>Binding</strong> object (target = node) and POST it to <code>/api/v1/namespaces/&lt;ns&gt;/pods/&lt;name&gt;/binding</code>; the pod spec stays without nodeName until the binding is processed.</p>

      <h3>Labels and selectors</h3>
      <p>Labels are key-value pairs on objects. Selectors filter by labels. Services and ReplicaSets use selectors to target pods.</p>
      <div class="snippet-label">Pod with labels</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080</code></pre>
      <pre><code>kubectl get pods --selector app=App1</code></pre>
      <p>Annotations (e.g. <code>buildversion: 1.34</code>) go under <code>metadata.annotations</code> for non-identifying info.</p>

      <h3>Taints and tolerations</h3>
      <p>Taints restrict which pods can be scheduled on a node. Only pods with matching tolerations can run there. Effects: <code>NoSchedule</code>, <code>PreferNoSchedule</code>, <code>NoExecute</code>.</p>
      <pre><code>kubectl taint nodes node1 app=blue:NoSchedule</code></pre>
      <div class="snippet-label">Pod with toleration</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"</code></pre>
      <pre><code>kubectl describe node kubemaster | grep Taint</code></pre>

      <h3>Node selector</h3>
      <p>Add <code>nodeSelector</code> to pod spec with a label that nodes must have. Label node first: <code>kubectl label nodes node1 disk=ssd</code>.</p>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disk: ssd</code></pre>

      <h3>Resource requests and limits</h3>
      <p>Default request: 0.5 CPU, 256Mi memory. Default limit: 1 CPU, 512Mi. Set per container in <code>spec.containers[].resources</code>.</p>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "2"</code></pre>

      <h3>DaemonSets</h3>
      <p>Runs one copy of a pod on every node. Use for monitoring, logging, storage.</p>
      <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
      - name: monitoring-agent
        image: monitoring-agent</code></pre>
      <pre><code>kubectl create -f daemon-set-definition.yaml
kubectl get daemonsets
kubectl describe daemonsets monitoring-daemon</code></pre>

      <h3>Node affinity</h3>
      <p>Like nodeSelector but with expressive rules. Types: <code>requiredDuringSchedulingIgnoredDuringExecution</code> (hard requirement), <code>preferredDuringSchedulingIgnoredDuringExecution</code> (soft). Use <code>matchExpressions</code> with operators <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>.</p>
      <pre><code>spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values: [Large, Medium]
      # preferredDuringSchedulingIgnoredDuringExecution for soft preference</code></pre>

      <h3>Multiple schedulers</h3>
      <p>You can run a custom scheduler (e.g. <code>my-custom-scheduler</code>) alongside the default. In the pod spec set <code>schedulerName: my-custom-scheduler</code>. Only that scheduler will assign that pod. View events: <code>kubectl get events</code>.</p>

      <h3>Static pods</h3>
      <p>Kubelet can run pods from a directory on the host (no API server). Configure kubelet with <code>--pod-manifest-path</code> or in config as <code>staticPodPath</code>. Kubelet creates both static pods and API-server pods. Static pods are mirrored in the API as read-only.</p>

      <!-- 04 Logging & Monitoring -->
      <h2 id="section-04">04 — Logging and Monitoring</h2>
      <h3>Application logs</h3>
      <p>Container stdout/stderr are captured by kubelet and exposed via the API. Use <code>kubectl logs</code>. For a <strong>multi-container</strong> pod you must specify the container name.</p>
      <pre><code>kubectl logs -f &lt;pod-name&gt;
kubectl logs -f &lt;pod-name&gt; &lt;container-name&gt;
kubectl logs &lt;pod-name&gt; --previous
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>
      <p><code>-f</code> streams logs; <code>--previous</code> shows logs from the <strong>previous</strong> container instance (e.g. after a crash/restart).</p>
      <h3>Cluster metrics</h3>
      <p><strong>Metrics Server</strong> provides resource metrics (CPU/memory) for <code>kubectl top nodes</code> and <code>kubectl top pods</code>. It is not for long-term storage; use a monitoring stack (Prometheus, etc.) for that. Legacy Heapster is deprecated.</p>

      <!-- 05 Application Lifecycle -->
      <h2 id="section-05">05 — Application Lifecycle Management</h2>
      <h3>Rolling updates and rollback</h3>
      <p>Deployment strategies: <strong>Recreate</strong> (all down then up), <strong>RollingUpdate</strong> (default).</p>
      <pre><code>kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
kubectl apply -f deployment-definition.yaml
kubectl rollout undo deployment/myapp-deployment</code></pre>

      <h3>ConfigMaps</h3>
      <p>Create: imperative <code>kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod</code> or from file <code>--from-file=app_config.properties</code>.</p>
      <div class="snippet-label">ConfigMap YAML</div>
      <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod</code></pre>
      <pre><code>kubectl create -f config-map.yaml
kubectl get configmaps
kubectl get cm
kubectl describe configmaps app-config</code></pre>
      <div class="snippet-label">Inject ConfigMap into pod (envFrom)</div>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config</code></pre>

      <h3>Secrets</h3>
      <p>Store sensitive data (base64 in YAML). Create: <code>kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd</code>. Encode for YAML: <code>echo -n "paswrd" | base64</code>.</p>
      <pre><code>apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk</code></pre>
      <pre><code>kubectl create -f secret-data.yaml
kubectl get secrets
kubectl describe secret app-secret
kubectl get secret app-secret -o yaml</code></pre>
      <p>Decode: <code>echo -n "bX1zcWw=" | base64 --decode</code>. Inject into pod with <code>envFrom: - secretRef: name: app-secret</code> (same structure as configMapRef).</p>

      <h3>Commands and arguments</h3>
      <p>Override image <strong>CMD</strong> with <code>command</code> (args to entrypoint) and <strong>ENTRYPOINT</strong> with <code>args</code> in the container spec. In Kubernetes, <code>command</code> corresponds to Docker ENTRYPOINT and <code>args</code> to CMD. Example: <code>command: ["sleep"]</code>, <code>args: ["3600"]</code>.</p>
      <h3>Init containers</h3>
      <p>Run to completion before main containers start. Use for one-time setup or waiting for dependencies. If an init container fails, the pod is restarted (all init containers re-run). Order: initContainers run sequentially; then main containers start.</p>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']</code></pre>

      <h3>Liveness and readiness probes</h3>
      <p>Probes make deployments <strong>self-healing</strong>. <strong>livenessProbe</strong>: if it fails, the container is restarted. <strong>readinessProbe</strong>: if it fails, the pod is removed from Service endpoints (no traffic until ready). Types: <code>httpGet</code> (HTTP path), <code>exec</code> (command), <code>tcpSocket</code> (port open).</p>
      <div class="snippet-label">Probes example</div>
      <pre><code>containers:
- name: app
  image: myapp:1.0
  livenessProbe:
    httpGet:
      path: /healthz
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 20
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10</code></pre>

      <h3>HorizontalPodAutoscaler (HPA)</h3>
      <p><strong>Workload autoscaling</strong>: HPA scales a Deployment/ReplicaSet based on CPU (or custom metrics). Requires <strong>Metrics Server</strong> in the cluster for <code>kubectl top</code> and CPU-based scaling. Set <code>resources.requests</code> on the deployment so HPA can compute utilization.</p>
      <div class="snippet-label">HPA YAML</div>
      <pre><code>apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80</code></pre>
      <pre><code>kubectl autoscale deployment myapp-deployment --min=2 --max=10 --cpu-percent=80
kubectl get hpa
kubectl describe hpa myapp-hpa</code></pre>

      <!-- 06 Cluster Maintenance -->
      <h2 id="section-06">06 — Cluster Maintenance</h2>
      <h3>OS upgrades and node maintenance</h3>
      <p><strong>Important:</strong> If a node is down for <strong>more than 5 minutes</strong>, the control plane may terminate pods that were on that node (they will be recreated elsewhere if managed by a controller).</p>
      <ul>
        <li><strong>kubectl drain &lt;node&gt;</strong>: Evicts workloads from the node (pods are rescheduled elsewhere) and marks the node <strong>unschedulable</strong> (cordon). Use before OS upgrade or maintenance.</li>
        <li><strong>kubectl uncordon &lt;node&gt;</strong>: Marks the node schedulable again after it is back.</li>
        <li><strong>kubectl cordon &lt;node&gt;</strong>: Only marks the node unschedulable; does <strong>not</strong> evict existing pods.</li>
      </ul>
      <figure class="diagram">
        <svg viewBox="0 0 420 120" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <rect x="20" y="20" width="118" height="80" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="79" y="48" fill="#1a7f37" font-size="15" font-weight="600" text-anchor="middle">cordon</text>
          <text x="79" y="68" fill="#656d76" font-size="13" text-anchor="middle">No new pods</text>
          <text x="79" y="84" fill="#656d76" font-size="13" text-anchor="middle">Pods stay</text>
          <rect x="151" y="20" width="118" height="80" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="210" y="48" fill="#0969da" font-size="15" font-weight="600" text-anchor="middle">drain</text>
          <text x="210" y="68" fill="#656d76" font-size="13" text-anchor="middle">Cordon + evict</text>
          <text x="210" y="84" fill="#656d76" font-size="13" text-anchor="middle">Pods move away</text>
          <rect x="282" y="20" width="118" height="80" rx="8" fill="#eaeef2" stroke="#d0d7de"/>
          <text x="341" y="48" fill="#1a7f37" font-size="15" font-weight="600" text-anchor="middle">uncordon</text>
          <text x="341" y="68" fill="#656d76" font-size="13" text-anchor="middle">Allow scheduling</text>
          <text x="341" y="84" fill="#656d76" font-size="13" text-anchor="middle">again</text>
        </svg>
        <figcaption>cordon = no new pods; drain = cordon + evict; uncordon = allow scheduling again.</figcaption>
      </figure>

      <h3>Kubernetes versions</h3>
      <p>Kubernetes supports the last 3 minor versions. Upgrade one minor version at a time. kubeadm, kubelet, and control-plane components can be at different versions during upgrade.</p>

      <h3>Cluster upgrade (kubeadm)</h3>
      <p><strong>Master:</strong></p>
      <pre><code>kubeadm upgrade plan
apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply v1.12.0
apt-get upgrade kubelet=1.12.0-00
systemctl restart kubelet
kubectl get nodes</code></pre>
      <p><strong>Workers:</strong> drain, upgrade kubeadm and kubelet, update node config, restart kubelet, uncordon.</p>
      <pre><code>kubectl drain node-1
apt-get upgrade -y kubeadm=1.12.0-00 kubelet=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet
kubectl uncordon node-1</code></pre>

      <h3>Backup and restore</h3>
      <p><strong>Resource configs:</strong> store YAML in Git; or <code>kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml</code> (partial). Tools like Velero can backup/restore resources.</p>
      <p><strong>ETCD snapshot:</strong></p>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db</code></pre>
      <p>With kubeadm, etcd runs as a pod; use the correct endpoint and certs. Example (adjust paths/endpoints for your setup):</p>
      <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
      <p>Restore: <code>etcdctl snapshot restore snapshot.db --data-dir=/var/lib/etcd-from-backup</code>, then point etcd to the new data-dir and restart control-plane components.</p>

      <h3>PodDisruptionBudget (PDB)</h3>
      <p>PDB limits voluntary disruptions (e.g. <code>kubectl drain</code>, node upgrades) so that a minimum number of pods stay available. <strong>minAvailable</strong>: at least N pods (number or percentage). <strong>maxUnavailable</strong>: at most N pods down (number or percentage). Drain respects PDBs and may block until pods can be rescheduled.</p>
      <div class="snippet-label">PDB example (minAvailable)</div>
      <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp</code></pre>
      <div class="snippet-label">PDB with maxUnavailable</div>
      <pre><code>spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: myapp</code></pre>
      <pre><code>kubectl get pdb
kubectl describe pdb myapp-pdb</code></pre>

      <!-- 07 Security -->
      <h2 id="section-07">07 — Security</h2>
      <h3>TLS in Kubernetes</h3>
      <p>Cluster components use <strong>TLS</strong>: servers use <strong>server certificates</strong>, clients use <strong>client certificates</strong> for authentication. API server, etcd, kubelet, and other control-plane components each have certs (typically under <code>/etc/kubernetes/pki/</code> on kubeadm). Certificate API can issue signed certs for users; the controller manager handles signing.</p>
      <h3>Viewing certificates</h3>
      <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout</code></pre>
      <p>Check subject, issuer, validity (not after). Same for other certs in <code>/etc/kubernetes/pki/</code> (ca.crt, apiserver-kubelet-client.crt, etc.).</p>
      <h3>kubeconfig</h3>
      <p>kubeconfig holds clusters (API server URL, CA), users (credentials: cert/key or token), and <strong>contexts</strong> (which cluster + which user + optional namespace). Default file: <code>~/.kube/config</code>.</p>
      <pre><code>kubectl config view
kubectl config view --kubeconfig=my-custom-config
kubectl config use-context &lt;context-name&gt;
kubectl config current-context
kubectl get pods --kubeconfig config</code></pre>
      <p>To set default namespace for current context: <code>kubectl config set-context $(kubectl config current-context) --namespace=dev</code>.</p>
      <h3>RBAC — Role and RoleBinding</h3>
      <p>Role: apiGroups, resources, verbs. RoleBinding: links subject (user/group/serviceaccount) to Role. Both are namespace-scoped.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "update", "delete", "create"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f developer-role.yaml
kubectl create -f devuser-developer-binding.yaml
kubectl get roles
kubectl get rolebindings
kubectl describe role developer
kubectl describe rolebinding devuser-developer-binding
kubectl auth can-i create pods --as=dev-user -n default</code></pre>

      <h3>ClusterRole and ClusterRoleBinding</h3>
      <p>ClusterRole can define access to cluster-scoped resources (e.g. nodes) or to namespaced resources across all namespaces.</p>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]</code></pre>
      <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io</code></pre>
      <pre><code>kubectl create -f cluster-admin-role.yaml
kubectl create -f cluster-admin-role-binding.yaml
kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false</code></pre>

      <h3>Network policy</h3>
      <p>Controls ingress/egress to pods by selector. Default: no restrictions. Once a NetworkPolicy selects a pod, that pod denies traffic not allowed by any policy.</p>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: api-pod
    ports:
    - protocol: TCP
      port: 3306</code></pre>
      <pre><code>kubectl create -f policy-definition.yaml</code></pre>
      <h3>Security context</h3>
      <p>Restrict pod/container behavior: run as non-root user, read-only root filesystem, drop capabilities. Set at <code>spec.securityContext</code> (pod) or <code>spec.containers[].securityContext</code> (container).</p>
      <pre><code>spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: ubuntu
    image: ubuntu
    securityContext:
      runAsUser: 1000
      capabilities:
        drop: [ALL]</code></pre>
      <figure class="diagram">
        <svg viewBox="0 0 400 110" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <rect x="24" y="24" width="100" height="62" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="74" y="48" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">Role</text>
          <text x="74" y="64" fill="#656d76" font-size="12" text-anchor="middle">apiGroups, resources</text>
          <text x="74" y="78" fill="#656d76" font-size="12" text-anchor="middle">verbs</text>
          <path d="M124 55 L148 55" stroke="#1a7f37" stroke-width="2"/>
          <rect x="148" y="24" width="104" height="62" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="200" y="48" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">RoleBinding</text>
          <text x="200" y="66" fill="#656d76" font-size="12" text-anchor="middle">subjects, roleRef</text>
          <path d="M252 55 L276 55" stroke="#1a7f37" stroke-width="2"/>
          <rect x="276" y="24" width="100" height="62" rx="6" fill="#eaeef2" stroke="#1a7f37"/>
          <text x="326" y="48" fill="#1f2328" font-size="14" font-weight="600" text-anchor="middle">User</text>
          <text x="326" y="66" fill="#656d76" font-size="12" text-anchor="middle">permissions</text>
        </svg>
        <figcaption>RBAC: Role defines permissions; RoleBinding binds Role to User/Group/ServiceAccount.</figcaption>
      </figure>

      <!-- 08 Storage -->
      <h2 id="section-08">08 — Storage</h2>
      <h3>Persistent Volume (PV) and Persistent Volume Claim (PVC)</h3>
      <p>PV is <strong>cluster-scoped</strong> storage (admin provisions); PVC is <strong>namespace-scoped</strong> (user requests storage). They bind when capacity and <code>accessModes</code> match. If no PV matches, PVC stays <strong>Pending</strong>. Access modes: ReadWriteOnce (RWO), ReadOnlyMany (ROX), ReadWriteMany (RWX).</p>
      <figure class="diagram">
        <svg viewBox="0 0 500 110" preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
          <rect x="24" y="24" width="130" height="62" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="89" y="48" fill="#1f2328" font-size="15" font-weight="600" text-anchor="middle">PV</text>
          <text x="89" y="66" fill="#656d76" font-size="13" text-anchor="middle">cluster-scoped</text>
          <path d="M154 55 L178 55" stroke="#1a7f37" stroke-width="2"/>
          <rect x="178" y="24" width="130" height="62" rx="6" fill="#eaeef2" stroke="#0969da"/>
          <text x="243" y="48" fill="#1f2328" font-size="15" font-weight="600" text-anchor="middle">PVC</text>
          <text x="243" y="66" fill="#656d76" font-size="13" text-anchor="middle">namespace-scoped</text>
          <path d="M308 55 L332 55" stroke="#1a7f37" stroke-width="2"/>
          <rect x="332" y="24" width="130" height="62" rx="6" fill="#eaeef2" stroke="#1a7f37"/>
          <text x="397" y="48" fill="#1f2328" font-size="15" font-weight="600" text-anchor="middle">Pod</text>
          <text x="397" y="66" fill="#656d76" font-size="13" text-anchor="middle">volumeMounts</text>
        </svg>
        <figcaption>Storage flow: PV (pool) → PVC (claim) → Pod (volumeMounts). PVC and Pod must be in same namespace.</figcaption>
      </figure>
      <div class="snippet-label">PV definition</div>
      <pre><code>kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-vol1
spec:
  accessModes: ["ReadWriteOnce"]
  capacity:
    storage: 1Gi
  hostPath:
    path: /tmp/data</code></pre>
      <div class="snippet-label">PVC definition</div>
      <pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 1Gi</code></pre>
      <pre><code>kubectl create -f pv-definition.yaml
kubectl create -f pvc-definition.yaml
kubectl get pv
kubectl get pvc
kubectl delete pvc myclaim
kubectl delete pv pv-vol1</code></pre>

      <h3>Using PVC in a pod</h3>
      <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: myfrontend
    image: nginx
    volumeMounts:
    - mountPath: "/var/www/html"
      name: web
  volumes:
  - name: web
    persistentVolumeClaim:
      claimName: myclaim</code></pre>
      <pre><code>kubectl create -f pod-definition.yaml
kubectl get pod,pvc,pv</code></pre>

      <h3>StorageClass (dynamic provisioning)</h3>
      <p>With a StorageClass, PVC can trigger automatic PV creation (no manual PV).</p>
      <pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd</code></pre>
      <pre><code>kubectl create -f sc-definition.yaml
kubectl get sc</code></pre>
      <p>In PVC add <code>storageClassName: google-storage</code>. Then create PVC and use <code>claimName</code> in pod volume as above.</p>

      <!-- 09 Networking -->
      <h2 id="section-09">09 — Networking</h2>
      <h3>Service networking</h3>
      <p>Service gets a ClusterIP; kube-proxy programs iptables (or IPVS). Check: <code>kubectl get svc</code>, <code>kubectl describe svc &lt;name&gt;</code>. ClusterIP range is configured on API server.</p>

      <h3>DNS and CoreDNS</h3>
      <p>Pods use the <strong>kube-dns</strong> service (ClusterIP, usually 10.96.0.10) for DNS. CoreDNS runs in <code>kube-system</code>; config in ConfigMap <code>coredns</code>. Cluster domain is usually <code>cluster.local</code>.</p>
      <p><strong>Service FQDN:</strong> <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>. Same-namespace pods can use just <code>&lt;service-name&gt;</code>.</p>
      <p><strong>Pod FQDN:</strong> Pod IP is converted to a DNS name by replacing dots with dashes: <code>10-244-1-3.&lt;namespace&gt;.pod.cluster.local</code>. Example: pod IP 10.244.1.3 in namespace <code>apps</code> → <code>10-244-1-3.apps.pod.cluster.local</code>.</p>
      <pre><code>kubectl run -it test --image=busybox:1.28 --rm --restart=Never -- nslookup nginx-service.apps.svc.cluster.local</code></pre>
      <pre><code>kubectl get pods -n kube-system
kubectl get deployment -n kube-system
kubectl get configmap coredns -n kube-system
kubectl describe cm coredns -n kube-system
kubectl get service -n kube-system</code></pre>
      <p><strong>Corefile:</strong> CoreDNS config is in the <code>Corefile</code> key of the <code>coredns</code> ConfigMap. Edit with <code>kubectl edit configmap coredns -n kube-system</code>. Common plugins: <code>file</code> (zone files), <code>forward</code> (upstream DNS), <code>cache</code>, <code>loop</code>, <code>log</code>, <code>errors</code>. Add <code>log</code> and <code>errors</code> for debugging. <strong>Stub domains:</strong> forward specific domains to custom DNS (e.g. <code>consul.local</code> to a Consul server) by adding a <code>forward</code> block in the Corefile.</p>

      <h3>Ingress</h3>
      <p>Need an Ingress controller (e.g. nginx-ingress) and Ingress resources (rules). Ingress exposes HTTP/HTTPS routes to services.</p>
      <div class="snippet-label">Ingress with paths</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <div class="snippet-label">Ingress with host rules</div>
      <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 80
  - host: watch.my-online-store.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: watch-service
            port:
              number: 80</code></pre>
      <pre><code>kubectl create -f ingress.yaml
kubectl get ingress
kubectl describe ingress ingress-wear-watch</code></pre>

      <h3>Gateway API</h3>
      <p>The <strong>Gateway API</strong> is the newer, more expressive way to expose HTTP/gRPC/TCP routes (successor concept to Ingress). Key resources: <code>Gateway</code> (infrastructure that receives traffic), <code>HTTPRoute</code> (rules for routing to backends). Install a Gateway controller (e.g. Gateway API implementation) and create Gateway + HTTPRoute resources. The exam may reference it; know it exists as the evolution of Ingress.</p>

      <!-- 10 Design & Install -->
      <h2 id="section-10">10 — Design and Install Kubernetes Cluster</h2>
      <p><strong>Design:</strong> Plan control-plane count (HA = multiple masters), etcd (stacked with API server or external cluster), load balancer in front of API servers. Choose node sizing and networking (pod CIDR, service CIDR).</p>
      <p><strong>High availability:</strong> Multiple API server replicas behind a load balancer; etcd cluster (odd number, 3 or 5); scheduler and controller manager can run as active/standby (only one active).</p>

      <!-- 11 kubeadm -->
      <h2 id="section-11">11 — Install Kubernetes the kubeadm Way</h2>
      <p><strong>kubeadm</strong> bootstraps the control plane and generates certs, manifests, and kubeconfig. You then join worker nodes with <code>kubeadm join</code>. The exam rarely asks for a full install; <strong>upgrading an existing cluster</strong> (kubeadm upgrade plan / apply, drain/uncordon workers) is frequently tested.</p>

      <!-- 12 Troubleshooting -->
      <h2 id="section-12">12 — Troubleshooting</h2>
      <h3>Application failure — step-by-step</h3>
      <ul class="step-list">
        <li><strong>Is the service reachable?</strong> <code>curl http://&lt;node-ip&gt;:&lt;nodePort&gt;</code> or from inside cluster <code>curl http://&lt;svc-name&gt;.&lt;ns&gt;.svc.cluster.local:port</code>.</li>
        <li><strong>Does the service have endpoints?</strong> <code>kubectl get endpoints &lt;service-name&gt;</code>. If empty, the service selector does not match any pod (wrong labels or pods not ready).</li>
        <li><strong>Compare selector with pod labels:</strong> <code>kubectl describe service &lt;name&gt;</code> (see Selector) and <code>kubectl get pods --show-labels</code>. Fix selector or pod labels.</li>
        <li><strong>Pod status:</strong> <code>kubectl get pods</code>, <code>kubectl describe pod &lt;pod-name&gt;</code>. Check events (image pull, CrashLoopBackOff, etc.).</li>
        <li><strong>Container logs:</strong> <code>kubectl logs &lt;pod-name&gt;</code>, <code>kubectl logs &lt;pod-name&gt; -c &lt;container&gt;</code>, <code>kubectl logs &lt;pod-name&gt; --previous</code> for crashed container.</li>
      </ul>
      <pre><code>kubectl get pods -o wide
kubectl describe pod &lt;pod-name&gt;
kubectl logs &lt;pod-name&gt; -f --previous</code></pre>

      <h3>Control plane failure</h3>
      <p>Verify nodes are Ready, then check control-plane components. With kubeadm they are pods in <code>kube-system</code>; with manual install they are often systemd services.</p>
      <pre><code>kubectl get nodes
kubectl get pods -n kube-system
service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
service kubelet status
kubectl logs kube-apiserver-master -n kube-system
sudo journalctl -u kube-apiserver</code></pre>

      <h3>Worker node failure</h3>
      <p>If a node is <strong>NotReady</strong>, check <code>kubectl describe node &lt;node&gt;</code> (conditions, LastHeartbeatTime). On the node: kubelet status, logs, and cert validity (kubelet may have expired cert).</p>
      <pre><code>kubectl get nodes
kubectl describe node worker-1
service kubelet status
sudo journalctl -u kubelet
openssl x509 -in /var/lib/kubelet/worker-1.crt -text</code></pre>

      <!-- 13 Other Topics -->
      <h2 id="section-13">13 — Other Topics</h2>
      <h3>JSON path and custom columns</h3>
      <pre><code>kubectl get nodes -o json
kubectl get pods -o json
kubectl get pods -o=jsonpath='{.items[0].spec.containers[0].image}'
kubectl get pods -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu
kubectl get nodes --sort-by=.metadata.name
kubectl get nodes --sort-by=.status.capacity.cpu</code></pre>

      <h3>2025 curriculum: Helm and Kustomize</h3>
      <p><strong>Helm</strong> packages Kubernetes manifests into <strong>charts</strong>. Use for cluster components and repeatable installs. Common commands: <code>helm install &lt;release&gt; &lt;chart&gt;</code>, <code>helm upgrade</code>, <code>helm list</code>, <code>helm uninstall</code>. Add repos: <code>helm repo add &lt;name&gt; &lt;url&gt;</code>, <code>helm repo update</code>.</p>
      <p><strong>Kustomize</strong> customizes base YAML (overlays, patches, common labels). Built into kubectl: <code>kubectl apply -k &lt;dir&gt;</code> or <code>kustomize build &lt;dir&gt; | kubectl apply -f -</code>. Typical layout: <code>base/</code> (resources) + <code>kustomization.yaml</code>; overlays (e.g. <code>dev/</code>, <code>prod/</code>) reference the base and add patches.</p>

      <h3>CRDs and operators</h3>
      <p><strong>Custom Resource Definitions (CRDs)</strong> extend the API with custom resources. After defining a CRD, you get new resource types (e.g. <code>kubectl get myresource</code>). <strong>Operators</strong> are controllers that manage custom resources and their lifecycle (install, upgrade, backup). They use CRDs + a controller process. List CRDs: <code>kubectl get crd</code>.</p>

      <h3>Extension interfaces: CNI, CSI, CRI</h3>
      <p>Kubernetes uses pluggable interfaces. <strong>CNI</strong> (Container Network Interface): pod networking; plugins (Calico, Flannel, Weave) implement the pod network. <strong>CSI</strong> (Container Storage Interface): storage drivers for PVs; dynamic provisioning via StorageClass uses a CSI driver. <strong>CRI</strong> (Container Runtime Interface): container runtimes (containerd, CRI-O); kubelet talks to the runtime via CRI. Configure/validate these when installing or troubleshooting the cluster.</p>

      <!-- 14 Playground & Labs -->
      <h2 id="section-14">14 — Playground &amp; Lightning Labs</h2>
      <p>Use the links below to practice in a real cluster, then try the scenarios on your own before revealing the solution.</p>

      <h3>Online playgrounds</h3>
      <ul class="playground-links">
        <li>
          <a href="https://killercoda.com/kubernetes" target="_blank" rel="noopener noreferrer">Killercoda — Kubernetes</a>
          <div class="link-desc">Free browser-based Kubernetes labs; no local install. Multiple CKA-focused scenarios.</div>
        </li>
        <li>
          <a href="https://www.katacoda.com/courses/kubernetes" target="_blank" rel="noopener noreferrer">Katacoda (legacy)</a>
          <div class="link-desc">Kubernetes playgrounds (some scenarios may be deprecated; Killercoda is the successor).</div>
        </li>
        <li>
          <a href="https://labs.play-with-k8s.com/" target="_blank" rel="noopener noreferrer">Play with Kubernetes</a>
          <div class="link-desc">Spin up a temporary cluster in the browser; good for quick tests.</div>
        </li>
        <li>
          <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/" target="_blank" rel="noopener noreferrer">Kubernetes Basics (official)</a>
          <div class="link-desc">Official interactive tutorial with an in-browser terminal.</div>
        </li>
      </ul>

      <h3>Practice scenarios (try yourself, then reveal solution)</h3>

      <div class="playground-scenario">
        <h4>Scenario 1: Deployment and Service</h4>
        <p class="playground-context">Namespace <code>app</code> exists. Create a deployment and expose it.</p>
        <p class="playground-task"><strong>Task:</strong> Create a deployment named <code>nginx-deploy</code> with image <code>nginx:1.21</code>, 3 replicas. Expose it as a ClusterIP service <code>nginx-svc</code> on port 80. Use namespace <code>app</code>.</p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create deployment nginx-deploy --image=nginx:1.21 --replicas=3 -n app
kubectl expose deployment nginx-deploy --name=nginx-svc --port=80 -n app
kubectl get deploy,svc -n app</code></pre>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 2: ConfigMap and Pod</h4>
        <p class="playground-context">You need a pod that reads a config value from a ConfigMap.</p>
        <p class="playground-task"><strong>Task:</strong> Create a ConfigMap <code>app-cm</code> with <code>APP_ENV=production</code>. Create a pod <code>busybox-cm</code> (image <code>busybox:1.28</code>) that runs <code>sleep 3600</code> and has environment variable <code>APP_ENV</code> from the ConfigMap.</p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl create configmap app-cm --from-literal=APP_ENV=production
kubectl run busybox-cm --image=busybox:1.28 --restart=Never -- sleep 3600 --overrides='
{"spec":{"containers":[{"name":"busybox-cm","image":"busybox:1.28","command":["sleep","3600"],"env":[{"name":"APP_ENV","valueFrom":{"configMapKeyRef":{"name":"app-cm","key":"APP_ENV"}}}]}]}}'</code></pre>
          <p style="margin-top:0.75rem;font-size:0.9rem;">Or create a YAML file with <code>env</code> and <code>valueFrom.configMapKeyRef</code> and <code>kubectl apply -f</code>.</p>
        </div>
      </div>

      <div class="playground-scenario">
        <h4>Scenario 3: Node selector and drain</h4>
        <p class="playground-context">Node <code>worker-1</code> has label <code>disk=ssd</code>. You need to run a pod there and later drain the node.</p>
        <p class="playground-task"><strong>Task:</strong> Create a pod <code>ssd-pod</code> (image <code>nginx:alpine</code>) that is scheduled only on nodes with <code>disk=ssd</code>. Then cordon and drain <code>worker-1</code> (ignore DaemonSet pods).</p>
        <button type="button" class="playground-toggle" aria-expanded="false">Show solution</button>
        <div class="playground-solution">
          <pre><code>kubectl run ssd-pod --image=nginx:alpine --restart=Never --overrides='
{"spec":{"nodeSelector":{"disk":"ssd"}}}'
kubectl cordon worker-1
kubectl drain worker-1 --ignore-daemonsets --delete-emptydir-data</code></pre>
        </div>
      </div>

      <!-- 15 Mock Exams -->
      <h2 id="section-15">15 — Mock Exams</h2>
      <p>Time yourself (e.g. 10–15 minutes per question). Complete the task in your cluster or playground, then check the solution.</p>

      <div class="mock-question">
        <h4>Mock 1: RBAC — read-only pods in a namespace</h4>
        <p class="mock-context">Namespace <code>dev</code> exists. A user/service account should only list and get pods in <code>dev</code>, no create/delete.</p>
        <p class="mock-task"><strong>Task:</strong> Create a Role that allows <code>get</code>, <code>list</code> on <code>pods</code> in namespace <code>dev</code>. Create a RoleBinding binding that Role to a user named <code>jane</code>.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl create role pod-reader --verb=get,list --resource=pods -n dev
kubectl create rolebinding jane-pod-reader --role=pod-reader --user=jane -n dev</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Or apply YAML with <code>kind: Role</code> (rules with apiGroups <code>[""]</code>, resources <code>["pods"]</code>, verbs <code>["get","list"]</code>) and <code>kind: RoleBinding</code> (roleRef, subjects with name <code>jane</code>).</p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 2: Multi-container pod and logs</h4>
        <p class="mock-context">A pod has two containers: <code>main</code> and <code>sidecar</code>. It is failing and you need to inspect the previous instance’s logs.</p>
        <p class="mock-task"><strong>Task:</strong> Write the exact <code>kubectl logs</code> command to stream logs from the <strong>previous</strong> instance of the <code>sidecar</code> container in pod <code>myapp-pod</code> (namespace <code>default</code>).</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl logs myapp-pod -c sidecar --previous -f</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;"><code>-c sidecar</code> selects the container, <code>--previous</code> shows the crashed/previous instance, <code>-f</code> streams.</p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 3: Static pod</h4>
        <p class="mock-context">The control plane uses static pods. You need to run a static pod that runs <code>busybox</code> with <code>sleep 3600</code>.</p>
        <p class="mock-task"><strong>Task:</strong> Create a static pod manifest (YAML) named <code>static-busybox</code> on the control plane node. Assume the static pod path is <code>/etc/kubernetes/manifests</code>. Provide the manifest path and key fields.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code># File: /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command: ["sleep", "3600"]</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Copy this file to the control plane node at <code>/etc/kubernetes/manifests/</code>. Kubelet will create the pod. No <code>kubectl apply</code> needed.</p>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 4: Service and endpoints</h4>
        <p class="mock-context">A service <code>web-svc</code> in namespace <code>prod</code> is not getting traffic. Pods are running with label <code>app=web</code>.</p>
        <p class="mock-task"><strong>Task:</strong> (1) Check if the service has endpoints. (2) If empty, fix the service so it targets pods with label <code>app=web</code>. Assume the service exists but has the wrong selector.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>kubectl get endpoints web-svc -n prod
kubectl get svc web-svc -n prod -o yaml   # inspect selector
# Fix: patch or edit so selector is app=web
kubectl patch svc web-svc -n prod -p '{"spec":{"selector":{"app":"web"}}}'
# Or kubectl edit svc web-svc -n prod and set spec.selector.app: web
kubectl get endpoints web-svc -n prod     # should list pod IPs now</code></pre>
        </div>
      </div>

      <div class="mock-question">
        <h4>Mock 5: etcd backup</h4>
        <p class="mock-context">You need to take a snapshot of etcd for disaster recovery. The cluster was set up with kubeadm; etcd runs as a pod and listens on <code>https://127.0.0.1:2379</code> with certs under <code>/etc/kubernetes/pki/etcd/</code>.</p>
        <p class="mock-task"><strong>Task:</strong> Run the <code>etcdctl snapshot save</code> command with the correct API version, endpoint, and cert flags. Save to <code>/tmp/etcd-snapshot.db</code>.</p>
        <button type="button" class="solution-toggle" aria-expanded="false">Show solution</button>
        <div class="mock-solution">
          <pre><code>ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key</code></pre>
          <p style="margin-top:0.5rem;font-size:0.9rem;">Run from the control plane node (or from inside the etcd pod). Verify with <code>ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-snapshot.db</code>.</p>
        </div>
      </div>

      <h2 id="section-16">16 — Ultimate Mocks</h2>
      <p>Extended scenario-based mocks: multi-step troubleshooting, storage (PV/PVC), networking (NetworkPolicy, Ingress), and cluster upgrades. Use the same format: try the scenario in a playground, then compare with the solutions in the course docs or your notes.</p>

      <h2 id="section-17">17 — Tips and Tricks</h2>
      <p><strong>Test pods for networking:</strong> Run a temporary pod for DNS/connectivity tests: <code>kubectl run dnsutils --image=busybox:1.28 --restart=Never --rm -it -- nslookup kubernetes.default.svc.cluster.local</code>. For a long-lived debug pod: <code>kubectl run debug --image=nicolaka/netshoot -it --rm --restart=Never -- bash</code>.</p>
      <p><strong>Useful one-liners:</strong> <code>kubectl get pods -A -o wide</code>; <code>kubectl get events -A --sort-by='.lastTimestamp'</code>; <code>kubectl describe node | grep -A5 "Allocated resources"</code>. Set default namespace: <code>kubectl config set-context --current --namespace=&lt;ns&gt;</code>.</p>

    </main>
  </div>
  <footer class="site-footer">
    CKA Study Guide — Standalone reference for Certified Kubernetes Administrator exam preparation.
  </footer>
  <script>
    (function() {
      var t = document.getElementById('nav-toggle');
      var c = document.getElementById('nav-close');
      var n = document.getElementById('nav');
      function closeNav() {
        n.classList.remove('is-open');
        if (t) t.setAttribute('aria-label', 'Open menu');
      }
      var main = document.querySelector('main');
      var headerH = 56;

      function easeOutCubic(t) {
        return 1 - Math.pow(1 - t, 3);
      }

      function smoothScrollTo(el) {
        if (!el) return;
        var targetY = el.getBoundingClientRect().top + window.pageYOffset - headerH;
        var startY = window.pageYOffset;
        var dist = targetY - startY;
        var duration = Math.min(600, 300 + Math.abs(dist) * 0.15);
        var start = performance.now();
        function step(now) {
          var elapsed = now - start;
          var p = Math.min(1, elapsed / duration);
          var eased = easeOutCubic(p);
          window.scrollTo(0, startY + dist * eased);
          if (p < 1) requestAnimationFrame(step);
        }
        requestAnimationFrame(step);
      }

      function wrapContentBlocks() {
        if (!main) return;
        var intro = document.getElementById('intro');
        if (intro) {
          var blockIntro = document.createElement('div');
          blockIntro.id = 'block-intro';
          blockIntro.className = 'content-block is-open';
          intro.parentNode.insertBefore(blockIntro, intro);
          blockIntro.appendChild(intro);
        }
        main.querySelectorAll('h2[id^="section-"]').forEach(function(h2) {
          var id = h2.getAttribute('id');
          var block = document.createElement('div');
          block.id = 'block-' + id;
          block.className = 'content-block';
          var nodes = [h2];
          var next = h2.nextElementSibling;
          while (next && next.tagName !== 'H2') {
            nodes.push(next);
            next = next.nextElementSibling;
          }
          h2.parentNode.insertBefore(block, h2);
          nodes.forEach(function(node) { block.appendChild(node); });
        });
      }

      function openBlock(block) {
        if (!block) return;
        main.querySelectorAll('.content-block').forEach(function(b) { b.classList.remove('is-open'); });
        block.classList.remove('is-open');
        block.offsetHeight;
        block.classList.add('is-open');
        smoothScrollTo(block);
      }

      if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', wrapContentBlocks);
      } else {
        wrapContentBlocks();
      }

      if (c) c.addEventListener('click', closeNav);
      if (t && n) {
        t.addEventListener('click', function() {
          n.classList.toggle('is-open');
          t.setAttribute('aria-label', n.classList.contains('is-open') ? 'Close menu' : 'Open menu');
        });
        n.querySelectorAll('a[href^="#"]').forEach(function(a) {
          a.addEventListener('click', function(ev) {
            var id = a.getAttribute('href');
            if (id === '#') return;
            ev.preventDefault();
            closeNav();
            var hash = id.slice(1);
            var blockId = hash === 'intro' ? 'block-intro' : 'block-' + hash;
            var block = document.getElementById(blockId);
            openBlock(block);
          });
        });
        document.addEventListener('click', function(e) {
          if (n.classList.contains('is-open') && !n.contains(e.target) && e.target !== t)
            closeNav();
        });
      }

      function initSolutionToggles() {
        document.querySelectorAll('.solution-toggle, .playground-toggle').forEach(function(btn) {
          btn.addEventListener('click', function() {
            var solution = this.nextElementSibling;
            if (!solution || !solution.classList.contains('mock-solution') && !solution.classList.contains('playground-solution')) return;
            var isOpen = solution.classList.toggle('is-open');
            this.setAttribute('aria-expanded', isOpen);
            this.textContent = isOpen ? 'Hide solution' : 'Show solution';
          });
        });
      }
      if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', initSolutionToggles);
      } else {
        initSolutionToggles();
      }
    })();
  </script>
</body>
</html>
