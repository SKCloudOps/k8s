<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CKA Exam — Comprehensive Cheat Sheet</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link
    href="https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
    rel="stylesheet">
  <style>
    :root {
      --bg: #ffffff;
      --surface: #f6f8fa;
      --surface2: #eaeef2;
      --text: #1f2328;
      --text-muted: #656d76;
      --accent: #0d47a1;
      --accent2: #1b5e20;
      --border: #d0d7de;
      --code-bg: #f0f0f0;
      --nav-width: 260px;
      --content-max: 860px;
      --warn: #c62828;
      --tip: #2e7d32;
    }

    *,
    *::before,
    *::after {
      box-sizing: border-box;
    }

    html {
      font-size: 15px;
      scroll-behavior: smooth;
    }

    body {
      margin: 0;
      font-family: 'Outfit', -apple-system, BlinkMacSystemFont, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.7;
      display: flex;
      min-height: 100vh;
    }

    /* ── Sidebar Nav ── */
    nav {
      position: fixed;
      top: 0;
      left: 0;
      bottom: 0;
      width: var(--nav-width);
      background: var(--surface);
      border-right: 1px solid var(--border);
      overflow-y: auto;
      padding: 1rem 0;
      z-index: 100;
    }

    nav h2 {
      font-size: .85rem;
      font-weight: 700;
      color: var(--accent);
      padding: 0 1.2rem;
      margin: 0 0 .5rem;
      letter-spacing: .5px;
      text-transform: uppercase;
    }

    nav a {
      display: block;
      padding: .35rem 1.2rem;
      font-size: .82rem;
      color: var(--text-muted);
      text-decoration: none;
      transition: .15s;
    }

    nav a:hover,
    nav a.active {
      color: var(--accent);
      background: var(--surface2);
      font-weight: 500;
    }

    /* ── Main Content ── */
    main {
      margin-left: var(--nav-width);
      flex: 1;
      max-width: var(--content-max);
      padding: 2rem 2.5rem 4rem;
    }

    h1 {
      font-size: 1.9rem;
      font-weight: 700;
      color: var(--accent);
      border-bottom: 3px solid var(--accent);
      padding-bottom: .4rem;
      margin-top: 2.5rem;
    }

    h2 {
      font-size: 1.3rem;
      color: var(--accent);
      margin-top: 1.8rem;
    }

    h3 {
      font-size: 1.05rem;
      color: var(--accent2);
      margin-top: 1.3rem;
    }

    pre {
      background: var(--code-bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: .8rem 1rem;
      overflow-x: auto;
      font-family: 'JetBrains Mono', 'Consolas', monospace;
      font-size: .78rem;
      line-height: 1.55;
      color: #1a1a2e;
      margin: .5rem 0 1rem;
    }

    code {
      font-family: 'JetBrains Mono', 'Consolas', monospace;
      background: var(--code-bg);
      padding: .15em .35em;
      border-radius: 3px;
      font-size: .85em;
    }

    pre code {
      background: none;
      padding: 0;
      font-size: inherit;
    }

    .tip,
    .exam-note {
      border-left: 4px solid;
      padding: .5rem 1rem;
      margin: .8rem 0;
      border-radius: 0 6px 6px 0;
      font-size: .9rem;
    }

    .tip {
      border-color: var(--tip);
      background: #e8f5e9;
    }

    .exam-note {
      border-color: var(--warn);
      background: #ffebee;
    }

    .tip strong {
      color: var(--tip);
    }

    .exam-note strong {
      color: var(--warn);
    }

    ul {
      padding-left: 1.5rem;
    }

    li {
      margin-bottom: .25rem;
    }

    .yaml-label {
      display: inline-block;
      background: var(--accent);
      color: #fff;
      font-size: .7rem;
      padding: .1rem .5rem;
      border-radius: 3px;
      margin-bottom: .25rem;
      font-weight: 600;
      letter-spacing: .5px;
    }

    .imp-label {
      display: inline-block;
      background: #d84315;
      color: #fff;
      font-size: .7rem;
      padding: .1rem .5rem;
      border-radius: 3px;
      margin-bottom: .25rem;
      font-weight: 600;
      letter-spacing: .5px;
    }

    .imp-block {
      border-left: 4px solid #ff7043;
      background: #1e2532;
      padding: .6rem 1rem;
      margin: .8rem 0;
      border-radius: 0 8px 8px 0;
      font-size: .9rem;
      color: #cdd6f4;
    }

    .imp-block strong {
      color: #fab387;
    }

    .imp-block p,
    .imp-block li {
      color: #cdd6f4;
    }

    .imp-block code {
      background: #313244;
      color: #a6e3a1;
      border-radius: 4px;
      padding: .15em .4em;
    }

    .imp-block pre {
      background: #11131c;
      border-color: #ff7043;
      color: #cdd6f4;
    }

    .imp-block pre code {
      background: none;
      color: inherit;
      padding: 0;
    }

    hr {
      border: none;
      border-top: 2px solid var(--border);
      margin: 2.5rem 0;
    }

    /* ── Print Styles ── */
    @media print {
      nav {
        display: none;
      }

      main {
        margin-left: 0;
        max-width: 100%;
        padding: .5cm;
      }

      h1 {
        page-break-before: always;
      }

      h1:first-of-type {
        page-break-before: avoid;
      }

      pre {
        font-size: 7pt;
        break-inside: avoid;
      }

      body {
        font-size: 9pt;
      }
    }

    /* ── Responsive ── */
    @media (max-width: 900px) {
      nav {
        display: none;
      }

      main {
        margin-left: 0;
        padding: 1rem;
      }
    }

    /* ── Imperative Command Tooltip Tables ── */
    .cmd-table {
      border: 1px solid var(--border);
      border-radius: 8px;
      overflow: hidden;
      margin: 0.75rem 0 1.5rem;
    }
    .cmd-row {
      display: flex;
      align-items: flex-start;
      justify-content: space-between;
      padding: 0.45rem 1rem;
      border-bottom: 1px solid var(--border);
      background: var(--code-bg);
      transition: background 0.15s;
      gap: 0.75rem;
    }
    .cmd-row:last-child { border-bottom: none; }
    .cmd-row:hover { background: var(--surface2); }
    .cmd-row code {
      background: none;
      padding: 0;
      flex: 1;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.875rem;
      line-height: 1.55;
      white-space: pre-wrap;
      overflow-wrap: break-word;
      color: #1e293b;
    }
    .tip-icon {
      flex-shrink: 0;
      width: 18px;
      height: 18px;
      border-radius: 50%;
      background: var(--accent2);
      color: white;
      font-size: 0.68rem;
      font-family: sans-serif;
      font-weight: 700;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      margin-top: 3px;
      cursor: help;
      position: relative;
      user-select: none;
    }
    .tip-icon::after {
      content: attr(data-tip);
      position: absolute;
      right: 0;
      top: calc(100% + 7px);
      background: #1e293b;
      color: #e2e8f0;
      font-size: 0.8rem;
      font-family: 'Outfit', sans-serif;
      font-weight: 400;
      line-height: 1.6;
      padding: 0.65rem 1rem;
      border-radius: 8px;
      width: 300px;
      white-space: normal;
      z-index: 1000;
      box-shadow: 0 4px 20px rgba(0,0,0,0.3);
      pointer-events: none;
      opacity: 0;
      transform: translateY(-4px);
      transition: opacity 0.18s, transform 0.18s;
    }
    .tip-icon:hover::after {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>

<body>

  <!-- ═══════════ BACK TO HOME (fixed top-right) ═══════════ -->
  <a href="index.html"
    style="position:fixed;top:0.75rem;right:1rem;z-index:9999;background:var(--accent);color:#fff;text-decoration:none;padding:0.4rem 0.85rem;border-radius:6px;font-family:'Outfit',sans-serif;font-size:0.85rem;font-weight:600;box-shadow:0 2px 8px rgba(0,0,0,0.18);display:inline-flex;align-items:center;gap:0.35rem;">&larr;
    Home</a>

  <!-- ═══════════ SIDEBAR NAV ═══════════ -->
  <nav>
    <h2>CKA Cheat Sheet</h2>
    <a href="#setup">1. Exam Setup & Aliases</a>
    <a href="#imperative">2. Imperative Commands</a>
    <a href="#kubectl-ref">3. kubectl Reference</a>
    <a href="#core">4. Core Concepts</a>
    <a href="#pod-yaml">5. Pod YAML Skeleton</a>
    <a href="#deploy-svc">6. Deployment + Service</a>
    <a href="#scheduling">7. Scheduling</a>
    <a href="#resources">8. Resources & Quotas</a>
    <a href="#ds-job">9. DaemonSet, Job, CronJob</a>
    <a href="#static-pods">10. Static Pods</a>
    <a href="#cm-secret">11. ConfigMaps & Secrets</a>
    <a href="#multi-container">12. Multi-Container & Init</a>
    <a href="#probes">13. Probes</a>
    <a href="#rollouts">14. Rolling Updates</a>
    <a href="#hpa">15. HPA</a>
    <a href="#rbac">16. RBAC</a>
    <a href="#tls-csr">17. TLS & Certificates</a>
    <a href="#kubeconfig">18. kubeconfig</a>
    <a href="#security-ctx">19. Security Contexts</a>
    <a href="#netpol">20. NetworkPolicy</a>
    <a href="#storage">21. Storage</a>
    <a href="#networking">22. Services & Networking</a>
    <a href="#dns">23. DNS & CoreDNS</a>
    <a href="#ingress">24. Ingress</a>
    <a href="#etcd">25. ETCD Backup & Restore</a>
    <a href="#upgrade">26. Cluster Upgrade</a>
    <a href="#node-maint">27. Node Maintenance</a>
    <a href="#pdb">28. PDB</a>
    <a href="#troubleshoot">29. Troubleshooting</a>
    <a href="#jsonpath">30. JSONPath & Output</a>
    <a href="#helm">31. Helm</a>
    <a href="#kustomize">32. Kustomize</a>
    <a href="#kubeadm">33. kubeadm Install</a>
    <a href="#crd">34. CRDs & Operators</a>
    <a href="#practice">35. Practice Scenarios</a>
    <a href="#exam-tips">36. Exam Tips</a>
  </nav>

  <!-- ═══════════ MAIN CONTENT ═══════════ -->
  <main>

    <!-- TITLE -->
    <div style="text-align:center; padding: 2rem 0 1rem;">
      <h1 style="border:none; font-size:2.5rem; margin:0;">CKA EXAM</h1>
      <p style="font-size:1.4rem; color:var(--accent2); font-weight:600; margin:.3rem 0;">Comprehensive Cheat Sheet</p>
      <p style="color:var(--text-muted);">Certified Kubernetes Administrator &mdash; All Imperative Commands | YAML
        Configs | Troubleshooting</p>
      <div class="exam-note" style="display:inline-block; text-align:left; margin-top:1rem;">
        <strong>Exam:</strong> 2 hours | 66% to pass | One tab: kubernetes.io/docs<br>
        <strong>Domains:</strong> Architecture 25% | Workloads 15% | Networking 20% | Storage 10% | Troubleshooting 30%
      </div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="setup">1. Exam Environment Setup &amp; Shell Aliases</h1>
    <p>Run these commands <strong>FIRST</strong> when the exam starts:</p>

    <h3>Shell Aliases (~/.bashrc)</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>alias k=kubectl</code><span class="tip-icon" data-tip="Master alias for kubectl. Saves keystrokes on every command. Type 'k' instead of 'kubectl' throughout the exam.">i</span></div>
      <div class="cmd-row"><code>alias kn='kubectl config set-context --current --namespace'</code><span class="tip-icon" data-tip="Quick namespace switch. Usage: kn &lt;namespace&gt;. Sets default namespace for all subsequent commands without -n flag.">i</span></div>
      <div class="cmd-row"><code>alias kgp='kubectl get pods'</code><span class="tip-icon" data-tip="Fast pod listing. Shows all pods in current namespace with status, restarts, and ready state. Add -A for all namespaces.">i</span></div>
      <div class="cmd-row"><code>alias kgs='kubectl get svc'</code><span class="tip-icon" data-tip="Lists all Services in current namespace with cluster IPs, external IPs, and ports. Useful for quick service endpoint checks.">i</span></div>
      <div class="cmd-row"><code>alias kgn='kubectl get nodes'</code><span class="tip-icon" data-tip="Lists all worker nodes with their status (Ready/NotReady), roles, and age. Add -o wide for IP addresses and container runtime.">i</span></div>
      <div class="cmd-row"><code>alias kga='kubectl get all'</code><span class="tip-icon" data-tip="Shows all resources (pods, services, deployments, etc.) in current namespace at a glance. Useful for quick cluster overview.">i</span></div>
      <div class="cmd-row"><code>alias kaf='kubectl apply -f'</code><span class="tip-icon" data-tip="Quickly apply YAML files. Usage: kaf &lt;file.yaml&gt;. Idempotent — safe to run multiple times.">i</span></div>
      <div class="cmd-row"><code>alias kdp='kubectl describe pod'</code><span class="tip-icon" data-tip="Detailed pod inspection showing spec, status, events, and errors. Essential for debugging pod failures on the exam.">i</span></div>
      <div class="cmd-row"><code>alias kl='kubectl logs'</code><span class="tip-icon" data-tip="View container logs. Usage: kl &lt;pod&gt; [&lt;container&gt;]. Add -f to tail (follow) logs in real time. Add -p for previous (crashed) container logs.">i</span></div>
      <div class="cmd-row"><code>alias ke='kubectl exec -it'</code><span class="tip-icon" data-tip="Execute interactive commands inside a pod. Usage: ke &lt;pod&gt; -- /bin/bash or /bin/sh. Essential for container debugging.">i</span></div>
      <div class="cmd-row"><code>source &lt;(kubectl completion bash)</code><span class="tip-icon" data-tip="Enables kubectl bash autocompletion. After running, press TAB to autocomplete resource names, flags, and subcommands. Must source this in ~/.bashrc.">i</span></div>
      <div class="cmd-row"><code>complete -o default -F __start_kubectl k</code><span class="tip-icon" data-tip="Extends autocompletion to the 'k' alias. Without this, TAB won't work after your alias. Run this after sourcing kubectl completion.">i</span></div>
      <div class="cmd-row"><code>export KUBE_EDITOR=vi</code><span class="tip-icon" data-tip="Sets default editor for 'kubectl edit' commands. vi is preferred on exam systems as it's always available. Change to nano if you prefer.">i</span></div>
    </div>

    <h3>Vim Settings (~/.vimrc)</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>set tabstop=2</code><span class="tip-icon" data-tip="Display tab characters as 2-space width. Essential for YAML editing where indentation must be exact (YAML uses 2-space indent).">i</span></div>
      <div class="cmd-row"><code>set shiftwidth=2</code><span class="tip-icon" data-tip="Auto-indent uses 2 spaces when you press Tab or use the &gt;&gt; command. Maintains YAML formatting requirements.">i</span></div>
      <div class="cmd-row"><code>set expandtab</code><span class="tip-icon" data-tip="Convert all Tab key presses into spaces instead of literal tab characters. Required for YAML — never mix tabs and spaces.">i</span></div>
      <div class="cmd-row"><code>set number</code><span class="tip-icon" data-tip="Display line numbers on the left margin. Helps navigate large YAML files and quickly find errors by line number.">i</span></div>
      <div class="cmd-row"><code>set autoindent</code><span class="tip-icon" data-tip="Automatically indent new lines to match the previous line's indentation. Speeds up YAML multi-line writing.">i</span></div>
    </div>

    <div class="exam-note"><strong>EXAM:</strong> Switch context at the start of EVERY question:
      <code>kubectl config use-context &lt;context-name&gt;</code>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="imperative">2. Imperative Commands &mdash; Speed Reference</h1>
    <p>Use imperative commands with <code>--dry-run=client -o yaml</code> to generate YAML fast, then edit as needed.
    </p>

    <h3>Pod</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt;</code><span class="tip-icon" data-tip="Creates a Pod with default restart policy (Always). Kubernetes restarts the container on failure. Quick way to spin up a running pod.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; --restart=Never</code><span class="tip-icon" data-tip="Creates a standalone Pod that does NOT restart on failure. Ideal for one-off debug tasks or exam quick tests.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; --dry-run=client -o yaml &gt; pod.yaml</code><span class="tip-icon" data-tip="Generates Pod YAML without creating any resource. --dry-run=client is client-side only (no API call). Redirect &gt; saves to file so you can edit before applying.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; --port=80</code><span class="tip-icon" data-tip="Declares containerPort 80 in the Pod spec. Does NOT create a Service or open a firewall rule — purely informational metadata.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; --labels=&quot;app=web,env=prod&quot;</code><span class="tip-icon" data-tip="Attaches key=value labels to the Pod. Labels are used by Selectors, Services, NetworkPolicies, and scheduling rules to target specific pods.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; --command -- sleep 3600</code><span class="tip-icon" data-tip="Overrides the container ENTRYPOINT (maps to spec.containers[].command). Everything after -- is the new command. Use to keep pods running for debugging.">i</span></div>
      <div class="cmd-row"><code>k run &lt;name&gt; --image=&lt;img&gt; -- &lt;arg1&gt; &lt;arg2&gt;</code><span class="tip-icon" data-tip="Passes arguments to the container's default ENTRYPOINT (maps to spec.containers[].args / Docker CMD). Does NOT override the entrypoint itself.">i</span></div>
    </div>

    <h3>Deployment</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create deployment &lt;name&gt; --image=&lt;img&gt; [--replicas=N]</code><span class="tip-icon" data-tip="Creates a Deployment managing N replicas of the container with rolling-update and self-healing support. Default replicas=1 if omitted.">i</span></div>
      <div class="cmd-row"><code>k create deployment &lt;name&gt; --image=&lt;img&gt; --dry-run=client -o yaml &gt; deploy.yaml</code><span class="tip-icon" data-tip="Outputs Deployment YAML to file without creating it. Edit the file to add env vars, resources, volumes, etc., then apply.">i</span></div>
      <div class="cmd-row"><code>k scale deployment &lt;name&gt; --replicas=N</code><span class="tip-icon" data-tip="Immediately adjusts the desired replica count. Kubernetes adds or removes pods to match the new count.">i</span></div>
      <div class="cmd-row"><code>k set image deployment/&lt;name&gt; &lt;container&gt;=&lt;img&gt;</code><span class="tip-icon" data-tip="Updates the image of a named container in the Deployment. Triggers a rolling update — old pods are gradually replaced with new ones.">i</span></div>
      <div class="cmd-row"><code>k rollout status deployment/&lt;name&gt;</code><span class="tip-icon" data-tip="Watches and reports the rollout progress in real time. Exits with code 0 on success, non-zero on failure. Useful in CI/CD pipelines.">i</span></div>
      <div class="cmd-row"><code>k rollout history deployment/&lt;name&gt;</code><span class="tip-icon" data-tip="Lists revision history of the Deployment. Add --revision=N to see details of a specific revision. Requires --record flag or change-cause annotation to see useful messages.">i</span></div>
      <div class="cmd-row"><code>k rollout undo deployment/&lt;name&gt;</code><span class="tip-icon" data-tip="Rolls back to the immediately previous revision. Safe way to recover from a bad image update without downtime.">i</span></div>
      <div class="cmd-row"><code>k rollout undo deployment/&lt;name&gt; --to-revision=2</code><span class="tip-icon" data-tip="Rolls back to a specific revision number. Check available revisions first with 'k rollout history deployment/&lt;name&gt;'.">i</span></div>
    </div>

    <h3>Service</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k expose deployment &lt;name&gt; --port=&lt;port&gt; [--name=&lt;svc-name&gt;]</code><span class="tip-icon" data-tip="Creates a ClusterIP Service targeting the Deployment's pods. --port is the Service port. --name overrides the default Service name (same as Deployment).">i</span></div>
      <div class="cmd-row"><code>k expose deployment &lt;name&gt; --port=80 --type=NodePort</code><span class="tip-icon" data-tip="Creates a NodePort Service. Kubernetes assigns a random port (30000–32767) on every node that forwards to the pods. Use --node-port=XXXXX to set a specific one.">i</span></div>
      <div class="cmd-row"><code>k expose pod &lt;name&gt; --port=80</code><span class="tip-icon" data-tip="Creates a ClusterIP Service targeting a single named Pod by its label selector. Useful for quickly testing connectivity to a specific pod.">i</span></div>
      <div class="cmd-row"><code>k create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml</code><span class="tip-icon" data-tip="Generates a NodePort Service YAML with an explicit node-port 30080 and tcp mapping 80:80 (servicePort:targetPort). Unlike 'expose', this lets you set the nodePort imperatively.">i</span></div>
      <div class="cmd-row"><code>k expose deployment &lt;name&gt; --port=80 --type=ClusterIP --dry-run=client -o yaml</code><span class="tip-icon" data-tip="Generates ClusterIP Service YAML without creating it. Edit the output to add targetPort, selector tweaks, or annotations, then apply.">i</span></div>
    </div>

    <h3>Namespace</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create namespace &lt;name&gt;</code><span class="tip-icon" data-tip="Creates a Namespace to logically isolate resources. Teams, environments (dev/staging/prod), or projects are commonly separated by namespace.">i</span></div>
      <div class="cmd-row"><code>k create ns &lt;name&gt;</code><span class="tip-icon" data-tip="Shorthand alias for 'kubectl create namespace'. Functionally identical — saves keystrokes on the exam.">i</span></div>
    </div>

    <h3>ConfigMap &amp; Secret</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create configmap &lt;name&gt; --from-literal=KEY=val [--from-literal=KEY2=val2]</code><span class="tip-icon" data-tip="Creates a ConfigMap with inline key/value pairs. Each --from-literal adds one entry. Inject into pods as env vars (envFrom) or as volume files.">i</span></div>
      <div class="cmd-row"><code>k create configmap &lt;name&gt; --from-file=path/to/file</code><span class="tip-icon" data-tip="Creates a ConfigMap where the filename becomes the key and the file contents become the value. Mount as a volume to make the file available inside the pod.">i</span></div>
      <div class="cmd-row"><code>k create configmap &lt;name&gt; --from-file=key=path</code><span class="tip-icon" data-tip="Same as --from-file but overrides the key name. Useful when you want a specific key name regardless of the actual filename.">i</span></div>
      <div class="cmd-row"><code>k create secret generic &lt;name&gt; --from-literal=KEY=val</code><span class="tip-icon" data-tip="Creates a generic Secret. Values are base64-encoded automatically by Kubernetes. Access in pods via env vars or volume mounts.">i</span></div>
      <div class="cmd-row"><code>k create secret generic &lt;name&gt; --from-file=key=path</code><span class="tip-icon" data-tip="Creates a Secret from a file on disk, e.g. a private key or config file. File contents are base64-encoded and stored under the specified key.">i</span></div>
      <div class="cmd-row"><code>k create secret tls &lt;name&gt; --cert=tls.crt --key=tls.key</code><span class="tip-icon" data-tip="Creates a TLS type Secret with tls.crt and tls.key entries. Used by Ingress resources to terminate HTTPS. Certificate must be valid PEM format.">i</span></div>
      <div class="cmd-row"><code>k create secret docker-registry regcred --docker-server=&lt;url&gt; --docker-username=&lt;user&gt; --docker-password=&lt;pass&gt; --docker-email=&lt;email&gt;</code><span class="tip-icon" data-tip="Creates an imagePullSecret for private container registries. Reference it in a Pod spec under imagePullSecrets to allow pulling private images.">i</span></div>
    </div>

    <h3>ServiceAccount</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create serviceaccount &lt;name&gt;</code><span class="tip-icon" data-tip="Creates a ServiceAccount that pods can use to authenticate to the Kubernetes API. Assign RBAC roles/bindings to control what the pod can do.">i</span></div>
      <div class="cmd-row"><code>k create sa &lt;name&gt;</code><span class="tip-icon" data-tip="Shorthand alias for 'kubectl create serviceaccount'. Functionally identical — saves keystrokes on the exam.">i</span></div>
      <div class="cmd-row"><code>k create token &lt;sa-name&gt;    # short-lived token (K8s 1.24+)</code><span class="tip-icon" data-tip="Generates a short-lived JWT token for a ServiceAccount (TokenRequest API). Use --duration to set expiry. Replaces the old auto-mounted long-lived secret tokens removed in K8s 1.24.">i</span></div>
    </div>

    <h3>Job &amp; CronJob</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create job &lt;name&gt; --image=busybox -- echo hello</code><span class="tip-icon" data-tip="Creates a Job that runs 'echo hello' once and completes. Jobs guarantee the pod runs to successful completion (exit 0). Kubernetes retries on failure.">i</span></div>
      <div class="cmd-row"><code>k create job &lt;name&gt; --image=busybox --dry-run=client -o yaml -- echo hello</code><span class="tip-icon" data-tip="Generates Job YAML without creating it. Edit to add completions, parallelism, backoffLimit, or activeDeadlineSeconds before applying.">i</span></div>
      <div class="cmd-row"><code>k create cronjob &lt;name&gt; --image=busybox --schedule=&quot;0 * * * *&quot; -- echo hi</code><span class="tip-icon" data-tip="Creates a CronJob that runs 'echo hi' at the top of every hour (cron syntax: min hour dom month dow). Each run spawns a new Job.">i</span></div>
      <div class="cmd-row"><code>k create cronjob &lt;name&gt; --image=busybox --schedule=&quot;*/5 * * * *&quot; --dry-run=client -o yaml -- echo hi</code><span class="tip-icon" data-tip="Generates CronJob YAML. Edit to add concurrencyPolicy (Allow/Forbid/Replace), successfulJobsHistoryLimit, or failedJobsHistoryLimit before applying.">i</span></div>
    </div>

    <h3>RBAC</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create role &lt;name&gt; --verb=get,list,create,delete --resource=pods -n &lt;ns&gt;</code><span class="tip-icon" data-tip="Creates a namespaced Role granting specific verbs on a resource. Verbs: get, list, watch, create, update, patch, delete. Must then bind with RoleBinding to take effect.">i</span></div>
      <div class="cmd-row"><code>k create rolebinding &lt;name&gt; --role=&lt;role&gt; --user=&lt;user&gt; -n &lt;ns&gt;</code><span class="tip-icon" data-tip="Binds a Role to a user in a specific namespace. The user gains all permissions defined in the Role — but only within that namespace.">i</span></div>
      <div class="cmd-row"><code>k create rolebinding &lt;name&gt; --role=&lt;role&gt; --serviceaccount=&lt;ns&gt;:&lt;sa&gt; -n &lt;ns&gt;</code><span class="tip-icon" data-tip="Binds a Role to a ServiceAccount (format: namespace:serviceaccount-name). Used to grant pods (via their SA) access to namespace resources.">i</span></div>
      <div class="cmd-row"><code>k create clusterrole &lt;name&gt; --verb=get,list --resource=nodes</code><span class="tip-icon" data-tip="Creates a cluster-scoped Role. ClusterRoles can grant access to non-namespaced resources (nodes, PVs, namespaces) or be reused across all namespaces.">i</span></div>
      <div class="cmd-row"><code>k create clusterrolebinding &lt;name&gt; --clusterrole=&lt;name&gt; --user=&lt;user&gt;</code><span class="tip-icon" data-tip="Binds a ClusterRole to a user cluster-wide. The user can perform the allowed actions on matching resources in ALL namespaces and cluster-scoped resources.">i</span></div>
      <div class="cmd-row"><code>k create clusterrolebinding &lt;name&gt; --clusterrole=&lt;name&gt; --serviceaccount=&lt;ns&gt;:&lt;sa&gt;</code><span class="tip-icon" data-tip="Binds a ClusterRole to a ServiceAccount cluster-wide. Common for operators or system components that need cross-namespace or node-level access.">i</span></div>
    </div>

    <h3>Ingress</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create ingress &lt;name&gt; --rule=&quot;host/path=svc:80&quot; --dry-run=client -o yaml</code><span class="tip-icon" data-tip="Generates Ingress YAML. Rule format: host/path=serviceName:port. Omit host for a default path rule. Always dry-run first and edit to add TLS, annotations (nginx class), or multiple rules.">i</span></div>
    </div>

    <h3>Generate → Edit → Apply Pattern</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k run mypod --image=nginx --dry-run=client -o yaml &gt; mypod.yaml</code><span class="tip-icon" data-tip="Step 1: Generate base YAML scaffold using imperative command. --dry-run=client prevents any API call; -o yaml outputs YAML; &gt; saves to file.">i</span></div>
      <div class="cmd-row"><code>vi mypod.yaml   # add labels, resources, volumes, etc.</code><span class="tip-icon" data-tip="Step 2: Edit the generated YAML to add fields that have no imperative flag: resource limits, volumeMounts, env vars, readiness probes, init containers, etc.">i</span></div>
      <div class="cmd-row"><code>k apply -f mypod.yaml</code><span class="tip-icon" data-tip="Step 3: Apply the edited YAML. 'apply' is idempotent — run again to update. On the exam this is the recommended way to create/update resources from files.">i</span></div>
      <div class="cmd-row"><code>k edit deployment nginx</code><span class="tip-icon" data-tip="Opens the live resource YAML in your editor ($KUBE_EDITOR or vi). Changes are applied on save. Cannot change immutable fields (e.g., pod spec in some cases).">i</span></div>
      <div class="cmd-row"><code>k replace --force -f pod.yaml</code><span class="tip-icon" data-tip="Deletes the existing resource and recreates it from file. Use when you need to change immutable fields (e.g., containerPort). WARNING: causes a brief outage.">i</span></div>
      <div class="cmd-row"><code>k delete pod x --force --grace-period=0</code><span class="tip-icon" data-tip="Immediately terminates a stuck or terminating pod without waiting for graceful shutdown. --grace-period=0 skips the default 30s drain. Use sparingly.">i</span></div>
    </div>

    <div class="tip"><strong>TIP:</strong> Always add <code>-n &lt;namespace&gt;</code> or
      <code>--namespace=&lt;ns&gt;</code> for namespace-scoped resources.
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="kubectl-ref">3. kubectl Quick Reference</h1>

    <h3>Inspect &amp; Debug</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pods -A -o wide</code><span class="tip-icon" data-tip="Lists all pods in ALL namespaces (-A = --all-namespaces). -o wide adds extra columns: node name, IP, nominated node. Great for a cluster-wide overview.">i</span></div>
      <div class="cmd-row"><code>k get all -n &lt;ns&gt;</code><span class="tip-icon" data-tip="Shows Pods, Services, Deployments, ReplicaSets, StatefulSets, DaemonSets, Jobs, and CronJobs in the given namespace. Does NOT include ConfigMaps, Secrets, PVCs etc.">i</span></div>
      <div class="cmd-row"><code>k describe pod &lt;name&gt;</code><span class="tip-icon" data-tip="Prints detailed info: events, conditions, container state, resource requests/limits, volumes, node assignment. First place to look when a pod is stuck in Pending or CrashLoopBackOff.">i</span></div>
      <div class="cmd-row"><code>k logs &lt;pod&gt; [-c &lt;container&gt;] [--previous] [-f]</code><span class="tip-icon" data-tip="Streams container stdout/stderr. -c selects a container in multi-container pods. --previous fetches logs of the last crashed container. -f follows/tails live output.">i</span></div>
      <div class="cmd-row"><code>k exec -it &lt;pod&gt; -- sh</code><span class="tip-icon" data-tip="Opens an interactive shell inside the running container. -i keeps stdin open, -t allocates a pseudo-TTY. Use 'bash' instead of 'sh' if the image has bash.">i</span></div>
      <div class="cmd-row"><code>k exec -it &lt;pod&gt; -c &lt;container&gt; -- sh</code><span class="tip-icon" data-tip="Same as above but targets a specific container by name. Required in multi-container pods (sidecars, init containers that are still running, etc.).">i</span></div>
      <div class="cmd-row"><code>k get events --sort-by='.lastTimestamp'</code><span class="tip-icon" data-tip="Lists cluster events sorted by time (newest last). Crucial for diagnosing image pull failures, scheduling errors, OOM kills, and node issues in the current namespace.">i</span></div>
      <div class="cmd-row"><code>k get events -A --sort-by='.lastTimestamp'</code><span class="tip-icon" data-tip="Same as above but across all namespaces. Add --field-selector type=Warning to filter only warning events and reduce noise.">i</span></div>
      <div class="cmd-row"><code>k top nodes</code><span class="tip-icon" data-tip="Shows live CPU and memory usage per node. Requires metrics-server to be installed. Use to identify resource pressure before scheduling new workloads.">i</span></div>
      <div class="cmd-row"><code>k top pods --sort-by=memory</code><span class="tip-icon" data-tip="Lists pod resource usage sorted by memory consumption. Useful for spotting memory leaks or oversized pods that could be evicted.">i</span></div>
      <div class="cmd-row"><code>k top pods --sort-by=cpu</code><span class="tip-icon" data-tip="Lists pod resource usage sorted by CPU consumption. Add -A for all namespaces. Add -n &lt;ns&gt; to scope to a namespace.">i</span></div>
    </div>

    <h3>Docs in Terminal (Exam Allowed)</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k explain pod</code><span class="tip-icon" data-tip="Displays the API schema and description for the Pod resource. Shows top-level fields, types, and a short description. No internet needed — works offline in the exam.">i</span></div>
      <div class="cmd-row"><code>k explain pod.spec.containers</code><span class="tip-icon" data-tip="Drills into the containers array field of pod.spec. Shows all container sub-fields (image, ports, env, command, args, etc.) with their types and descriptions.">i</span></div>
      <div class="cmd-row"><code>k explain pod.spec.containers.resources</code><span class="tip-icon" data-tip="Shows the resources field structure: requests (guaranteed minimum) and limits (maximum CPU/memory). Use to confirm the exact field names before writing YAML.">i</span></div>
      <div class="cmd-row"><code>k explain deployment.spec --recursive</code><span class="tip-icon" data-tip="Prints the entire field tree of deployment.spec in a collapsed hierarchy. --recursive shows all nested fields at once — great for discovering deeply nested field names quickly.">i</span></div>
      <div class="cmd-row"><code>k api-resources          # list all resource types</code><span class="tip-icon" data-tip="Lists every resource type the cluster supports: shortName, API group, namespaced flag, and kind. Use to find the correct resource name and group (e.g., 'ingresses' in 'networking.k8s.io').">i</span></div>
      <div class="cmd-row"><code>k api-resources --namespaced=true</code><span class="tip-icon" data-tip="Filters to only namespace-scoped resources (pods, deployments, services, etc.). Helpful to confirm whether a resource type requires -n &lt;namespace&gt;.">i</span></div>
      <div class="cmd-row"><code>k api-resources --namespaced=false</code><span class="tip-icon" data-tip="Filters to only cluster-scoped resources (nodes, PersistentVolumes, ClusterRoles, Namespaces, etc.). These exist outside any namespace.">i</span></div>
      <div class="cmd-row"><code>k api-versions           # list all API versions</code><span class="tip-icon" data-tip="Lists all API groups and versions available in the cluster (e.g., apps/v1, networking.k8s.io/v1). Use this to set the correct apiVersion in YAML manifests.">i</span></div>
    </div>

    <h3>Edit / Patch / Delete</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k edit deploy &lt;name&gt;</code><span class="tip-icon" data-tip="Opens the live Deployment YAML in your $KUBE_EDITOR (default: vi). Changes apply on save. Cannot change immutable fields; use 'replace --force' for those.">i</span></div>
      <div class="cmd-row"><code>k patch svc s1 -p '{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'</code><span class="tip-icon" data-tip="Applies a strategic merge patch to a resource inline without opening an editor. Ideal for changing a single field quickly. -p accepts JSON. Use --type=json for JSON Patch or --type=merge for a plain merge patch.">i</span></div>
      <div class="cmd-row"><code>k replace --force -f pod.yaml</code><span class="tip-icon" data-tip="Deletes the existing resource immediately, then recreates it from file. Required to change immutable fields (e.g., container ports). Causes a brief outage — use only when necessary.">i</span></div>
      <div class="cmd-row"><code>k delete pod x --force --grace-period=0</code><span class="tip-icon" data-tip="Immediately terminates a Pod, bypassing the default 30-second graceful shutdown window. Use on stuck or Terminating pods. Data in emptyDir volumes is lost immediately.">i</span></div>
    </div>

    <h3>Label / Annotate</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k label pod &lt;name&gt; env=prod</code><span class="tip-icon" data-tip="Adds or sets the label 'env=prod' on the pod. Labels are key/value metadata used by selectors in Services, NetworkPolicies, and scheduling rules.">i</span></div>
      <div class="cmd-row"><code>k label pod &lt;name&gt; env=staging --overwrite</code><span class="tip-icon" data-tip="Updates an existing label value. Without --overwrite, kubectl rejects the command if the label key already exists to prevent accidental overwrites.">i</span></div>
      <div class="cmd-row"><code>k label pod &lt;name&gt; env-                      # remove label</code><span class="tip-icon" data-tip="Removes the 'env' label from the pod. The trailing dash (-) is the kubectl syntax to delete a label key. Does not affect other labels.">i</span></div>
      <div class="cmd-row"><code>k label node &lt;name&gt; disk=ssd</code><span class="tip-icon" data-tip="Adds a label to a node. Node labels are used by nodeSelector and nodeAffinity in pod specs to constrain scheduling to specific nodes.">i</span></div>
      <div class="cmd-row"><code>k annotate pod &lt;name&gt; key=value</code><span class="tip-icon" data-tip="Adds a non-identifying annotation to the pod. Annotations store arbitrary metadata (build info, owner, URLs) not used for selection — unlike labels.">i</span></div>
      <div class="cmd-row"><code>k get pods --selector app=web</code><span class="tip-icon" data-tip="Filters pods by label selector. Only pods with 'app=web' are returned. Equivalent to -l app=web. Works with any resource type.">i</span></div>
      <div class="cmd-row"><code>k get pods --selector app=web,env=prod       # AND logic</code><span class="tip-icon" data-tip="Multiple comma-separated selectors are ANDed — returns pods that match ALL conditions. There is no built-in OR selector in kubectl; use -o json | jq for complex filtering.">i</span></div>
      <div class="cmd-row"><code>k get pods --show-labels</code><span class="tip-icon" data-tip="Adds a LABELS column to the output showing all key=value pairs on each pod. Useful to inspect what labels are currently set before running a selector.">i</span></div>
      <div class="cmd-row"><code>k get pods -L app,env                        # show as columns</code><span class="tip-icon" data-tip="Shows specific label keys as individual columns in the output. Cleaner than --show-labels when you only care about a few specific labels.">i</span></div>
      <div class="cmd-row"><code>k get pods --selector env=production --no-headers | wc -l</code><span class="tip-icon" data-tip="Counts pods matching the selector. --no-headers suppresses the header row so wc -l counts only data lines. Common exam pattern for 'how many pods are in production?'">i</span></div>
    </div>

    <h3>Context &amp; Namespace</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k config use-context &lt;ctx&gt;</code><span class="tip-icon" data-tip="Switches the active kubeconfig context. Each context bundles a cluster, user, and namespace. On the CKA exam you are always told to switch context first — do this before every task.">i</span></div>
      <div class="cmd-row"><code>k config current-context</code><span class="tip-icon" data-tip="Prints the name of the currently active context. Run this to confirm you are on the right cluster before executing exam tasks.">i</span></div>
      <div class="cmd-row"><code>k config get-contexts</code><span class="tip-icon" data-tip="Lists all contexts defined in your kubeconfig with their cluster, user, and namespace. The active context is marked with an asterisk (*) in the CURRENT column.">i</span></div>
      <div class="cmd-row"><code>k config set-context --current --namespace=dev</code><span class="tip-icon" data-tip="Sets a default namespace for the current context so you don't need -n dev on every command. Lasts for the life of the context or until changed.">i</span></div>
      <div class="cmd-row"><code>k config view</code><span class="tip-icon" data-tip="Prints the full merged kubeconfig file (clusters, users, contexts). Use --minify to show only the active context. Add -o jsonpath to extract a specific field.">i</span></div>
    </div>

    <h3>Node Maintenance</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k cordon &lt;node&gt;</code><span class="tip-icon" data-tip="Marks the node as unschedulable (SchedulingDisabled). Existing pods continue to run; no new pods will be scheduled on this node. First step before draining for maintenance.">i</span></div>
      <div class="cmd-row"><code>k drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data</code><span class="tip-icon" data-tip="Evicts all pods from the node (respecting PodDisruptionBudgets). --ignore-daemonsets skips DaemonSet-managed pods (they cannot be removed). --delete-emptydir-data allows evicting pods with emptyDir volumes (data is lost).">i</span></div>
      <div class="cmd-row"><code>k drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data --force</code><span class="tip-icon" data-tip="Same as above but also evicts pods not managed by a controller (bare pods). Without --force, drain refuses to evict unmanaged pods since they won't be rescheduled.">i</span></div>
      <div class="cmd-row"><code>k uncordon &lt;node&gt;</code><span class="tip-icon" data-tip="Re-enables scheduling on the node after maintenance. The node becomes eligible to receive new pods again. Run after kubeadm upgrade or node patching is complete.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="core">4. Core Concepts &mdash; Cluster Architecture</h1>

    <h2>Control Plane Components</h2>
    <p><strong>kube-apiserver:</strong> Front-end REST API. Only component that talks to etcd. Authenticates,
      authorizes, validates requests.</p>
    <p><strong>etcd:</strong> Distributed key-value store. Uses RAFT consensus. Single source of truth. Backup with
      etcdctl snapshot.</p>
    <p><strong>kube-scheduler:</strong> Watches unscheduled pods. Filtering (predicates) → Scoring (priorities) →
      Binding.</p>
    <p><strong>kube-controller-manager:</strong> Runs controllers (Node, Replication, Endpoints). Reconciliation loop:
      current state → desired state.</p>
    <p><strong>cloud-controller-manager:</strong> Cloud-specific logic (nodes, LBs, routes).</p>

    <h2>Worker Node Components</h2>
    <p><strong>kubelet:</strong> Agent on each node. Registers node, ensures containers run. Reads PodSpecs from API
      server or static files.</p>
    <p><strong>kube-proxy:</strong> Network proxy. Programs iptables/IPVS for Service routing.</p>
    <p><strong>Container Runtime:</strong> containerd or CRI-O (Docker removed in K8s 1.24+). Uses CRI standard.</p>

    <h3>crictl Commands (CRI debugging)</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>crictl ps -a</code><span class="tip-icon" data-tip="Lists all containers on the node, including both running and stopped. Shows container ID, image, state, and name. Essential for debugging CRI issues when kubectl commands fail.">i</span></div>
      <div class="cmd-row"><code>crictl images</code><span class="tip-icon" data-tip="Lists all container images cached on the node. Shows image ID, repository tag, and size. Use to verify if an image is already present or check for disk space issues.">i</span></div>
      <div class="cmd-row"><code>crictl pods</code><span class="tip-icon" data-tip="Lists all pods on the node (even if not in API server). Shows pod ID, name, namespace, and state. Useful when investigating pods that don't appear in kubectl get pods.">i</span></div>
      <div class="cmd-row"><code>crictl logs &lt;container-id&gt;</code><span class="tip-icon" data-tip="Fetches container logs directly from the CRI runtime. Use when kubectl logs fails due to API issues or for containers in non-Running state.">i</span></div>
      <div class="cmd-row"><code>crictl inspect &lt;container-id&gt;</code><span class="tip-icon" data-tip="Prints detailed metadata about a container including image, mounts, network namespace, environment variables, and runtime config. JSON output — useful for scripting.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="pod-yaml">5. Pod YAML Skeleton (Complete)</h1>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Step 1:</strong> Generate the base YAML skeleton with imperative command:<br>
      <pre><code># Generate base pod YAML
k run mypod --image=nginx:1.21 --port=80 --labels="app=myapp" \
  --dry-run=client -o yaml > mypod.yaml

# With a command override
k run mypod --image=busybox --command --dry-run=client -o yaml -- sleep 3600 > mypod.yaml</code></pre>
      <strong>Step 2:</strong> The above generates a minimal Pod spec. Open with <code>vi mypod.yaml</code> and add
      these fields manually:
      <ul>
        <li><code>spec.serviceAccountName</code> — add under spec</li>
        <li><code>spec.nodeSelector</code> — add under spec for scheduling</li>
        <li><code>spec.tolerations</code> — add under spec for tainted nodes</li>
        <li><code>spec.initContainers</code> — add before <code>containers:</code></li>
        <li><code>env / envFrom</code> — add under the container for ConfigMap/Secret injection</li>
        <li><code>resources.requests/limits</code> — add under the container</li>
        <li><code>livenessProbe / readinessProbe</code> — add under the container</li>
        <li><code>volumeMounts</code> + <code>volumes</code> — add under container and spec respectively</li>
        <li><code>securityContext</code> — add at pod-level or container-level</li>
      </ul>
      <strong>Step 3:</strong> Apply: <code>k apply -f mypod.yaml</code>
    </div>

    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
  namespace: default
  labels:
    app: myapp
spec:
  serviceAccountName: my-sa
  nodeSelector:
    disk: ssd
  tolerations:
  - key: "node-role"
    operator: "Exists"
    effect: "NoSchedule"
  initContainers:
  - name: init
    image: busybox
    command: ['sh','-c','sleep 5']
  containers:
  - name: app
    image: nginx:1.21
    ports:
    - containerPort: 80
    command: ["sleep"]          # overrides ENTRYPOINT
    args: ["3600"]              # overrides CMD
    env:
    - name: KEY
      value: val
    - name: FROM_CM
      valueFrom:
        configMapKeyRef:
          name: my-cm
          key: APP_KEY
    - name: FROM_SECRET
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_PASS
    envFrom:
    - configMapRef:
        name: my-cm
    - secretRef:
        name: my-secret
    resources:
      requests: { cpu: 100m, memory: 128Mi }
      limits:   { cpu: 500m, memory: 256Mi }
    livenessProbe:
      httpGet: { path: /healthz, port: 80 }
      initialDelaySeconds: 10
      periodSeconds: 20
    readinessProbe:
      httpGet: { path: /ready, port: 80 }
      initialDelaySeconds: 5
      periodSeconds: 10
    volumeMounts:
    - name: data
      mountPath: /data
    - name: config-vol
      mountPath: /etc/config
    securityContext:
      runAsUser: 1000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      capabilities:
        add: ["NET_ADMIN"]
        drop: ["ALL"]
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: my-pvc
  - name: config-vol
    configMap:
      name: my-cm</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="deploy-svc">6. Deployment + Service YAML</h1>

    <h2>Deployment</h2>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Generate Deployment YAML:</strong>
      <pre><code># Create directly (fastest)
k create deployment webapp --image=nginx:1.21 --replicas=3

# Generate YAML to customize
k create deployment webapp --image=nginx:1.21 --replicas=3 \
  --dry-run=client -o yaml > deploy.yaml</code></pre>
      <strong>What the command generates vs what you need to add:</strong>
      <pre><code># ✅ Generated automatically:
apiVersion: apps/v1
kind: Deployment
metadata: { name: webapp }
spec:
  replicas: 3
  selector: { matchLabels: { app: webapp } }   # auto-matched
  template:
    metadata: { labels: { app: webapp } }       # auto-matched
    spec:
      containers:
      - name: nginx
        image: nginx:1.21

# ✏️ Add manually in vi:
#   spec.strategy          → rollingUpdate config
#   containers[].ports     → containerPort: 80
#   containers[].resources → requests/limits</code></pre>
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 1, maxUnavailable: 0 }
  selector:
    matchLabels: { app: webapp }
  template:
    metadata:
      labels: { app: webapp }
    spec:
      containers:
      - name: webapp
        image: nginx:1.21
        ports: [{ containerPort: 80 }]
        resources:
          requests: { cpu: 100m, memory: 128Mi }
          limits:   { cpu: 500m, memory: 256Mi }</code></pre>

    <h2>Service (NodePort)</h2>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Create Service imperatively:</strong>
      <pre><code># Expose existing deployment as NodePort (fastest)
k expose deployment webapp --name=webapp-svc --port=80 --type=NodePort

# Generate YAML (to set specific nodePort)
k expose deployment webapp --name=webapp-svc --port=80 \
  --type=NodePort --dry-run=client -o yaml > svc.yaml

# With specific nodePort (can't set via imperative, must edit YAML)
vi svc.yaml   # add nodePort: 30080 under ports
k apply -f svc.yaml

# Alternative: create service directly with nodePort
k create service nodeport webapp-svc --tcp=80:80 \
  --node-port=30080 --dry-run=client -o yaml > svc.yaml
# ⚠️ Note: this won't set the selector correctly — edit selector to match pod labels</code></pre>
      <strong>Key difference:</strong> <code>k expose</code> auto-copies selector from the deployment.
      <code>k create service</code> uses <code>app: &lt;svc-name&gt;</code> as selector, which may not match.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
spec:
  type: NodePort
  selector: { app: webapp }
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080</code></pre>

    <h2>Service (ClusterIP)</h2>
    <div class="imp-block">
      <strong>Imperative:</strong> <code>k expose deployment back-end --port=80 --name=back-end</code> (ClusterIP is
      default type)
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp</code></pre>

    <p><strong>Service Types:</strong> ClusterIP (internal, default) | NodePort (node port 30000-32767) | LoadBalancer
      (cloud LB)</p>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="scheduling">7. Scheduling &mdash; Taints, Tolerations, Affinity, nodeSelector</h1>

    <h2>Taints &amp; Tolerations</h2>
    <p>Taints on <strong>NODES</strong> repel pods. Tolerations on <strong>PODS</strong> allow them onto tainted nodes.
    </p>
    <p><strong>Effects:</strong> NoSchedule | PreferNoSchedule | NoExecute (evicts existing pods)</p>
    <div class="cmd-table">
      <div class="cmd-row"><code>k taint nodes node01 app=blue:NoSchedule</code><span class="tip-icon" data-tip="Adds a taint to a node. Format: key=value:Effect. NoSchedule prevents new pods unless they tolerate the taint. Existing pods are NOT affected — use NoExecute for that.">i</span></div>
      <div class="cmd-row"><code>k taint nodes node01 app=blue:NoSchedule-</code><span class="tip-icon" data-tip="Removes a specific taint from a node. The trailing minus (-) must match the exact taint (key=value:Effect). Without it, the taint is added instead of removed.">i</span></div>
      <div class="cmd-row"><code>k taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-</code><span class="tip-icon" data-tip="Removes the control-plane taint that prevents user pods from scheduling on the control plane. Essential for setting up single-node clusters (e.g. minikube, lab environments).">i</span></div>
      <div class="cmd-row"><code>k describe node node01 | grep -i taint</code><span class="tip-icon" data-tip="Shows all taints on a node along with their effects and toleration seconds. Useful to verify current taints before scheduling pods or applying tolerations.">i</span></div>
    </div>

    <h3>Toleration in Pod YAML</h3>
    <span class="yaml-label">YAML</span>
    <pre><code>tolerations:
- key: "app"
  operator: "Equal"       # or "Exists" (matches any value)
  value: "blue"
  effect: "NoSchedule"
# NoExecute toleration with grace period:
- key: "node.kubernetes.io/not-ready"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 300</code></pre>

    <h2>nodeSelector (Simplest)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k label nodes node01 size=Large</code><span class="tip-icon" data-tip="Adds a label to a node for use with nodeSelector or nodeAffinity. Format: key=value. Labels persist until explicitly removed or the node is deleted.">i</span></div>
      <div class="cmd-row"><code>k label nodes node01 size-</code><span class="tip-icon" data-tip="Removes the 'size' label from a node. The trailing dash (-) deletes the label key. Pods previously constrained by nodeSelector to this label can now scatter to other nodes.">i</span></div>
    </div>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Generate pod with nodeSelector:</strong>
      <pre><code># Generate base YAML, then add nodeSelector
k run scheduled-pod --image=nginx --dry-run=client -o yaml > pod.yaml
vi pod.yaml
# Add under spec:
#   nodeSelector:
#     size: Large
k apply -f pod.yaml</code></pre>
      <strong>Note:</strong> <code>k run</code> does not support <code>--node-selector</code> flag. You must generate
      YAML and add <code>nodeSelector</code> manually.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  nodeSelector:
    size: Large</code></pre>

    <h2>Node Affinity (Advanced)</h2>
    <div class="imp-block">
      <strong>No imperative shortcut</strong> — Node affinity must be added via YAML. Use
      <code>k run &lt;name&gt; --image=&lt;img&gt; --dry-run=client -o yaml</code> to generate base, then add the
      <code>affinity</code> block under <code>spec:</code>. Use
      <code>k explain pod.spec.affinity.nodeAffinity --recursive</code> to quickly look up the structure.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:   # HARD rule
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In          # In, NotIn, Exists, DoesNotExist, Gt, Lt
            values: [Large, Medium]
      preferredDuringSchedulingIgnoredDuringExecution:  # SOFT rule
      - weight: 50
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values: [ssd]</code></pre>

    <h2>Manual Scheduling (nodeName)</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  nodeName: node02        # bypasses scheduler entirely</code></pre>
    <div class="exam-note"><strong>EXAM:</strong> nodeName can only be set at creation time. To move a pod: export YAML
      → edit → delete → recreate.</div>

    <h2>Dedicated Nodes Pattern (Taint + Affinity)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k taint nodes node01 dedicated=team-a:NoSchedule</code><span class="tip-icon" data-tip="Step 1: Taint the node to repel general pods. Only pods with the matching toleration can be scheduled.">i</span></div>
      <div class="cmd-row"><code>k label nodes node01 dedicated=team-a</code><span class="tip-icon" data-tip="Step 2: Label the node with the same key. Used for nodeAffinity to ensure only the team's pods (with toleration) land on this node.">i</span></div>
      <div class="cmd-row"><code>Pod spec: toleration + nodeAffinity (see YAML)</code><span class="tip-icon" data-tip="Step 3: Pod must have a matching toleration (to pass the taint) AND nodeAffinity/nodeSelector pointing to the label. Together they guarantee the pod only runs on dedicated nodes.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="resources">8. Resource Limits, LimitRange, ResourceQuota</h1>

    <h2>Resource Requests &amp; Limits</h2>
    <p><strong>Requests</strong> = guaranteed allocation (scheduler uses). <strong>Limits</strong> = max at runtime.</p>
    <p>CPU &gt; limit → <strong>throttled</strong> (never killed). Memory &gt; limit → <strong>OOMKilled</strong> (exit
      137).</p>
    <p>CPU: 1 = 1 vCPU, 500m = 0.5 CPU. Memory: Mi (mebibytes), Gi (gibibytes).</p>

    <span class="yaml-label">YAML</span>
    <pre><code>resources:
  requests:
    memory: "64Mi"
    cpu: "250m"
  limits:
    memory: "128Mi"
    cpu: "500m"</code></pre>

    <p><strong>QoS Classes:</strong> Guaranteed (requests==limits) &gt; Burstable (requests&lt;limits) &gt; BestEffort
      (no requests/limits)</p>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pod x -o jsonpath='{.status.qosClass}'</code><span class="tip-icon" data-tip="Checks the QoS class assigned to a pod. Returns: Guaranteed, Burstable, or BestEffort. Determines eviction priority during node memory pressure.">i</span></div>
      <div class="cmd-row"><code>k top nodes</code><span class="tip-icon" data-tip="Shows current CPU and memory usage per node. Requires metrics-server addon. Helps identify resource constraints before scheduling new pods.">i</span></div>
      <div class="cmd-row"><code>k top pods --sort-by=memory</code><span class="tip-icon" data-tip="Lists pods sorted by memory consumption (highest first). Add -A for all namespaces. Use to spot memory hogs and potential leaks.">i</span></div>
      <div class="cmd-row"><code>k describe node node01 | grep -A 10 "Allocated resources"</code><span class="tip-icon" data-tip="Shows total node capacity and allocated/requested resources from running pods. Helps determine how much headroom remains for new pods.">i</span></div>
    </div>

    <h2>LimitRange (per-container defaults)</h2>
    <div class="imp-block">
      <strong>No imperative command</strong> — LimitRange must be created via YAML. Quick copy from docs: search
      <code>kubernetes.io/docs</code> for "LimitRange".<br>
      <code>k explain limitrange.spec.limits --recursive</code> to see structure.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: LimitRange
metadata:
  name: mem-cpu-limits
  namespace: dev
spec:
  limits:
  - type: Container
    default:          { cpu: "500m", memory: "128Mi" }
    defaultRequest:   { cpu: "250m", memory: "64Mi" }
    max:              { cpu: "2", memory: "1Gi" }
    min:              { cpu: "100m", memory: "16Mi" }</code></pre>

    <h2>ResourceQuota (namespace-total)</h2>
    <div class="imp-block">
      <strong>Imperative:</strong>
      <pre><code># Create ResourceQuota imperatively
k create quota compute-quota -n dev \
  --hard=pods=10,requests.cpu=4,requests.memory=4Gi,limits.cpu=8,limits.memory=8Gi

# Generate YAML to customize
k create quota compute-quota -n dev \
  --hard=pods=10,requests.cpu=4 \
  --dry-run=client -o yaml > quota.yaml</code></pre>
      <strong>Verify:</strong> <code>k describe quota compute-quota -n dev</code> — shows used vs hard limits.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 4Gi
    limits.cpu: "8"
    limits.memory: 8Gi
    services: "5"
    persistentvolumeclaims: "4"</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="ds-job">9. DaemonSet, Job, CronJob</h1>

    <h2>DaemonSet</h2>
    <p>One pod per node (all or subset via nodeSelector). Use for: log collectors, node monitoring, CNI/CSI plugins.</p>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No direct <code>k create daemonset</code> command exists!</strong> Use this trick:
      <pre><code># Step 1: Generate a Deployment YAML as base
k create deployment monitoring-daemon --image=prom/node-exporter \
  -n kube-system --dry-run=client -o yaml > ds.yaml

# Step 2: Edit ds.yaml and change these:
#   kind: Deployment  →  kind: DaemonSet
#   Remove: spec.replicas (DaemonSet doesn't use replicas)
#   Remove: spec.strategy (DaemonSet uses updateStrategy instead)
#   Add tolerations if needed for control-plane nodes

# Step 3: Apply
k apply -f ds.yaml</code></pre>
      <strong>Key changes from Deployment → DaemonSet:</strong>
      <ul>
        <li>Change <code>kind: Deployment</code> → <code>kind: DaemonSet</code></li>
        <li>Delete the <code>replicas:</code> line</li>
        <li>Delete the <code>strategy:</code> block (if any)</li>
        <li>Keep <code>selector</code> and <code>template</code> as-is</li>
      </ul>
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  namespace: kube-system
spec:
  selector:
    matchLabels: { app: monitoring-daemon }
  template:
    metadata:
      labels: { app: monitoring-daemon }
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      containers:
      - name: agent
        image: prom/node-exporter
        resources:
          limits: { memory: "200Mi", cpu: "100m" }</code></pre>

    <pre><code>k get ds -A
k describe ds monitoring-daemon -n kube-system
k rollout undo ds monitoring-daemon -n kube-system</code></pre>

    <h2>Job</h2>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Create Job imperatively:</strong>
      <pre><code># Create directly
k create job pi-job --image=perl -- perl -Mbignum=bpi -wle 'print bpi(2000)'

# Generate YAML to add completions/parallelism
k create job pi-job --image=perl \
  --dry-run=client -o yaml -- perl -Mbignum=bpi -wle 'print bpi(2000)' > job.yaml

# Then edit to add:
#   spec.completions: 3       (how many times to run)
#   spec.parallelism: 2       (how many run at once)
#   spec.backoffLimit: 4      (max retries)
#   spec.activeDeadlineSeconds: 100  (timeout)
k apply -f job.yaml</code></pre>
      <strong>Generated vs manual fields:</strong> The command generates <code>restartPolicy: Never</code> automatically
      (required for Jobs). Add <code>completions</code>, <code>parallelism</code>, <code>backoffLimit</code> manually.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: batch/v1
kind: Job
metadata: { name: pi-job }
spec:
  completions: 3
  parallelism: 2
  backoffLimit: 4
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl","-Mbignum=bpi","-wle","print bpi(2000)"]
      restartPolicy: Never</code></pre>

    <h2>CronJob</h2>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Create CronJob imperatively:</strong>
      <pre><code># Create directly
k create cronjob backup --image=busybox --schedule="0 2 * * *" -- /bin/sh -c "date"

# Generate YAML to customize
k create cronjob backup --image=busybox --schedule="0 2 * * *" \
  --dry-run=client -o yaml -- /bin/sh -c "date" > cj.yaml

# Edit to add:
#   spec.successfulJobsHistoryLimit: 3
#   spec.failedJobsHistoryLimit: 1
#   spec.jobTemplate.spec.template.spec.restartPolicy: OnFailure
k apply -f cj.yaml</code></pre>
      <strong>Common schedules:</strong> <code>*/5 * * * *</code> (every 5 min), <code>0 * * * *</code> (hourly),
      <code>0 2 * * *</code> (daily at 2am), <code>0 0 * * 0</code> (weekly Sunday).
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: batch/v1
kind: CronJob
metadata: { name: backup }
spec:
  schedule: "0 2 * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: busybox
            command: ["/bin/sh","-c","date"]
          restartPolicy: OnFailure</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="static-pods">10. Static Pods</h1>
    <p>Managed by kubelet directly from <code>/etc/kubernetes/manifests/</code>. Not through API server. Control-plane
      components are static pods via kubeadm.</p>
    <p>To delete: remove the manifest file. <code>kubectl delete</code> will NOT work permanently.</p>
    <p>Naming: node name appended (e.g., <code>kube-apiserver-controlplane</code>).</p>

    <div class="cmd-table">
      <div class="cmd-row"><code>ps aux | grep kubelet | grep config</code><span class="tip-icon" data-tip="Finds the kubelet process and identifies the config file path. Look for --config=/var/lib/kubelet/config.yaml to confirm the kubelet config location.">i</span></div>
      <div class="cmd-row"><code>cat /var/lib/kubelet/config.yaml | grep staticPodPath</code><span class="tip-icon" data-tip="Extracts the staticPodPath setting from kubelet config. Default is usually /etc/kubernetes/manifests/. This is where kubelet watches for pod YAML files.">i</span></div>
      <div class="cmd-row"><code>cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/static-nginx.yaml</code><span class="tip-icon" data-tip="Creates a static pod YAML file in the kubelet watch directory. The kubelet automatically detects and runs it. No need to kubectl apply — kubelet manages it directly.">i</span></div>
      <div class="cmd-row"><code>rm /etc/kubernetes/manifests/static-nginx.yaml</code><span class="tip-icon" data-tip="Deletes the static pod. The kubelet detects the deletion and terminates the pod. kubectl delete pod won't work permanently because kubelet will recreate it from the file.">i</span></div>
      <div class="cmd-row"><code>k get pod kube-apiserver-controlplane -n kube-system -o jsonpath='{.metadata.ownerReferences[*].kind}'</code><span class="tip-icon" data-tip="Verifies the pod is static by checking its ownerReference. Static pods have ownerReferences.kind == 'Node', while API pods have 'ReplicaSet' or other controllers.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="cm-secret">11. ConfigMaps &amp; Secrets</h1>

    <h2>ConfigMap</h2>
    <h3>⚡ Imperative Commands</h3>
    <div class="imp-block">
      <strong>ConfigMaps are best created imperatively:</strong>
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod</code><span class="tip-icon" data-tip="Creates ConfigMap from inline key=value pairs. Each --from-literal creates one entry. Fastest method for exam, especially for small configs.">i</span></div>
      <div class="cmd-row"><code>k create configmap app-config --from-file=app.properties</code><span class="tip-icon" data-tip="Creates ConfigMap from a file. The filename becomes the key, file contents become the value. Use --from-file=key=path to customize the key name.">i</span></div>
      <div class="cmd-row"><code>k create configmap app-config --from-file=config-key=/path/to/file</code><span class="tip-icon" data-tip="Creates ConfigMap with a specific key name. Format: --from-file=KEY=path. Useful when the source filename doesn't match the desired config key.">i</span></div>
      <div class="cmd-row"><code>k create configmap app-config --from-literal=APP_COLOR=blue --dry-run=client -o yaml &gt; cm.yaml</code><span class="tip-icon" data-tip="Generates ConfigMap YAML without creating it. Useful to inspect or modify before applying. After reviewing, run kubectl apply -f cm.yaml.">i</span></div>
      <div class="cmd-row"><code>k get cm</code><span class="tip-icon" data-tip="Lists all ConfigMaps in current namespace. Add -n &lt;namespace&gt; for a specific namespace, or -A for all namespaces.">i</span></div>
      <div class="cmd-row"><code>k describe cm app-config</code><span class="tip-icon" data-tip="Shows ConfigMap contents (key=value pairs) and metadata. Essential for verifying the config was created with correct data.">i</span></div>
    </div>
    <p><strong>Pod injection:</strong> After creating ConfigMap, generate pod YAML and add <code>envFrom</code> or
      <code>env.valueFrom.configMapKeyRef</code> under the container, or mount as volume.</p>

    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod</code></pre>

    <h3>Inject into Pod</h3>
    <span class="yaml-label">YAML</span>
    <pre><code># All keys as env vars
envFrom:
- configMapRef:
    name: app-config

# Single key
env:
- name: APP_COLOR
  valueFrom:
    configMapKeyRef:
      name: app-config
      key: APP_COLOR

# As volume (files)
volumes:
- name: config-vol
  configMap:
    name: app-config</code></pre>

    <h2>Secret</h2>
    <h3>⚡ Imperative Commands</h3>
    <div class="imp-block">
      <strong>All 3 secret types can be created imperatively:</strong>
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_Password=pass123</code><span class="tip-icon" data-tip="Creates a generic Secret with literal key=value pairs. Values are automatically base64-encoded by Kubernetes. This is the most common secret type for app credentials.">i</span></div>
      <div class="cmd-row"><code>k create secret tls webapp-tls --cert=tls.crt --key=tls.key</code><span class="tip-icon" data-tip="Creates a TLS-type Secret for Ingress HTTPS termination. Requires valid PEM-format certificate and private key files. Kubernetes auto-encodes both files in base64.">i</span></div>
      <div class="cmd-row"><code>k create secret docker-registry regcred --docker-server=private-registry.io --docker-username=user --docker-password=pass --docker-email=user@org.com</code><span class="tip-icon" data-tip="Creates a docker-registry type Secret for pulling images from private registries. Used via imagePullSecrets in pod spec. Automatically base64-encodes credentials.">i</span></div>
      <div class="cmd-row"><code>k create secret generic app-secret --from-literal=DB_Host=mysql --dry-run=client -o yaml &gt; secret.yaml</code><span class="tip-icon" data-tip="Generates Secret YAML without creating it. Values in output are already base64-encoded. Review before applying with kubectl apply -f secret.yaml.">i</span></div>
      <div class="cmd-row"><code>k get secret app-secret -o yaml</code><span class="tip-icon" data-tip="Shows Secret contents as YAML. Data values appear base64-encoded — decode them with echo -n 'encoded' | base64 --decode to verify contents.">i</span></div>
      <div class="cmd-row"><code>echo -n "bX1zcWw=" | base64 --decode</code><span class="tip-icon" data-tip="Decodes a base64-encoded secret value back to plain text. Use to verify secret contents are correct.">i</span></div>
      <div class="cmd-row"><code>echo -n "myvalue" | base64</code><span class="tip-icon" data-tip="Encodes a value to base64 for manual YAML creation. If writing secrets in YAML directly (not imperative), encode all values first and put them under 'data:' section.">i</span></div>
    </div>
    <p><strong>Key advantage:</strong> Imperative command <strong>auto-encodes values to base64</strong>. If writing YAML
      manually, you must encode with <code>echo -n "value" | base64</code>, or use <code>stringData:</code> instead of <code>data:</code> to avoid manual encoding.</p>

    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=          # base64 encoded
  DB_Password: cGFzd3Jk

# Inject same as ConfigMap:
# envFrom: - secretRef: name: app-secret
# OR env: - valueFrom: secretKeyRef: name/key</code></pre>

    <h3>Pull from Private Registry</h3>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  containers:
  - name: app
    image: private-registry.io/apps/internal-app
  imagePullSecrets:
  - name: regcred</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="multi-container">12. Multi-Container Pods &amp; Init Containers</h1>

    <h2>Patterns</h2>
    <p><strong>Sidecar:</strong> auxiliary (log shipper, proxy). <strong>Ambassador:</strong> proxy to external.
      <strong>Adapter:</strong> transforms output.
    </p>

    <h3>Sidecar Example</h3>
    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No direct command for multi-container pods.</strong> Use this pattern:
      <pre><code># Step 1: Generate base pod with first container
k run app-with-sidecar --image=busybox \
  --dry-run=client -o yaml \
  --command -- sh -c 'while true; do echo running >> /var/log/app.log; sleep 5; done' > mc.yaml

# Step 2: Open mc.yaml and duplicate the containers section:
#   - Copy the container block
#   - Rename to "sidecar", change image/command
#   - Add shared volumeMounts to BOTH containers
#   - Add volumes: emptyDir at spec level

# Step 3: Apply
k apply -f mc.yaml</code></pre>
      <strong>What to add manually:</strong>
      <ul>
        <li>Second container under <code>containers:</code> (same indent level as first)</li>
        <li><code>volumeMounts</code> in each container pointing to shared mount path</li>
        <li><code>volumes: - name: shared-vol</code> with <code>emptyDir: {}</code> under spec</li>
      </ul>
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'while true; do echo "$(date) INFO running" &gt;&gt; /var/log/app.log; sleep 5; done']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  - name: sidecar
    image: busybox
    command: ['sh', '-c', 'tail -f /var/log/app.log']
    volumeMounts:
    - name: log-volume
      mountPath: /var/log
  volumes:
  - name: log-volume
    emptyDir: {}</code></pre>

    <h2>Init Containers</h2>
    <p>Run to completion <strong>BEFORE</strong> main containers. Sequential. Failure → pod restarts.</p>
    <div class="imp-block">
      <strong>Generate and add init containers:</strong>
      <pre><code># Generate base pod YAML
k run myapp --image=busybox:1.28 --dry-run=client -o yaml \
  --command -- sh -c 'echo running && sleep 3600' > pod.yaml

# Edit pod.yaml — add initContainers BEFORE containers:
#   initContainers:          &lt;— same indent as containers
#   - name: init-myservice
#     image: busybox:1.28
#     command: ['sh', '-c', 'until nslookup myservice; do sleep 2; done']
k apply -f pod.yaml</code></pre>
      <strong>Key rule:</strong> <code>initContainers</code> goes at the same level as <code>containers</code> under
      <code>spec</code>. Init containers run in order, each must complete before the next starts.
    </div>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting; sleep 2; done']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting; sleep 2; done']
  containers:
  - name: myapp
    image: busybox:1.28
    command: ['sh', '-c', 'echo running &amp;&amp; sleep 3600']</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="probes">13. Probes &mdash; Liveness &amp; Readiness</h1>
    <p><strong>livenessProbe:</strong> fails → container restarted. <strong>readinessProbe:</strong> fails → removed
      from Service endpoints.</p>
    <p><strong>Types:</strong> httpGet, exec, tcpSocket</p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No imperative flag for probes.</strong> Generate base pod YAML and add probes manually:
      <pre><code># Generate pod YAML
k run myapp --image=myapp:1.0 --port=8080 --dry-run=client -o yaml > pod.yaml

# Edit pod.yaml — add probes under the container spec:
#   livenessProbe:
#     httpGet: { path: /healthz, port: 8080 }
#     initialDelaySeconds: 15
#     periodSeconds: 20
#   readinessProbe:
#     httpGet: { path: /ready, port: 8080 }
#     initialDelaySeconds: 5
#     periodSeconds: 10
k apply -f pod.yaml

# Quick reference for probe structure:
k explain pod.spec.containers.livenessProbe
k explain pod.spec.containers.readinessProbe</code></pre>
      <strong>Which probe type to use:</strong>
      <ul>
        <li><strong>httpGet</strong> — for web apps (path + port)</li>
        <li><strong>exec</strong> — for file/script checks (<code>command: ["cat", "/tmp/healthy"]</code>)</li>
        <li><strong>tcpSocket</strong> — for databases/services (<code>port: 3306</code>)</li>
      </ul>
    </div>

    <span class="yaml-label">YAML</span>
    <pre><code>containers:
- name: app
  image: myapp:1.0
  livenessProbe:
    httpGet:
      path: /healthz
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 20
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10

# Exec probe:
  livenessProbe:
    exec:
      command: ["cat", "/tmp/healthy"]
    initialDelaySeconds: 5

# TCP probe:
  readinessProbe:
    tcpSocket:
      port: 3306
    initialDelaySeconds: 10</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="rollouts">14. Rolling Updates &amp; Rollbacks</h1>
    <p><strong>Strategies:</strong> RollingUpdate (default, gradual) | Recreate (kill all, then create new)</p>

    <h3>⚡ Imperative Commands (All Done Without YAML)</h3>
    <div class="imp-block">
      <strong>Rolling updates are fully imperative — no YAML editing needed:</strong>
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k set image deployment/myapp nginx=nginx:1.25</code><span class="tip-icon" data-tip="Updates the image of a named container in the Deployment. Triggers a rolling update — old pods are gradually replaced with new ones. Format: deployment/name container=image.">i</span></div>
      <div class="cmd-row"><code>k rollout status deployment/myapp</code><span class="tip-icon" data-tip="Watches and reports rolling update progress in real time. Exit code 0 on success, non-zero on failure. Exits when rollout completes or timeout reached.">i</span></div>
      <div class="cmd-row"><code>k rollout history deployment/myapp</code><span class="tip-icon" data-tip="Lists revision history of the Deployment. Shows revision numbers. Add --revision=N to see details of a specific revision (requires --record or change-cause annotation).">i</span></div>
      <div class="cmd-row"><code>k rollout undo deployment/myapp</code><span class="tip-icon" data-tip="Rolls back to the immediately previous revision. Safe way to recover from a bad image update without downtime. Does not delete the intermediate revisions.">i</span></div>
      <div class="cmd-row"><code>k rollout undo deployment/myapp --to-revision=2</code><span class="tip-icon" data-tip="Rolls back to a specific revision number. Check available revisions first with 'k rollout history deployment/name'. Revision 2 becomes the new current revision.">i</span></div>
      <div class="cmd-row"><code>k rollout pause deployment/myapp</code><span class="tip-icon" data-tip="Pauses the rolling update. Useful for canary deployments: update one pod, test it, then resume to roll out the rest. Paused deployments don't process scale changes.">i</span></div>
      <div class="cmd-row"><code>k rollout resume deployment/myapp</code><span class="tip-icon" data-tip="Resumes a paused rolling update. The deployment continues rolling out pods until it reaches the desired replica count.">i</span></div>
    </div>
    <p><strong>When you need YAML:</strong> Only to set <code>strategy</code> config (<code>maxSurge</code>,
      <code>maxUnavailable</code>) or change strategy type to <code>Recreate</code>.</p>
    <div class="cmd-table" style="margin-top: 1rem;">
      <div class="cmd-row"><code>k get deployment myapp -o yaml &gt; deploy.yaml</code><span class="tip-icon" data-tip="Exports current Deployment YAML. Edit the spec.strategy block, then reapply with kubectl apply -f deploy.yaml.">i</span></div>
      <div class="cmd-row"><code>k edit deployment myapp</code><span class="tip-icon" data-tip="Opens live Deployment YAML in your $KUBE_EDITOR (typically vi). Changes are applied on save. Cannot modify immutable fields.">i</span></div>
    </div>

    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1           # max extra pods during rollout
      maxUnavailable: 0     # all existing pods stay until new ones ready</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="hpa">15. HPA &mdash; Horizontal Pod Autoscaler</h1>
    <p>Scales Deployment replicas based on CPU/memory. Requires Metrics Server. Pods <strong>MUST</strong> have
      <code>resources.requests</code>.
    </p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Create HPA imperatively (fastest in exam):</strong>
      <pre><code># Create HPA directly (no YAML needed!)
k autoscale deployment myapp --min=2 --max=10 --cpu-percent=80

# Generate YAML for custom metrics
k autoscale deployment myapp --min=2 --max=10 --cpu-percent=80 \
  --dry-run=client -o yaml > hpa.yaml

# The generated YAML uses autoscaling/v1 (basic).
# Edit to autoscaling/v2 if you need memory metrics or custom metrics.

# Verify
k get hpa
k describe hpa myapp</code></pre>
      <strong>Conversion note:</strong> <code>k autoscale</code> generates <code>autoscaling/v1</code> with only CPU.
      For memory metrics, generate YAML and change <code>apiVersion</code> to <code>autoscaling/v2</code>, then add the
      <code>metrics:</code> block as shown below.
    </div>

    <pre><code>k autoscale deployment myapp --min=2 --max=10 --cpu-percent=80
k get hpa
k describe hpa myapp</code></pre>

    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="rbac">16. RBAC &mdash; Roles, Bindings, ServiceAccounts</h1>

    <h2>Imperative (Fastest in Exam)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create role dev-role --verb=get,list,create,delete --resource=pods -n dev</code><span class="tip-icon" data-tip="Creates a namespaced Role granting verbs on a resource. Format: --verb=verbs --resource=type. Verbs: get, list, watch, create, update, patch, delete.">i</span></div>
      <div class="cmd-row"><code>k create rolebinding dev-bind --role=dev-role --user=jane -n dev</code><span class="tip-icon" data-tip="Binds a Role to a user in a namespace. The user gains permissions defined in Role within that namespace only.">i</span></div>
      <div class="cmd-row"><code>k create clusterrole node-viewer --verb=get,list --resource=nodes</code><span class="tip-icon" data-tip="Creates a cluster-scoped Role granting access to non-namespaced resources (nodes, PVs, namespaces).">i</span></div>
      <div class="cmd-row"><code>k create clusterrolebinding node-bind --clusterrole=node-viewer --user=jane</code><span class="tip-icon" data-tip="Binds a ClusterRole to a user cluster-wide. User can perform actions on all matching resources in all namespaces.">i</span></div>
      <div class="cmd-row"><code>k auth can-i create pods --as=jane -n dev</code><span class="tip-icon" data-tip="Tests if user has permission. Returns 'yes' or 'no'. Essential for verifying RBAC before running commands.">i</span></div>
      <div class="cmd-row"><code>k auth can-i '*' '*' --as=system:serviceaccount:default:my-sa</code><span class="tip-icon" data-tip="Tests if ServiceAccount has full admin permissions. Format: --as=system:serviceaccount:ns:name.">i</span></div>
      <div class="cmd-row"><code>k auth can-i list pods -n dev --as system:serviceaccount:dev:app-sa</code><span class="tip-icon" data-tip="Tests if a ServiceAccount can list pods in a namespace. Debug pod permissions before deployment.">i</span></div>
    </div>

    <h2>Role YAML</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: dev
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]</code></pre>

    <h2>RoleBinding YAML</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-binding
  namespace: dev
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io</code></pre>

    <h2>ClusterRole + ClusterRoleBinding</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin-role
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin-role
  apiGroup: rbac.authorization.k8s.io</code></pre>

    <h2>ServiceAccount</h2>
    <pre><code>k create sa dashboard-sa
k create token dashboard-sa    # short-lived token (1.24+)</code></pre>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  serviceAccountName: dashboard-sa
  automountServiceAccountToken: false   # opt-out</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="tls-csr">17. TLS &amp; Certificates &mdash; CSR Workflow</h1>

    <h2>Key Certificate Files (kubeadm)</h2>
    <p>All under <code>/etc/kubernetes/pki/</code>:</p>
    <ul>
      <li><code>ca.crt / ca.key</code> — Cluster CA (root of trust)</li>
      <li><code>apiserver.crt / apiserver.key</code> — API server TLS</li>
      <li><code>apiserver-kubelet-client.crt</code> — API server → kubelet client cert</li>
      <li><code>apiserver-etcd-client.crt</code> — API server → etcd client cert</li>
      <li><code>etcd/ca.crt, etcd/server.crt</code> — etcd own CA and server cert</li>
    </ul>

    <h2>Certificate Inspection</h2>
    <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
# Check: Not Before / Not After (expiry), Subject, Issuer, SAN
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A2 Validity</code></pre>

    <h2>Create User Certificate (CSR Workflow)</h2>
    <pre><code># 1. Generate key and CSR
openssl genrsa -out jane.key 2048
openssl req -new -key jane.key -subj "/CN=jane/O=dev" -out jane.csr

# 2. Base64 encode the CSR
cat jane.csr | base64 | tr -d '\n'</code></pre>

    <span class="yaml-label">YAML</span>
    <pre><code># 3. CSR Object
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata: { name: jane }
spec:
  request: &lt;base64-encoded-csr&gt;
  signerName: kubernetes.io/kube-apiserver-client
  usages: [client auth]</code></pre>

    <pre><code># 4. Approve and extract
k certificate approve jane
k get csr jane -o jsonpath='{.status.certificate}' | base64 -d &gt; jane.crt

# 5. Configure kubeconfig
k config set-credentials jane --client-certificate=jane.crt --client-key=jane.key
k config set-context jane-ctx --cluster=kubernetes --user=jane
k config use-context jane-ctx

# Deny a CSR
k certificate deny bad-user</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="kubeconfig">18. kubeconfig</h1>
    <p>Default: <code>~/.kube/config</code>. Contains: clusters (API server + CA), users (certs/tokens), contexts
      (cluster + user + ns).</p>

    <pre><code>k config view
k config view --kubeconfig=my-config
k config get-contexts
k config current-context
k config use-context &lt;context&gt;
k config set-context --current --namespace=dev
k config set-credentials user --client-certificate=user.crt --client-key=user.key
k config set-context user-ctx --cluster=kubernetes --user=user</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="security-ctx">19. Security Contexts</h1>
    <p>Pod-level or container-level. Container overrides pod. Capabilities only at container level.</p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No imperative flag for security contexts.</strong> Generate and edit:
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k run secure-pod --image=ubuntu --dry-run=client -o yaml > pod.yaml</code><span class="tip-icon" data-tip="Generates base pod YAML. Add securityContext blocks under spec (pod-level) and containers (container-level).">i</span></div>
      <div class="cmd-row"><code>k explain pod.spec.securityContext</code><span class="tip-icon" data-tip="Shows pod-level securityContext fields: runAsUser, fsGroup, runAsGroup, seLinuxOptions. Container values override pod-level.">i</span></div>
      <div class="cmd-row"><code>k explain pod.spec.containers.securityContext.capabilities</code><span class="tip-icon" data-tip="Shows Linux capabilities structure (add/drop). Capabilities can ONLY be set at container level, not pod level.">i</span></div>
      <div class="cmd-row"><code>k exec my-pod -- whoami</code><span class="tip-icon" data-tip="Shows effective user inside container. Verify runAsUser setting applied correctly.">i</span></div>
      <div class="cmd-row"><code>k exec my-pod -- id</code><span class="tip-icon" data-tip="Shows user ID, group ID, and group membership inside container. Confirms securityContext worked.">i</span></div>
    </div>
    <p><strong>Remember:</strong> <code>capabilities</code> can only be set at <strong>container level</strong>, not pod
      level.
    </p>

    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  securityContext:           # Pod-level
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  containers:
  - name: ubuntu
    image: ubuntu
    securityContext:          # Container-level (overrides pod)
      runAsUser: 1000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
        drop: ["ALL"]</code></pre>

    <pre><code>k exec my-pod -- whoami
k exec my-pod -- id</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="netpol">20. NetworkPolicy</h1>
    <p>Controls ingress/egress to pods. Requires CNI with policy support (Calico, Cilium). Default: allow all. Once
      policy selects pod → deny all not explicitly allowed.</p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No <code>kubectl create networkpolicy</code> command exists!</strong> You must write YAML from scratch or
      copy from docs.
      <pre><code># Quick way: Use k explain to build it
k explain networkpolicy.spec --recursive

# Or search kubernetes.io/docs for "NetworkPolicy" in exam browser

# Fastest approach — memorize this minimal template:
cat &lt;&lt;EOF | k apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: my-policy
  namespace: prod
spec:
  podSelector:
    matchLabels: { app: db }
  policyTypes: [Ingress]
  ingress:
  - from:
    - podSelector:
        matchLabels: { app: api }
    ports: [{ protocol: TCP, port: 3306 }]
EOF

# Verify
k get netpol -n prod
k describe netpol my-policy -n prod</code></pre>
      <strong>Exam tip:</strong> Always specify <code>policyTypes</code> explicitly. An empty
      <code>podSelector: {}</code> selects ALL pods in the namespace.
    </div>

    <h2>Default Deny All</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress</code></pre>

    <h2>Allow Specific Ingress</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-db
  namespace: prod
spec:
  podSelector:
    matchLabels: { app: db }
  policyTypes: [Ingress, Egress]
  ingress:
  - from:
    - podSelector:
        matchLabels: { app: api }
    - namespaceSelector:
        matchLabels: { env: prod }
    ports: [{ protocol: TCP, port: 3306 }]
  egress:
  - to:
    - podSelector:
        matchLabels: { app: cache }
    ports: [{ protocol: TCP, port: 6379 }]</code></pre>

    <h3>Cross-namespace &amp; IP Block</h3>
    <span class="yaml-label">YAML</span>
    <pre><code># Cross-namespace access
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        name: monitoring

# External CIDR access
ingress:
- from:
  - ipBlock:
      cidr: 203.0.113.0/24
      except:
      - 203.0.113.128/25</code></pre>

    <pre><code># Test from debug pod
k run test --rm -it --image=busybox:1.28 --restart=Never -- sh
nc -zv db 3306
wget -qO- http://web.default.svc.cluster.local:80</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="storage">21. Storage &mdash; PV, PVC, StorageClass</h1>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No imperative commands for PV/PVC/StorageClass</strong> — all must be created via YAML. But use these
      shortcuts:
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k explain pv.spec --recursive</code><span class="tip-icon" data-tip="Shows entire PersistentVolume spec structure with all fields. Useful for building PV manifests.">i</span></div>
      <div class="cmd-row"><code>k explain pvc.spec --recursive</code><span class="tip-icon" data-tip="Shows PersistentVolumeClaim spec structure. Check accessModes, resources, storageClassName fields.">i</span></div>
      <div class="cmd-row"><code>k explain sc --recursive</code><span class="tip-icon" data-tip="Shows StorageClass spec. Reference provisioner, parameters, reclaimPolicy fields.">i</span></div>
      <div class="cmd-row"><code>k get pv,pvc</code><span class="tip-icon" data-tip="Lists all PersistentVolumes and PersistentVolumeClaims. Shows binding status (Bound/Pending).">i</span></div>
      <div class="cmd-row"><code>k describe pvc my-pvc</code><span class="tip-icon" data-tip="Shows PVC details including status, events, and binding info. Check Events for binding errors.">i</span></div>
      <div class="cmd-row"><code>k get pv</code><span class="tip-icon" data-tip="Lists all PersistentVolumes. Shows capacity, access modes, reclaim policy, and binding status.">i</span></div>
      <div class="cmd-row"><code>k get sc</code><span class="tip-icon" data-tip="Lists all StorageClasses. Shows provisioner and volume expansion allowance.">i</span></div>
      <div class="cmd-row"><code>k describe pv pv1</code><span class="tip-icon" data-tip="Shows PV details: capacity, access modes, reclaim policy, and current claim binding.">i</span></div>
      <div class="cmd-row"><code>k describe pvc pvc1</code><span class="tip-icon" data-tip="Shows PVC details: bound PV, size, and events.">i</span></div>
    </div>
    <p><strong>PVC → Pod workflow:</strong> Create PV first → Create PVC (must match accessModes &amp; storage) →
      Reference PVC in pod's <code>volumes:</code> section.
    </p>

    <h2>PersistentVolume (PV)</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity: { storage: 1Gi }
  accessModes: [ReadWriteOnce]        # RWO, ROX, RWX
  persistentVolumeReclaimPolicy: Retain   # Retain, Delete, Recycle
  hostPath: { path: /mnt/data }</code></pre>

    <h2>PersistentVolumeClaim (PVC)</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc1
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests: { storage: 500Mi }
  storageClassName: ""     # empty = static binding only</code></pre>

    <h2>Using PVC in Pod</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc1</code></pre>

    <h2>StorageClass (Dynamic Provisioning)</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata: { name: fast }
provisioner: kubernetes.io/gce-pd
parameters: { type: pd-ssd }
reclaimPolicy: Delete
allowVolumeExpansion: true</code></pre>

    <pre><code>k get pv
k get pvc
k get sc
k describe pv pv1
k describe pvc pvc1</code></pre>

    <p><strong>AccessModes:</strong> RWO (ReadWriteOnce) | ROX (ReadOnlyMany) | RWX (ReadWriteMany)</p>
    <p><strong>Volume Types:</strong> emptyDir (temp), hostPath (node), persistentVolumeClaim (persist), configMap,
      secret</p>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="networking">22. Services &amp; Networking</h1>
    <p>Every Pod gets unique IP. Pods talk without NAT. Service = stable VIP + load balancing.</p>

    <h2>Service Types</h2>
    <p><strong>ClusterIP</strong> (default): internal VIP. <strong>NodePort:</strong> node IP + port (30000-32767).
      <strong>LoadBalancer:</strong> cloud LB.
    </p>

    <h2>Linux Networking Commands (CKA)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>ip link</code><span class="tip-icon" data-tip="Lists all network interfaces on the node. Shows interface names, MAC addresses, and UP/DOWN status.">i</span></div>
      <div class="cmd-row"><code>ip addr</code><span class="tip-icon" data-tip="Shows IP addresses assigned to all interfaces. Displays IPv4, IPv6, and their subnets.">i</span></div>
      <div class="cmd-row"><code>ip route show</code><span class="tip-icon" data-tip="Displays the routing table. Shows destination subnets, next-hop gateways, and which interface forwards traffic.">i</span></div>
      <div class="cmd-row"><code>ip route add 192.168.2.0/24 via 192.168.1.1</code><span class="tip-icon" data-tip="Adds a static route: traffic to 192.168.2.0/24 goes through gateway 192.168.1.1. Temporary until reboot.">i</span></div>
      <div class="cmd-row"><code>ip route add default via 192.168.1.1</code><span class="tip-icon" data-tip="Sets the default gateway for all unmatched traffic. Essential for external connectivity.">i</span></div>
      <div class="cmd-row"><code>cat /proc/sys/net/ipv4/ip_forward</code><span class="tip-icon" data-tip="Checks if IP forwarding is enabled (1=yes, 0=no). Required for inter-node pod communication in Kubernetes.">i</span></div>
      <div class="cmd-row"><code>echo 1 > /proc/sys/net/ipv4/ip_forward</code><span class="tip-icon" data-tip="Enables IP forwarding temporarily. Required for kubelet to route pod traffic. Persist in /etc/sysctl.conf for reboot.">i</span></div>
    </div>

    <h2>CNI (Container Network Interface)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>ls /etc/cni/net.d/</code><span class="tip-icon" data-tip="Lists CNI plugin configuration files (JSON). Each file defines which CNI plugin to use and its settings.">i</span></div>
      <div class="cmd-row"><code>ls /opt/cni/bin/</code><span class="tip-icon" data-tip="Lists CNI binary executables installed on the node. Examples: flannel, calico, weave binaries.">i</span></div>
      <div class="cmd-row"><code>k get node -o jsonpath='{.spec.podCIDR}'</code><span class="tip-icon" data-tip="Shows the pod CIDR (subnet) allocated to a node. CNI uses this range to assign IPs to pods.">i</span></div>
    </div>

    <h2>Service Debugging</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get svc,endpoints &lt;svc-name&gt;</code><span class="tip-icon" data-tip="Shows Service and its Endpoints. Empty endpoints → wrong selector label on pods.">i</span></div>
      <div class="cmd-row"><code>iptables-save | grep &lt;service-name&gt;</code><span class="tip-icon" data-tip="Shows iptables rules created by kube-proxy for the Service. Confirms service routing configuration on the node.">i</span></div>
      <div class="cmd-row"><code>k logs -n kube-system -l k8s-app=kube-proxy</code><span class="tip-icon" data-tip="Shows kube-proxy logs on the current host. Check for iptables/IPVS update errors or network rule failures.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="dns">23. DNS &amp; CoreDNS</h1>
    <p><strong>CoreDNS:</strong> cluster internal DNS. Runs as pods in kube-system. Config in coredns ConfigMap
      (Corefile).</p>
    <p><strong>FQDN:</strong> Service → <code>my-svc.my-ns.svc.cluster.local</code> | Pod →
      <code>1-2-3-4.my-ns.pod.cluster.local</code>
    </p>

    <div class="cmd-table">
      <div class="cmd-row"><code>k run -it --rm debug --image=busybox:1.28 -- nslookup kubernetes</code><span class="tip-icon" data-tip="Launches debug pod and queries DNS for 'kubernetes' service. Verifies CoreDNS is working.">i</span></div>
      <div class="cmd-row"><code>k run -it --rm debug --image=busybox:1.28 -- nslookup &lt;svc&gt;.&lt;ns&gt;.svc.cluster.local</code><span class="tip-icon" data-tip="Tests DNS resolution of a Service FQDN from debug pod. If fails, check CoreDNS pod status and service selector.">i</span></div>
      <div class="cmd-row"><code>k get pods -n kube-system -l k8s-app=kube-dns</code><span class="tip-icon" data-tip="Lists CoreDNS pods. Should show 2+ replicas in kube-system. If missing, DNS won't work for the cluster.">i</span></div>
      <div class="cmd-row"><code>k get configmap coredns -n kube-system -o yaml</code><span class="tip-icon" data-tip="Shows CoreDNS configuration (Corefile). Contains DNS plugins, zones, and custom upstream servers.">i</span></div>
      <div class="cmd-row"><code>k logs &lt;coredns-pod&gt; -n kube-system</code><span class="tip-icon" data-tip="Shows CoreDNS logs. Check for query failures, update errors, or plugin issues.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="ingress">24. Ingress</h1>
    <p>Requires Ingress Controller (e.g., nginx-ingress). Manages external HTTP/HTTPS access.</p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>Create Ingress imperatively:</strong>
    </div>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create ingress app-ingress --rule="/wear=wear-service:80"</code><span class="tip-icon" data-tip="Creates Ingress with a single path rule. Format: path=service:port. Traffic to /wear routes to wear-service:80.">i</span></div>
      <div class="cmd-row"><code>k create ingress app-ingress --rule="/wear=wear-service:80" --annotation nginx.ingress.kubernetes.io/rewrite-target=/</code><span class="tip-icon" data-tip="Creates Ingress with annotation. --annotation sets metadata for the controller. nginx.ingress... rewrites the request path.">i</span></div>
      <div class="cmd-row"><code>k create ingress app-ingress --rule="/wear=wear-service:80" --rule="/watch=watch-service:80" --dry-run=client -o yaml > ingress.yaml</code><span class="tip-icon" data-tip="Multiple path rules. Each --rule defines a separate path. Generates YAML without creating it for review/editing.">i</span></div>
      <div class="cmd-row"><code>k create ingress app-ingress --rule="wear.example.com/=wear-svc:80" --rule="watch.example.com/=watch-svc:80"</code><span class="tip-icon" data-tip="Host-based routing. Format: host/path=svc:port. Different subdomains route to different services.">i</span></div>
      <div class="cmd-row"><code>k create ingress app-ingress --rule="webapp.example.com/=webapp-svc:80,tls=webapp-tls" --dry-run=client -o yaml</code><span class="tip-icon" data-tip="TLS/HTTPS rule. Format: host/path=svc:port,tls=secret-name. Secret must be of type kubernetes.io/tls.">i</span></div>
    </div>
    <p><strong>What to add manually after generation:</strong></p>
    <div class="cmd-table">
      <div class="cmd-row\"><code>annotations</code><span class=\"tip-icon\" data-tip=\"Metadata for controller (e.g. rewrite-target). Can be added via --annotation flag or edited in YAML.\">i</span></div>
      <div class=\"cmd-row\"><code>pathType</code><span class=\"tip-icon\" data-tip=\"Matching strategy: Exact (exact path), Prefix (prefix match), ImplementationSpecific (default). Generated as Exact, change to Prefix as needed.\">i</span></div>
    </div>

    <h2>Path-based Routing</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service: { name: wear-service, port: { number: 80 } }
      - path: /watch
        pathType: Prefix
        backend:
          service: { name: watch-service, port: { number: 80 } }</code></pre>

    <h2>Host-based Routing</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  rules:
  - host: wear.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service: { name: wear-svc, port: { number: 80 } }
  - host: watch.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service: { name: watch-svc, port: { number: 80 } }</code></pre>

    <h2>Ingress with TLS</h2>
    <pre><code>k create secret tls webapp-tls --cert=tls.crt --key=tls.key -n apps</code></pre>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  tls:
  - hosts:
    - webapp.example.com
    secretName: webapp-tls
  rules:
  - host: webapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service: { name: webapp-svc, port: { number: 80 } }</code></pre>

    <pre><code>k create ingress my-ingress --rule="host/path=svc:80" --dry-run=client -o yaml
k get ingress
k describe ingress app-ingress</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="etcd">25. ETCD Backup &amp; Restore</h1>
    <div class="exam-note"><strong>EXAM:</strong> This appears in almost every CKA exam. Memorize the command with cert
      paths.</div>

    <h2>Backup</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key</code><span class="tip-icon" data-tip="Backs up entire etcd database to snapshot file. ETCDCTL_API=3 enables v3 API. Must provide client certs for secure connection.">i</span></div>
      <div class="cmd-row"><code>ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd.db</code><span class="tip-icon" data-tip="Verifies snapshot integrity and shows metadata. Confirms backup succeeded.">i</span></div>
    </div>

    <h2>Restore</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>etcdutl snapshot restore /tmp/etcd.db --data-dir=/var/lib/etcd-restored</code><span class="tip-icon" data-tip="Restores etcd from snapshot to new data directory. Old data-dir must be moved away first.">i</span></div>
      <div class="cmd-row"><code>vi /etc/kubernetes/manifests/etcd.yaml</code><span class="tip-icon" data-tip="Edit etcd static pod manifest. Change hostPath.path to /var/lib/etcd-restored.">i</span></div>
      <div class="cmd-row"><code>systemctl daemon-reload</code><span class="tip-icon" data-tip="Reloads systemd configuration. Required after editing unit files.">i</span></div>
      <div class="cmd-row"><code>systemctl restart kubelet</code><span class="tip-icon" data-tip="Restarts kubelet. Detects etcd manifest change and restarts etcd pod with new data path.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="upgrade">26. Cluster Upgrade (kubeadm)</h1>
    <p><strong>Rule:</strong> Upgrade one minor version at a time. Control plane first, then workers.</p>

    <h2>Control Plane</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>apt-mark unhold kubeadm</code><span class="tip-icon" data-tip="Unlocks kubeadm. Allows apt to upgrade it.">i</span></div>
      <div class="cmd-row"><code>apt-get update &amp;&amp; apt-get install -y kubeadm=1.XX.0-*</code><span class="tip-icon" data-tip="Updates repos and installs kubeadm version. Replace XX with target.">i</span></div>
      <div class="cmd-row"><code>apt-mark hold kubeadm</code><span class="tip-icon" data-tip="Locks kubeadm to prevent accidental future upgrades.">i</span></div>
      <div class="cmd-row"><code>kubeadm upgrade plan</code><span class="tip-icon" data-tip="Shows upgrade path and checklist. Displays what will be upgraded.">i</span></div>
      <div class="cmd-row"><code>kubeadm upgrade apply v1.XX.0</code><span class="tip-icon" data-tip="Upgrades control plane components. One-time per node.">i</span></div>
      <div class="cmd-row"><code>kubectl drain &lt;cp-node&gt; --ignore-daemonsets --delete-emptydir-data</code><span class="tip-icon" data-tip="Evicts pods from node. DaemonSet pods ignored, emptyDir volumes deleted.">i</span></div>
      <div class="cmd-row"><code>apt-mark unhold kubelet kubectl</code><span class="tip-icon" data-tip="Unlocks kubelet and kubectl packages.">i</span></div>
      <div class="cmd-row"><code>apt-get install -y kubelet=1.XX.0-* kubectl=1.XX.0-*</code><span class="tip-icon" data-tip="Installs target kubelet and kubectl versions.">i</span></div>
      <div class="cmd-row"><code>apt-mark hold kubelet kubectl</code><span class="tip-icon" data-tip="Re-locks packages to prevent future upgrades.">i</span></div>
      <div class="cmd-row"><code>systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code><span class="tip-icon" data-tip="Reloads systemd and restarts kubelet with new version.">i</span></div>
      <div class="cmd-row"><code>kubectl uncordon &lt;cp-node&gt;</code><span class="tip-icon" data-tip="Re-enables scheduling. Node returns to operational state.">i</span></div>
    </div>

    <h2>Worker Nodes</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>kubectl drain &lt;worker&gt; --ignore-daemonsets --delete-emptydir-data</code><span class="tip-icon" data-tip="Run from control plane. Evicts all pods from worker node.">i</span></div>
      <div class="cmd-row"><code>apt-mark unhold kubeadm kubelet</code><span class="tip-icon" data-tip="Unlocks packages on worker node.">i</span></div>
      <div class="cmd-row"><code>apt-get install -y kubeadm=1.XX.0-* kubelet=1.XX.0-*</code><span class="tip-icon" data-tip="Installs target versions on worker.">i</span></div>
      <div class="cmd-row"><code>kubeadm upgrade node</code><span class="tip-icon" data-tip="Upgrades kubelet configuration on worker node.">i</span></div>
      <div class="cmd-row"><code>systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code><span class="tip-icon" data-tip="Reloads and restarts kubelet with new config.">i</span></div>
      <div class="cmd-row"><code>apt-mark hold kubeadm kubelet</code><span class="tip-icon" data-tip="Re-locks packages on worker.">i</span></div>
      <div class="cmd-row"><code>kubectl uncordon &lt;worker&gt;</code><span class="tip-icon" data-tip="Run from control plane. Re-enables scheduling on worker.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="node-maint">27. Node Maintenance &mdash; Cordon, Drain, Uncordon</h1>
    <p><strong>cordon</strong> = no new pods | <strong>drain</strong> = cordon + evict | <strong>uncordon</strong> =
      allow scheduling again</p>
    <p>If node is down &gt; 5 min, pods may be terminated by controller.</p>

    <pre><code>kubectl cordon &lt;node&gt;                          # mark unschedulable
kubectl drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data
kubectl drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data --force
kubectl uncordon &lt;node&gt;                        # re-enable scheduling

# Typical workflow:
# 1. drain → 2. perform maintenance → 3. uncordon → 4. verify with kubectl get nodes</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="pdb">28. PodDisruptionBudget (PDB)</h1>
    <p>Limits voluntary disruptions. <code>drain</code> respects PDBs.</p>

    <h3>⚡ Imperative → YAML Workflow</h3>
    <div class="imp-block">
      <strong>No direct imperative command for PDB.</strong> Use heredoc or generate from docs:
      <pre><code># Fastest way: inline YAML
cat &lt;&lt;EOF | k apply -f -
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels: { app: myapp }
EOF

# Quick reference
k explain pdb.spec

# Verify
k get pdb
k describe pdb myapp-pdb</code></pre>
      <strong>Choose one:</strong> <code>minAvailable</code> (minimum pods that must stay up) OR
      <code>maxUnavailable</code> (max pods that can be down). Cannot use both.
    </div>

    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2       # OR maxUnavailable: 1
  selector:
    matchLabels: { app: myapp }</code></pre>

    <pre><code>k get pdb
k describe pdb myapp-pdb</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="troubleshoot">29. Troubleshooting Checklist (30% of Exam)</h1>

    <h2>Application Failure</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pods -o wide</code><span class="tip-icon" data-tip="Lists pods with node assignments and IP addresses.">i</span></div>
      <div class="cmd-row"><code>k describe pod &lt;name&gt;</code><span class="tip-icon" data-tip="Shows detailed pod events. Check Status, Conditions, and events section.">i</span></div>
      <div class="cmd-row"><code>k logs &lt;pod&gt; -c &lt;container&gt; --previous</code><span class="tip-icon" data-tip="Shows logs from previous crashed container instance. Use when container restarted.">i</span></div>
      <div class="cmd-row"><code>k get svc,endpoints &lt;svc-name&gt;</code><span class="tip-icon" data-tip="Lists service and its endpoints. Empty = no matching pods.">i</span></div>
      <div class="cmd-row"><code>k describe svc &lt;name&gt;</code><span class="tip-icon" data-tip="Shows service selector. Compare to pod labels to verify match.">i</span></div>
      <div class="cmd-row"><code>k get pods --show-labels</code><span class="tip-icon" data-tip="Lists pods with their labels. Use to debug selector mismatches.">i</span></div>
      <div class="cmd-row"><code>k exec -it &lt;pod&gt; -- sh</code><span class="tip-icon" data-tip="Executes shell in pod. Use to debug application internals.">i</span></div>
    </div>

    <h2>Control Plane Failure</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pods -n kube-system</code><span class="tip-icon" data-tip="Lists control plane components as static pods.">i</span></div>
      <div class="cmd-row"><code>k logs kube-apiserver-master -n kube-system</code><span class="tip-icon" data-tip="Shows API server logs. Check for startup or request errors.">i</span></div>
      <div class="cmd-row"><code>k logs kube-scheduler-master -n kube-system</code><span class="tip-icon" data-tip="Shows scheduler logs. Check scheduling errors or pod pending issues.">i</span></div>
      <div class="cmd-row"><code>k logs kube-controller-manager-master -n kube-system</code><span class="tip-icon" data-tip="Shows controller manager logs. Check for control loop errors.">i</span></div>
      <div class="cmd-row"><code>ETCDCTL_API=3 etcdctl endpoint health --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key</code><span class="tip-icon" data-tip="Checks etcd health. etcd down = cluster unavailable.">i</span></div>
    </div>

    <h2>Worker Node Failure</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get nodes</code><span class="tip-icon" data-tip="Lists node status and readiness.">i</span></div>
      <div class="cmd-row"><code>k describe node &lt;node&gt;</code><span class="tip-icon" data-tip="Shows node conditions (Ready, MemoryPressure, etc) and events.">i</span></div>
      <div class="cmd-row"><code>systemctl status kubelet</code><span class="tip-icon" data-tip="Run on the node. Check if kubelet service is running.">i</span></div>
      <div class="cmd-row"><code>journalctl -u kubelet -f</code><span class="tip-icon" data-tip="Shows kubelet logs in real-time. Run on the node.">i</span></div>
      <div class="cmd-row"><code>systemctl status containerd</code><span class="tip-icon" data-tip="Check container runtime status. Must be running for pods to start.">i</span></div>
      <div class="cmd-row"><code>systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code><span class="tip-icon" data-tip="Reloads systemd and restarts kubelet. Common fix for node issues.">i</span></div>
      <div class="cmd-row"><code>cat /var/lib/kubelet/config.yaml</code><span class="tip-icon" data-tip="Shows kubelet configuration. Check for misconfigured settings.">i</span></div>
      <div class="cmd-row"><code>cat /etc/kubernetes/kubelet.conf</code><span class="tip-icon" data-tip="Shows kubelet kubeconfig. Check server address and token.">i</span></div>
      <div class="cmd-row"><code>openssl x509 -in /var/lib/kubelet/&lt;node&gt;.crt -text -noout</code><span class="tip-icon" data-tip="Examines node certificate. Check expiry and common name.">i</span></div>
    </div>

    <h2>Network Troubleshooting</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pods -n kube-system</code><span class="tip-icon" data-tip="Verify CNI pods (Calico/Flannel/Weave) are running.">i</span></div>
      <div class="cmd-row"><code>k get pods -n kube-system -l k8s-app=kube-dns</code><span class="tip-icon" data-tip="Lists CoreDNS. Not running = cluster DNS broken.">i</span></div>
      <div class="cmd-row"><code>k run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes</code><span class="tip-icon" data-tip="Tests DNS resolution. Must resolve kubernetes service.">i</span></div>
      <div class="cmd-row"><code>k get ds kube-proxy -n kube-system</code><span class="tip-icon" data-tip="Verifies kube-proxy DaemonSet. Check if running on all nodes.">i</span></div>
      <div class="cmd-row"><code>iptables -L -t nat | grep &lt;svc&gt;</code><span class="tip-icon" data-tip="Checks NAT rules for service. Missing rules = traffic not routed.">i</span></div>
      <div class="cmd-row"><code>cat /proc/sys/net/ipv4/ip_forward</code><span class="tip-icon" data-tip="Must be 1. If 0, inter-pod communication fails.">i</span></div>
      <div class="cmd-row"><code>k run netshoot --image=nicolaka/netshoot -it --rm --restart=Never -- bash</code><span class="tip-icon" data-tip="Advanced networking debug pod. Includes netstat, curl, nc, etc.">i</span></div>
      <div class="cmd-row"><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A2 Validity</code><span class="tip-icon" data-tip="Checks API server cert expiry. Expired cert = cluster can't authenticate.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="jsonpath">30. JSONPath &amp; Output Formatting</h1>

    <div class="cmd-table">
      <div class="cmd-row"><code>k get pod x -o jsonpath='{.spec.nodeName}'</code><span class="tip-icon" data-tip="Extracts single value from resource. Returns pod's assigned node.">i</span></div>
      <div class="cmd-row"><code>k get pods -o jsonpath='{.items[*].metadata.name}'</code><span class="tip-icon" data-tip="Lists all pod names. [*] iterates all items.">i</span></div>
      <div class="cmd-row"><code>k get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'</code><span class="tip-icon" data-tip="Range loop with formatting. Prints node name and CPU capacity.">i</span></div>
      <div class="cmd-row"><code>k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'</code><span class="tip-icon" data-tip="Filters: ?() checks condition. Gets internal IPs only.">i</span></div>
      <div class="cmd-row"><code>k get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,IMAGE:.spec.containers[0].image</code><span class="tip-icon" data-tip="Custom columns. Displays selected fields in table format.">i</span></div>
      <div class="cmd-row"><code>k get pods --sort-by=.metadata.creationTimestamp</code><span class="tip-icon" data-tip="Sorts pods by creation time (oldest first).">i</span></div>
      <div class="cmd-row"><code>k get nodes --sort-by=.status.capacity.cpu</code><span class="tip-icon" data-tip="Sorts nodes by CPU capacity.">i</span></div>
      <div class="cmd-row"><code>k get pods --sort-by='.status.containerStatuses[0].restartCount'</code><span class="tip-icon" data-tip="Sorts by restart count. Finds problematic pods.">i</span></div>
      <div class="cmd-row"><code>k get nodes -o json &gt; /opt/output.json</code><span class="tip-icon" data-tip="Exports full JSON output to file. Common exam task.">i</span></div>
      <div class="cmd-row"><code>k get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' &gt; /opt/os.txt</code><span class="tip-icon" data-tip="Exports extracted field to file. Exam: save specific data.">i</span></div>
      <div class="cmd-row"><code>k get pods -A --no-headers | wc -l</code><span class="tip-icon" data-tip="Counts total pods. --no-headers removes header line.">i</span></div>
      <div class="cmd-row"><code>k get pods -A -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort | uniq</code><span class="tip-icon" data-tip="Lists all unique images running in cluster.">i</span></div>
      <div class="cmd-row"><code>k get pod &lt;name&gt; -o jsonpath='{.spec.nodeName}'</code><span class="tip-icon" data-tip="Finds which node a pod runs on.">i</span></div>
      <div class="cmd-row"><code>k get secret &lt;name&gt; -o jsonpath='{.data.password}' | base64 --decode</code><span class="tip-icon" data-tip="Decodes secret value. Secrets stored base64-encoded in etcd.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="helm">31. Helm</h1>
    <p>Package manager for Kubernetes. Charts = templated YAML. Helm 3 (no Tiller). Release = deployed chart instance.
    </p>

    <div class="cmd-table">
      <div class="cmd-row"><code>helm repo add bitnami https://charts.bitnami.com/bitnami</code><span class="tip-icon" data-tip="Adds chart repository. Required before using charts from it.">i</span></div>
      <div class="cmd-row"><code>helm repo update</code><span class="tip-icon" data-tip="Updates chart repos. Gets latest chart versions.">i</span></div>
      <div class="cmd-row"><code>helm install my-nginx bitnami/nginx</code><span class="tip-icon" data-tip="Installs chart release. 'my-nginx' = release name. Simple with defaults.">i</span></div>
      <div class="cmd-row"><code>helm install my-nginx bitnami/nginx -f values.yaml</code><span class="tip-icon" data-tip="Installs with custom values from file. Overrides chart defaults.">i</span></div>
      <div class="cmd-row"><code>helm install my-nginx bitnami/nginx --set service.port=8080</code><span class="tip-icon" data-tip="Installs with inline values. Useful for quick option changes.">i</span></div>
      <div class="cmd-row"><code>helm list</code><span class="tip-icon" data-tip="Lists all releases in current namespace.">i</span></div>
      <div class="cmd-row"><code>helm list -A</code><span class="tip-icon" data-tip="Lists releases across all namespaces.">i</span></div>
      <div class="cmd-row"><code>helm upgrade my-nginx bitnami/nginx --set replicaCount=3</code><span class="tip-icon" data-tip="Upgrades existing release. Updates deployment to 3 replicas.">i</span></div>
      <div class="cmd-row"><code>helm rollback my-nginx 1</code><span class="tip-icon" data-tip="Rolls back to previous release version. Returns to stable state.">i</span></div>
      <div class="cmd-row"><code>helm uninstall my-nginx</code><span class="tip-icon" data-tip="Removes release. Deletes all deployed resources.">i</span></div>
      <div class="cmd-row"><code>helm template my-nginx bitnami/nginx -f values.yaml</code><span class="tip-icon" data-tip="Renders YAML without installing. Preview what will be deployed.">i</span></div>
      <div class="cmd-row"><code>helm show values bitnami/nginx</code><span class="tip-icon" data-tip="Shows all available chart settings and defaults.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="kustomize">32. Kustomize</h1>
    <p>Built into kubectl. Patch-based customization without templating. Uses <code>kustomization.yaml</code>.</p>

    <span class="yaml-label">YAML</span>
    <pre><code># Base: kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml

# Overlay: overlays/prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: replace
        path: /spec/replicas
        value: 5
images:
  - name: myapp
    newTag: v2.0</code></pre>

    <div class="cmd-table">
      <div class="cmd-row"><code>kubectl apply -k overlays/prod</code><span class="tip-icon" data-tip="Applies kustomized YAML. -k flag uses kustomization.yaml from directory.">i</span></div>
      <div class="cmd-row"><code>kubectl kustomize overlays/prod</code><span class="tip-icon" data-tip="Previews final YAML without applying. Useful for debugging patches.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="kubeadm">33. kubeadm Installation</h1>

    <h2>Prerequisites (All Nodes)</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>sudo swapoff -a</code><span class="tip-icon" data-tip="Disables swap immediately. Required for kubelet.">i</span></div>
      <div class="cmd-row"><code>sed -i '/ swap / s/^/#/' /etc/fstab</code><span class="tip-icon" data-tip="Comments out swap partitions in fstab. Disables on reboot.">i</span></div>
      <div class="cmd-row"><code>sudo modprobe overlay</code><span class="tip-icon" data-tip="Loads overlay filesystem module. Required for containerd.">i</span></div>
      <div class="cmd-row"><code>sudo modprobe br_netfilter</code><span class="tip-icon" data-tip="Loads bridge netfilter module. Enables bridge traffic to iptables.">i</span></div>
      <div class="cmd-row"><code>sudo sysctl --system</code><span class="tip-icon" data-tip="Applies kernel settings from /etc/sysctl.d/. Enables IP forwarding.">i</span></div>
      <div class="cmd-row"><code>sudo apt-get update</code><span class="tip-icon" data-tip="Updates package lists.">i</span></div>
      <div class="cmd-row"><code>sudo apt-get install -y kubelet kubeadm kubectl</code><span class="tip-icon" data-tip="Installs Kubernetes tools.">i</span></div>
      <div class="cmd-row"><code>sudo apt-mark hold kubelet kubeadm kubectl</code><span class="tip-icon" data-tip="Prevents accidental upgrades during apt-get upgrade.">i</span></div>
    </div>

    <h2>Control Plane Init</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=&lt;master-ip&gt;</code><span class="tip-icon" data-tip="Initializes control plane. CIDR for pod network. Must match CNI plugin.">i</span></div>
      <div class="cmd-row"><code>mkdir -p $HOME/.kube</code><span class="tip-icon" data-tip="Creates .kube directory for config files.">i</span></div>
      <div class="cmd-row"><code>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</code><span class="tip-icon" data-tip="Copies admin kubeconfig. Required to run kubectl commands.">i</span></div>
      <div class="cmd-row"><code>sudo chown $(id -u):$(id -g) $HOME/.kube/config</code><span class="tip-icon" data-tip="Sets file ownership. Allows non-root user to use kubectl.">i</span></div>
      <div class="cmd-row"><code>kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml</code><span class="tip-icon" data-tip="Installs Flannel CNI. Creates pod network. Required for pod communication.">i</span></div>
    </div>

    <h2>Join Workers</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>sudo kubeadm join &lt;master-ip&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</code><span class="tip-icon" data-tip="Joins worker to cluster. Token from 'kubeadm init' output. Run on worker nodes.">i</span></div>
      <div class="cmd-row"><code>kubeadm token create --print-join-command</code><span class="tip-icon" data-tip="Generates new join command if token expired. Run on control plane.">i</span></div>
    </div>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="crd">34. CRDs &amp; Operators</h1>
    <p>Custom Resource Definitions (CRDs) extend the API with custom types.</p>
    <p>Operators = CRD + Controller that manages lifecycle (install, upgrade, backup).</p>

    <div class="cmd-table">
      <div class="cmd-row"><code>k get crd</code><span class="tip-icon" data-tip="Lists all custom resource definitions installed in cluster.">i</span></div>
      <div class="cmd-row"><code>k get &lt;custom-resource-name&gt;</code><span class="tip-icon" data-tip="Lists instances of custom resource. e.g. k get backups">i</span></div>
      <div class="cmd-row"><code>k describe crd &lt;name&gt;</code><span class="tip-icon" data-tip="Shows CRD schema and structure. Helps understand custom resource properties.">i</span></div>
    </div>

    <h2>Extension Interfaces</h2>
    <p><strong>CNI</strong> (Container Network Interface): pod networking plugins (Calico, Flannel, Weave)</p>
    <p><strong>CSI</strong> (Container Storage Interface): storage drivers for PVs</p>
    <p><strong>CRI</strong> (Container Runtime Interface): container runtimes (containerd, CRI-O)</p>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="practice">35. Practice Scenarios &amp; Mock Questions</h1>

    <h2>Scenario 1: Deployment + Service</h2>
    <p><strong>Task:</strong> Create deployment nginx-deploy --image=nginx:1.21 --replicas=3 -n app. Expose as ClusterIP
      service nginx-svc on port 80.</p>
    <h3>Solution:</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create deployment nginx-deploy --image=nginx:1.21 --replicas=3 -n app</code><span class="tip-icon" data-tip="Creates deployment with 3 replicas in app namespace.">i</span></div>
      <div class="cmd-row"><code>k expose deployment nginx-deploy --name=nginx-svc --port=80 -n app</code><span class="tip-icon" data-tip="Exposes deployment as ClusterIP service on port 80. Pods exposed internally.">i</span></div>
      <div class="cmd-row"><code>k get deploy,svc -n app</code><span class="tip-icon" data-tip="Verifies deployment and service are created.">i</span></div>
    </div>

    <h2>Scenario 2: RBAC &mdash; read-only pods</h2>
    <p><strong>Task:</strong> Create Role allowing get,list on pods in dev namespace. Bind to user jane.</p>
    <h3>Solution:</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>k create role pod-reader --verb=get,list --resource=pods -n dev</code><span class="tip-icon" data-tip="Creates role with get,list permissions on pods in dev namespace.">i</span></div>
      <div class="cmd-row"><code>k create rolebinding jane-pod-reader --role=pod-reader --user=jane -n dev</code><span class="tip-icon" data-tip="Binds role to user jane. Jane can now get/list pods in dev.">i</span></div>
      <div class="cmd-row"><code>k auth can-i get pods --as=jane -n dev</code><span class="tip-icon" data-tip="Tests if jane has permission. Should return 'yes'.">i</span></div>
    </div>

    <h2>Scenario 3: Static Pod</h2>
    <p><strong>Task:</strong> Create static pod static-busybox (busybox, sleep 3600) on control plane.</p>
    <h3>Solution:</h3>
    <pre><code>cat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/static-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command: ["sleep", "3600"]
EOF</code></pre>

    <h2>Scenario 4: etcd Backup</h2>
    <p><strong>Task:</strong> Snapshot etcd to /tmp/etcd-snapshot.db with correct certs.</p>
    <h3>Solution:</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key</code><span class="tip-icon" data-tip="Creates etcd backup. Must include all 3 certs for production cluster.">i</span></div>
    </div>

    <h2>Scenario 5: PV + PVC + Pod</h2>
    <p><strong>Task:</strong> PV pv-log 100Mi RWX hostPath /pv/log. PVC claim-log-1 50Mi RWX. Pod logger (nginx) mount
      at /var/log/nginx.</p>
    <h3>Solution:</h3>
    <span class="yaml-label">YAML</span>
    <pre><code># PV
apiVersion: v1
kind: PersistentVolume
metadata: { name: pv-log }
spec:
  capacity: { storage: 100Mi }
  accessModes: [ReadWriteMany]
  hostPath: { path: /pv/log }
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata: { name: claim-log-1 }
spec:
  accessModes: [ReadWriteMany]
  resources: { requests: { storage: 50Mi } }
---
# Pod
apiVersion: v1
kind: Pod
metadata: { name: logger }
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts: [{ name: log-vol, mountPath: /var/log/nginx }]
  volumes:
  - name: log-vol
    persistentVolumeClaim: { claimName: claim-log-1 }</code></pre>

    <h2>Scenario 6: NetworkPolicy &mdash; deny all + allow specific</h2>
    <p><strong>Task:</strong> Namespace secure. Select pods app=db. Allow ingress only from app=api on TCP 5432.</p>
    <h3>Solution:</h3>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
  namespace: secure
spec:
  podSelector: { matchLabels: { app: db } }
  policyTypes: [Ingress]
  ingress:
  - from:
    - podSelector: { matchLabels: { app: api } }
    ports: [{ protocol: TCP, port: 5432 }]</code></pre>

    <h2>Scenario 7: Cluster Upgrade (v1.30 → v1.31)</h2>
    <p><strong>Task:</strong> Upgrade control plane to v1.31.0.</p>
    <h3>Solution:</h3>
    <div class="cmd-table">
      <div class="cmd-row"><code>sudo apt-mark unhold kubeadm</code><span class="tip-icon" data-tip="Unlock apt from holding kubeadm. Allows upgrade.">i</span></div>
      <div class="cmd-row"><code>sudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm=1.31.0-*</code><span class="tip-icon" data-tip="Updates and installs target kubeadm version.">i</span></div>
      <div class="cmd-row"><code>sudo apt-mark hold kubeadm</code><span class="tip-icon" data-tip="Re-locks kubeadm to prevent accidental upgrades.">i</span></div>
      <div class="cmd-row"><code>sudo kubeadm upgrade plan</code><span class="tip-icon" data-tip="Shows upgrade path and what will be updated.">i</span></div>
      <div class="cmd-row"><code>sudo kubeadm upgrade apply v1.31.0</code><span class="tip-icon" data-tip="Upgrades control plane components to v1.31.0.">i</span></div>
      <div class="cmd-row"><code>kubectl drain &lt;cp-node&gt; --ignore-daemonsets</code><span class="tip-icon" data-tip="Evicts pods so node can be maintained. Daemonsets ignored.">i</span></div>
      <div class="cmd-row"><code>sudo apt-mark unhold kubelet kubectl</code><span class="tip-icon" data-tip="Unlocks kubelet and kubectl for upgrade.">i</span></div>
      <div class="cmd-row"><code>sudo apt-get install -y kubelet=1.31.0-* kubectl=1.31.0-*</code><span class="tip-icon" data-tip="Installs target versions of kubelet and kubectl.">i</span></div>
      <div class="cmd-row"><code>sudo apt-mark hold kubelet kubectl</code><span class="tip-icon" data-tip="Re-locks packages.">i</span></div>
      <div class="cmd-row"><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet</code><span class="tip-icon" data-tip="Reloads systemd and restarts kubelet with new version.">i</span></div>
      <div class="cmd-row"><code>kubectl uncordon &lt;cp-node&gt;</code><span class="tip-icon" data-tip="Re-enables scheduling. Node ready for pods again.">i</span></div>
    </div>

    <h2>Scenario 8: Ingress with TLS</h2>
    <p><strong>Task:</strong> Create TLS secret webapp-tls. Ingress webapp-ingress TLS on webapp.example.com →
      webapp-svc:80.</p>
    <h3>Solution:</h3>
    <pre><code>k create secret tls webapp-tls --cert=tls.crt --key=tls.key -n apps</code></pre>
    <span class="yaml-label">YAML</span>
    <pre><code>spec:
  tls:
  - hosts: [webapp.example.com]
    secretName: webapp-tls
  rules:
  - host: webapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service: { name: webapp-svc, port: { number: 80 } }</code></pre>

    <h2>Scenario 9: Multi-container Pod Logs</h2>
    <p><strong>Task:</strong> Stream previous logs from sidecar container in myapp-pod.</p>
    <h3>Solution:</h3>
    <pre><code>kubectl logs myapp-pod -c sidecar --previous -f</code></pre>

    <h2>Scenario 10: JSONPath Queries</h2>
    <p><strong>Task:</strong> List nodes sorted by CPU. Custom columns for pods. Get InternalIP of all nodes.</p>
    <h3>Solution:</h3>
    <pre><code># Nodes sorted by CPU
k get nodes --sort-by=.status.capacity.cpu

# Custom columns
k get pods -A -o=custom-columns='NAME:.metadata.name,IMAGE:.spec.containers[*].image'

# InternalIP
k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'</code></pre>

    <hr>

    <!-- ═══════════════════════════════════════════ -->
    <h1 id="exam-tips">36. Exam Tips &amp; Common Mistakes</h1>

    <h2>Time Management</h2>
    <p>2 hours, ~17 questions. Average ~7 min/question. Flag hard ones, return later. High-weight first (troubleshooting
      = 30%).</p>

    <h2>Essential Bookmarks for Exam</h2>
    <ul>
      <li>kubernetes.io/docs/reference/kubectl/cheatsheet/</li>
      <li>kubernetes.io/docs/concepts/ (Workloads, Services, Storage, Config)</li>
      <li>kubernetes.io/docs/tasks/ (Administer Cluster, Manage TLS, Configure Pods)</li>
      <li>kubernetes.io/docs/reference/ (API Reference)</li>
    </ul>

    <h2>Common Mistakes to Avoid</h2>
    <ul>
      <li>Forgetting to switch context: 
        <div class="cmd-table" style="display:inline-block; margin: 5px 0;">
          <div class="cmd-row" style="display:inline-block; margin: 0 5px;"><code>k config use-context &lt;context&gt;</code><span class="tip-icon" data-tip="Each exam question may use different cluster. Must switch!">i</span></div>
        </div>
        — each question may use different cluster</li>
      <li>Wrong namespace — always check and use <code>-n &lt;ns&gt;</code></li>
      <li>YAML indentation errors — use <code>k apply -f</code> and read the error</li>
      <li>Not verifying — always <code>k get</code> / <code>k describe</code> to confirm</li>
      <li>Spending too long on one question — flag and move on</li>
      <li>Forgetting 
        <div class="cmd-table" style="display:inline-block; margin: 5px 0;">
          <div class="cmd-row" style="display:inline-block; margin: 0 5px;"><code>--dry-run=client -o yaml</code><span class="tip-icon" data-tip="Fastest way to generate YAML templates without creating resources.">i</span></div>
        </div>
        — fastest way to generate templates</li>
    </ul>

    <h2>Useful One-Liners</h2>
    <div class="cmd-table">
      <div class="cmd-row"><code>k get pods -A -o wide</code><span class="tip-icon" data-tip="All pods with node info and internal IPs. Best overview.">i</span></div>
      <div class="cmd-row"><code>k get events -A --sort-by='.lastTimestamp'</code><span class="tip-icon" data-tip="Cluster events sorted by timestamp. Find warnings and errors.">i</span></div>
      <div class="cmd-row"><code>k get pods -w</code><span class="tip-icon" data-tip="Watch pods live. Shows pod changes in real-time.">i</span></div>
      <div class="cmd-row"><code>k delete pod &lt;name&gt; --force --grace-period=0</code><span class="tip-icon" data-tip="Force delete stuck Terminating pod immediately.">i</span></div>
      <div class="cmd-row"><code>k get pods -A -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort | uniq</code><span class="tip-icon" data-tip="Lists all unique images running in cluster.">i</span></div>
      <div class="cmd-row"><code>k run tmp --image=busybox:1.28 --rm -it --restart=Never -- sh</code><span class="tip-icon" data-tip="Quick debug pod. Creates busybox, shells into it, auto-deletes on exit.">i</span></div>
      <div class="cmd-row"><code>k get secret &lt;name&gt; -o jsonpath='{.data.password}' | base64 --decode</code><span class="tip-icon" data-tip="Decodes secret value. Secrets stored as base64 in etcd.">i</span></div>
      <div class="cmd-row"><code>grep staticPodPath /var/lib/kubelet/config.yaml</code><span class="tip-icon" data-tip="Shows directory monitored for static pods. Run on node.">i</span></div>
      <div class="cmd-row"><code>k get --raw='/readyz?verbose'</code><span class="tip-icon" data-tip="Checks component health. Shows if API server is ready.">i</span></div>
    </div>

    <h2>Priority Classes (Pod Scheduling)</h2>
    <span class="yaml-label">YAML</span>
    <pre><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata: { name: high-priority }
value: 1000000
globalDefault: false
preemptionPolicy: PreemptLowerPriority

# Use in Pod:
spec:
  priorityClassName: high-priority</code></pre>

    <h2>Admission Controllers</h2>
    <p>Intercept API requests after auth. <strong>Validating:</strong> accept/reject (PodSecurity, ResourceQuota).
      <strong>Mutating:</strong> modify (DefaultStorageClass, ServiceAccount). Configure via
      <code>--enable-admission-plugins</code> on kube-apiserver.
    </p>

    <h2>API Groups Quick Reference</h2>
    <pre><code># Core group (no prefix):  apiVersion: v1
# Pod, Service, ConfigMap, Secret, Namespace, PV, PVC, Endpoints

# apps group:  apiVersion: apps/v1
# Deployment, ReplicaSet, DaemonSet, StatefulSet

# batch group:  apiVersion: batch/v1
# Job, CronJob

# networking group:  apiVersion: networking.k8s.io/v1
# NetworkPolicy, Ingress

# rbac group:  apiVersion: rbac.authorization.k8s.io/v1
# Role, ClusterRole, RoleBinding, ClusterRoleBinding

# storage group:  apiVersion: storage.k8s.io/v1
# StorageClass

# policy group:  apiVersion: policy/v1
# PodDisruptionBudget

# autoscaling group:  apiVersion: autoscaling/v2
# HorizontalPodAutoscaler

# certificates group:  apiVersion: certificates.k8s.io/v1
# CertificateSigningRequest

# scheduling group:  apiVersion: scheduling.k8s.io/v1
# PriorityClass

k api-resources          # full list
k api-versions           # all API versions</code></pre>

  </main>

  <!-- ═══════════ Sidebar Active Link Tracking ═══════════ -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const links = document.querySelectorAll('nav a');
      const sections = [];
      links.forEach(a => {
        const id = a.getAttribute('href').slice(1);
        const el = document.getElementById(id);
        if (el) sections.push({ el, a });
      });

      const observer = new IntersectionObserver(entries => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            links.forEach(a => a.classList.remove('active'));
            const match = sections.find(s => s.el === entry.target);
            if (match) match.a.classList.add('active');
          }
        });
      }, { rootMargin: '-20% 0px -70% 0px' });

      sections.forEach(s => observer.observe(s.el));
    });
  </script>

</body>

</html>